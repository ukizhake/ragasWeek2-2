{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wa8ykQk92aLX"
      },
      "source": [
        "# Evaluation with RAGAS and Advanced Retrieval Methods Using LangChain\n",
        "\n",
        "In the following notebook we'll discuss a major component of LLM Ops:\n",
        "\n",
        "- Evaluation\n",
        "\n",
        "We're going to be leveraging the [RAGAS]() framework for our evaluations today as it's becoming a standard method of evaluating (at least directionally) RAG systems.\n",
        "\n",
        "We're also going to discuss a few more powerful Retrieval Systems that can potentially improve the quality of our generations!\n",
        "\n",
        "Let's start as we always do: Grabbing our dependencies!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5BN13TZlSCv4",
        "outputId": "5731c52b-311b-4191-b550-551e745c47df"
      },
      "outputs": [],
      "source": [
        "!pip install -U -q langchain openai ragas arxiv pymupdf chromadb wandb tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Lhqp5rUThG-",
        "outputId": "98908c24-9570-4766-99e1-3d91dad465ba"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "import openai\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DV_BOewX8CW0"
      },
      "source": [
        "### Data Collection\n",
        "\n",
        "We're going to be using papers from Arxiv as our context today.\n",
        "\n",
        "We can collect these documents rather straightforwardly with the `ArxivLoader` document loader from LangChain.\n",
        "\n",
        "Let's grab and load 5 documents.\n",
        "\n",
        "- [`ArxivLoader`](https://api.python.langchain.com/en/latest/document_loaders/langchain.document_loaders.arxiv.ArxivLoader.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DTDNFXaBSO2j",
        "outputId": "d57f54ac-010a-4f17-b990-3d3e7a2eee8c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain.document_loaders import ArxivLoader\n",
        "\n",
        "base_docs = ArxivLoader(query=\"Retrieval Augmented Generation\", load_max_docs=5).load()\n",
        "len(base_docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nNPAWPgNSyGP",
        "outputId": "bce2ebbf-201a-4993-9824-1f46352a3de6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'Published': '2022-02-13', 'Title': 'A Survey on Retrieval-Augmented Text Generation', 'Authors': 'Huayang Li, Yixuan Su, Deng Cai, Yan Wang, Lemao Liu', 'Summary': 'Recently, retrieval-augmented text generation attracted increasing attention\\nof the computational linguistics community. Compared with conventional\\ngeneration models, retrieval-augmented text generation has remarkable\\nadvantages and particularly has achieved state-of-the-art performance in many\\nNLP tasks. This paper aims to conduct a survey about retrieval-augmented text\\ngeneration. It firstly highlights the generic paradigm of retrieval-augmented\\ngeneration, and then it reviews notable approaches according to different tasks\\nincluding dialogue response generation, machine translation, and other\\ngeneration tasks. Finally, it points out some important directions on top of\\nrecent methods to facilitate future research.'}\n",
            "{'Published': '2023-05-11', 'Title': 'Active Retrieval Augmented Generation', 'Authors': 'Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, Graham Neubig', 'Summary': 'Despite the remarkable ability of large language models (LMs) to comprehend\\nand generate language, they have a tendency to hallucinate and create factually\\ninaccurate output. Augmenting LMs by retrieving information from external\\nknowledge resources is one promising solution. Most existing\\nretrieval-augmented LMs employ a retrieve-and-generate setup that only\\nretrieves information once based on the input. This is limiting, however, in\\nmore general scenarios involving generation of long texts, where continually\\ngathering information throughout the generation process is essential. There\\nhave been some past efforts to retrieve information multiple times while\\ngenerating outputs, which mostly retrieve documents at fixed intervals using\\nthe previous context as queries. In this work, we provide a generalized view of\\nactive retrieval augmented generation, methods that actively decide when and\\nwhat to retrieve across the course of the generation. We propose\\nForward-Looking Active REtrieval augmented generation (FLARE), a generic\\nretrieval-augmented generation method which iteratively uses a prediction of\\nthe upcoming sentence to anticipate future content, which is then utilized as a\\nquery to retrieve relevant documents to regenerate the sentence if it contains\\nlow-confidence tokens. We test FLARE along with baselines comprehensively over\\n4 long-form knowledge-intensive generation tasks/datasets. FLARE achieves\\nsuperior or competitive performance on all tasks, demonstrating the\\neffectiveness of our method. Code and datasets are available at\\nhttps://github.com/jzbjyb/FLARE.'}\n",
            "{'Published': '2023-08-08', 'Title': 'Hybrid Retrieval-Augmented Generation for Real-time Composition Assistance', 'Authors': 'Xuchao Zhang, Menglin Xia, Camille Couturier, Guoqing Zheng, Saravan Rajmohan, Victor Ruhle', 'Summary': \"Retrieval augmented models show promise in enhancing traditional language\\nmodels by improving their contextual understanding, integrating private data,\\nand reducing hallucination. However, the processing time required for retrieval\\naugmented large language models poses a challenge when applying them to tasks\\nthat require real-time responses, such as composition assistance.\\n  To overcome this limitation, we propose the Hybrid Retrieval-Augmented\\nGeneration (HybridRAG) framework that leverages a hybrid setting that combines\\nboth client and cloud models. HybridRAG incorporates retrieval-augmented memory\\ngenerated asynchronously by a Large Language Model (LLM) in the cloud. By\\nintegrating this retrieval augmented memory, the client model acquires the\\ncapability to generate highly effective responses, benefiting from the LLM's\\ncapabilities. Furthermore, through asynchronous memory integration, the client\\nmodel is capable of delivering real-time responses to user requests without the\\nneed to wait for memory synchronization from the cloud. Our experiments on\\nWikitext and Pile subsets show that HybridRAG achieves lower latency than a\\ncloud-based retrieval-augmented LLM, while outperforming client-only models in\\nutility.\"}\n",
            "{'Published': '2023-05-27', 'Title': 'Augmentation-Adapted Retriever Improves Generalization of Language Models as Generic Plug-In', 'Authors': 'Zichun Yu, Chenyan Xiong, Shi Yu, Zhiyuan Liu', 'Summary': \"Retrieval augmentation can aid language models (LMs) in knowledge-intensive\\ntasks by supplying them with external information. Prior works on retrieval\\naugmentation usually jointly fine-tune the retriever and the LM, making them\\nclosely coupled. In this paper, we explore the scheme of generic retrieval\\nplug-in: the retriever is to assist target LMs that may not be known beforehand\\nor are unable to be fine-tuned together. To retrieve useful documents for\\nunseen target LMs, we propose augmentation-adapted retriever (AAR), which\\nlearns LM's preferences obtained from a known source LM. Experiments on the\\nMMLU and PopQA datasets demonstrate that our AAR trained with a small source LM\\nis able to significantly improve the zero-shot generalization of larger target\\nLMs ranging from 250M Flan-T5 to 175B InstructGPT. Further analysis indicates\\nthat the preferences of different LMs overlap, enabling AAR trained with a\\nsingle source LM to serve as a generic plug-in for various target LMs. Our code\\nis open-sourced at https://github.com/OpenMatch/Augmentation-Adapted-Retriever.\"}\n",
            "{'Published': '2023-02-07', 'Title': 'Augmenting Zero-Shot Dense Retrievers with Plug-in Mixture-of-Memories', 'Authors': 'Suyu Ge, Chenyan Xiong, Corby Rosset, Arnold Overwijk, Jiawei Han, Paul Bennett', 'Summary': 'In this paper we improve the zero-shot generalization ability of language\\nmodels via Mixture-Of-Memory Augmentation (MoMA), a mechanism that retrieves\\naugmentation documents from multiple information corpora (\"external memories\"),\\nwith the option to \"plug in\" new memory at inference time. We develop a joint\\nlearning mechanism that trains the augmentation component with latent labels\\nderived from the end retrieval task, paired with hard negatives from the memory\\nmixture. We instantiate the model in a zero-shot dense retrieval setting by\\naugmenting a strong T5-based retriever with MoMA. Our model, MoMA, obtains\\nstrong zero-shot retrieval accuracy on the eighteen tasks included in the\\nstandard BEIR benchmark. It outperforms systems that seek generalization from\\nincreased model parameters and computation steps. Our analysis further\\nillustrates the necessity of augmenting with mixture-of-memory for robust\\ngeneralization, the benefits of augmentation learning, and how MoMA utilizes\\nthe plug-in memory at inference time without changing its parameters. We plan\\nto open source our code.'}\n"
          ]
        }
      ],
      "source": [
        "for doc in base_docs:\n",
        "  print(doc.metadata)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7ht6bJX9PAY"
      },
      "source": [
        "### Creating an Index\n",
        "\n",
        "Let's use a naive index creation strategy of just using `RecursiveCharacterTextSplitter` on our documents and embedding each into our `VectorStore` using `OpenAIEmbeddings()`.\n",
        "\n",
        "Let's use a rather generic 500 character chunk size.\n",
        "\n",
        "- [`RecursiveCharacterTextSplitter()`](https://api.python.langchain.com/en/latest/text_splitter/langchain.text_splitter.RecursiveCharacterTextSplitter.html)\n",
        "- [`Chroma`](https://api.python.langchain.com/en/latest/vectorstores/langchain.vectorstores.chroma.Chroma.html?highlight=chroma#langchain.vectorstores.chroma.Chroma)\n",
        "- [`OpenAIEmbeddings()`](https://api.python.langchain.com/en/latest/embeddings/langchain.embeddings.openai.OpenAIEmbeddings.html?highlight=openaiembeddings#langchain-embeddings-openai-openaiembeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "xne8P5dQTUiR"
      },
      "outputs": [],
      "source": [
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 1000,\n",
        "                                               chunk_overlap = 100,\n",
        "                                               length_function = len)\n",
        "\n",
        "\n",
        "docs = text_splitter.split_documents(base_docs)\n",
        "\n",
        "vectorstore = Chroma.from_documents(docs, OpenAIEmbeddings())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cnRzYx4c_2mZ",
        "outputId": "977c1b18-d6de-4d78-8a3f-78398398fd1c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "357"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WyUh8EVI_6TZ",
        "outputId": "30ba5ab0-64c3-46a8-c2fd-2b2d3c9b63cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "998\n"
          ]
        }
      ],
      "source": [
        "print(max([len(chunk.page_content) for chunk in docs]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYHnPaXl-cvJ"
      },
      "source": [
        "### Setting Up our Basic QA Chain\n",
        "\n",
        "Now we can instantiate our basic `RetrievalQA` chain, let's retrieve the top `k=3` documents.\n",
        "\n",
        "- [`RetrievalQA`](https://api.python.langchain.com/en/latest/chains/langchain.chains.retrieval_qa.base.RetrievalQA.html?highlight=retrievalqa#langchain-chains-retrieval-qa-base-retrievalqa)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "-TsjUWjbUfbW"
      },
      "outputs": [],
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "primary_qa_llm = ChatOpenAI(\n",
        "    model_name=\"gpt-3.5-turbo-16k\", \n",
        "    temperature=0\n",
        ")\n",
        "\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm = primary_qa_llm,\n",
        "    retriever=vectorstore.as_retriever(),\n",
        "    return_source_documents=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zO69de-F-oMD"
      },
      "source": [
        "Let's test it out!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2FS5NxC6UyU2",
        "outputId": "a4e54555-6201-41c0-9cef-2468894bab2d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RAG stands for Retrieval-Augmented Generation. It is a framework that combines retrieval and generation mechanisms to enhance language models. In the context of the given text, HybridRAG is a specific implementation of the RAG framework for real-time composition assistance. It leverages a hybrid setting that combines both client and cloud models to generate retrieval-augmented memory asynchronously.\n"
          ]
        }
      ],
      "source": [
        "query = \"What is RAG?\"\n",
        "\n",
        "result = qa_chain({\"query\" : query})\n",
        "\n",
        "print(result[\"result\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wyazkAIu85KL"
      },
      "source": [
        "### Ground Truth Dataset Creation Using GPT-3.5-turbo and GPT-4\n",
        "\n",
        "The next section might take you a long time to run, so the evaluation dataset is provided.\n",
        "\n",
        "The basic idea is that we can use LangChain to create questions based on our contexts, and then answer those questions.\n",
        "\n",
        "Let's look at how that works in the code!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "WB8gSL4CZoSC"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "texts = docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "V24T_gpPatAO"
      },
      "outputs": [],
      "source": [
        "from langchain.output_parsers import ResponseSchema\n",
        "from langchain.output_parsers import StructuredOutputParser\n",
        "\n",
        "question_schema = ResponseSchema(\n",
        "    name=\"question\",\n",
        "    description=\"a question about the context.\"\n",
        ")\n",
        "\n",
        "question_response_schemas = [\n",
        "    question_schema,\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "ebbmazGrdPap"
      },
      "outputs": [],
      "source": [
        "question_output_parser = StructuredOutputParser.from_response_schemas(question_response_schemas)\n",
        "format_instructions = question_output_parser.get_format_instructions()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "oPqC1_MXdRuh"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "qa_template = \"\"\"\\\n",
        "You are a University Professor creating a test for advanced students. For each context, create a question that is specific to the context. Avoid creating generic or general questions.\n",
        "\n",
        "question: a question about the context.\n",
        "\n",
        "Format the output as JSON with the following keys:\n",
        "question\n",
        "\n",
        "context: {context}\n",
        "\"\"\"\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_template(template=qa_template)\n",
        "\n",
        "messages = prompt_template.format_messages(\n",
        "    context=texts[0],\n",
        "    format_instructions=format_instructions\n",
        ")\n",
        "\n",
        "response = primary_qa_llm(messages)\n",
        "output_dict = question_output_parser.parse(response.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aKFw9kyZd7eB",
        "outputId": "cf125b12-403b-4620-c5d5-a5be28f72dc3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question\n",
            "What are the advantages of retrieval-augmented text generation compared to conventional generation models?\n"
          ]
        }
      ],
      "source": [
        "for k, v in output_dict.items():\n",
        "  print(k)\n",
        "  print(v)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dtASDdhLfd89",
        "outputId": "d2a942b6-cfa2-49d2-b0e8-860c509e417f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install -q -U tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Zolpr3CYeEYm"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [00:57<00:00,  5.73s/it]\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "qac_triples = []\n",
        "\n",
        "for text in tqdm(texts[:10]):\n",
        "  messages = prompt_template.format_messages(\n",
        "      context=text,\n",
        "      format_instructions=format_instructions\n",
        "  )\n",
        "  response = primary_qa_llm(messages)\n",
        "  try:\n",
        "    output_dict = question_output_parser.parse(response.content)\n",
        "  except Exception as e:\n",
        "    continue\n",
        "  output_dict[\"context\"] = text\n",
        "  qac_triples.append(output_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "vNB9Z2DrX2TC"
      },
      "outputs": [],
      "source": [
        "primary_ground_truth_llm = ChatOpenAI(model_name=\"gpt-4\", temperature=0)\n",
        "\n",
        "answer_schema = ResponseSchema(\n",
        "    name=\"answer\",\n",
        "    description=\"an answer to the question\"\n",
        ")\n",
        "\n",
        "answer_response_schemas = [\n",
        "    answer_schema,\n",
        "]\n",
        "\n",
        "answer_output_parser = StructuredOutputParser.from_response_schemas(answer_response_schemas)\n",
        "format_instructions = answer_output_parser.get_format_instructions()\n",
        "\n",
        "qa_template = \"\"\"\\\n",
        "You are a University Professor creating a test for advanced students. For each question and context, create an answer.\n",
        "\n",
        "answer: a question about the context.\n",
        "\n",
        "Format the output as JSON with the following keys:\n",
        "answer\n",
        "\n",
        "question: {question}\n",
        "context: {context}\n",
        "\"\"\"\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_template(template=qa_template)\n",
        "\n",
        "messages = prompt_template.format_messages(\n",
        "    context=qac_triples[0][\"context\"],\n",
        "    question=qac_triples[0][\"question\"],\n",
        "    format_instructions=format_instructions\n",
        ")\n",
        "\n",
        "response = primary_ground_truth_llm(messages)\n",
        "output_dict = answer_output_parser.parse(response.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rk-_lRR6fn5U",
        "outputId": "ed311efd-16c5-4590-82bc-2e139fc7e238"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "answer\n",
            "Retrieval-augmented text generation has remarkable advantages over conventional generation models, particularly achieving state-of-the-art performance in many NLP tasks. It follows a generic paradigm and has been applied successfully in different tasks including dialogue response generation, machine translation, and other generation tasks.\n"
          ]
        }
      ],
      "source": [
        "for k, v in output_dict.items():\n",
        "  print(k)\n",
        "  print(v)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "yCdH0e9rrAKd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [00:54<00:00,  5.41s/it]\n"
          ]
        }
      ],
      "source": [
        "for triple in tqdm(qac_triples):\n",
        "  messages = prompt_template.format_messages(\n",
        "      context=triple[\"context\"],\n",
        "      question=triple[\"question\"],\n",
        "      format_instructions=format_instructions\n",
        "  )\n",
        "  response = primary_ground_truth_llm(messages)\n",
        "  try:\n",
        "    output_dict = answer_output_parser.parse(response.content)\n",
        "  except Exception as e:\n",
        "    continue\n",
        "  triple[\"answer\"] = output_dict[\"answer\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "rrHXgH9Qs1ep"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install -q -U datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "uAvGGTyXsoHQ"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/homebrew/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "\n",
        "ground_truth_qac_set = pd.DataFrame(qac_triples)\n",
        "ground_truth_qac_set[\"context\"] = ground_truth_qac_set[\"context\"].map(lambda x: str(x.page_content))\n",
        "ground_truth_qac_set = ground_truth_qac_set.rename(columns={\"answer\" : \"ground_truth\"})\n",
        "\n",
        "\n",
        "eval_dataset = Dataset.from_pandas(ground_truth_qac_set)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "9459643d14994e269ec3897e5b071029",
            "285688f9e00548388c923b0bc2bc38fd",
            "8df3b60cdb35424da81299c607e17d0f",
            "7f925c5a99194d4daec387f3847337b8",
            "349dbc81cb3843caba834cd940e4d41c",
            "4316578e814246358ad08c55d80e44df",
            "9ddeea42439a49fd9fe2c310e8facb07",
            "5d6474260ff648adbadf28e8e0839fa2",
            "db3e3d58eeda4567964682c366157086",
            "ff4ca4f11c3843809b0fb2c194b58327",
            "8d4ac912eaf94f6c8e013977133a4906"
          ]
        },
        "id": "Nhp5X4M8zqrm",
        "outputId": "1190ec42-5c1d-4216-b734-e7c8dc7ee1cf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Creating CSV from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 112.49ba/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "16039"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "eval_dataset.to_csv(\"groundtruth_eval_dataset.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Al5cagr-rvL"
      },
      "source": [
        "### Evaluating RAG Pipelines\n",
        "\n",
        "If you skipped ahead and need to load the `.csv` directly - uncomment the code below.\n",
        "\n",
        "If you're using Colab to do this notebook - please ensure you add it to your session files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QJhes58R66-P"
      },
      "outputs": [],
      "source": [
        "# from datasets import Dataset\n",
        "# eval_dataset = Dataset.from_csv(\"groundtruth_eval_dataset.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5fAD8c_kthWc",
        "outputId": "35e579c6-b184-456f-84e9-83a39e0408ac"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['question', 'context', 'ground_truth', 'metadata'],\n",
              "    num_rows: 10\n",
              "})"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "eval_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IqFYbjLK-6X7"
      },
      "source": [
        "### Evaluation Using RAGAS\n",
        "\n",
        "Now we can evaluate using RAGAS!\n",
        "\n",
        "The set-up is fairly straightforward - we simply need to create a dataset with our generated answers and our contexts, and then evaluate using the framework.\n",
        "\n",
        "More details on the specific metrics can be found [here](https://github.com/explodinggradients/ragas/blob/main/docs/metrics.md)!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting ragas\n",
            "  Using cached ragas-0.0.16-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: numpy in /opt/homebrew/lib/python3.11/site-packages (from ragas) (1.26.0)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.34.0-py3-none-any.whl (7.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting sentence-transformers\n",
            "  Using cached sentence_transformers-2.2.2-py3-none-any.whl\n",
            "Requirement already satisfied: datasets in /opt/homebrew/lib/python3.11/site-packages (from ragas) (2.14.5)\n",
            "Requirement already satisfied: tiktoken in /opt/homebrew/lib/python3.11/site-packages (from ragas) (0.5.1)\n",
            "Requirement already satisfied: langchain>=0.0.288 in /opt/homebrew/lib/python3.11/site-packages (from ragas) (0.0.304)\n",
            "Requirement already satisfied: openai in /opt/homebrew/lib/python3.11/site-packages (from ragas) (0.28.1)\n",
            "Collecting pydantic<2.0\n",
            "  Using cached pydantic-1.10.13-cp311-cp311-macosx_11_0_arm64.whl (2.5 MB)\n",
            "Collecting pysbd>=0.3.4\n",
            "  Using cached pysbd-0.3.4-py3-none-any.whl (71 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /opt/homebrew/lib/python3.11/site-packages (from langchain>=0.0.288->ragas) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/homebrew/lib/python3.11/site-packages (from langchain>=0.0.288->ragas) (2.0.21)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/homebrew/lib/python3.11/site-packages (from langchain>=0.0.288->ragas) (3.8.5)\n",
            "Requirement already satisfied: anyio<4.0 in /opt/homebrew/lib/python3.11/site-packages (from langchain>=0.0.288->ragas) (3.7.1)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /opt/homebrew/lib/python3.11/site-packages (from langchain>=0.0.288->ragas) (0.6.1)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/homebrew/lib/python3.11/site-packages (from langchain>=0.0.288->ragas) (1.33)\n",
            "Requirement already satisfied: langsmith<0.1.0,>=0.0.38 in /opt/homebrew/lib/python3.11/site-packages (from langchain>=0.0.288->ragas) (0.0.41)\n",
            "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /opt/homebrew/lib/python3.11/site-packages (from langchain>=0.0.288->ragas) (2.8.7)\n",
            "Requirement already satisfied: requests<3,>=2 in /opt/homebrew/lib/python3.11/site-packages (from langchain>=0.0.288->ragas) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /opt/homebrew/lib/python3.11/site-packages (from langchain>=0.0.288->ragas) (8.2.3)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /opt/homebrew/lib/python3.11/site-packages (from pydantic<2.0->ragas) (4.8.0)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /opt/homebrew/lib/python3.11/site-packages (from datasets->ragas) (13.0.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /opt/homebrew/lib/python3.11/site-packages (from datasets->ragas) (0.3.7)\n",
            "Requirement already satisfied: pandas in /opt/homebrew/lib/python3.11/site-packages (from datasets->ragas) (2.1.1)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /opt/homebrew/lib/python3.11/site-packages (from datasets->ragas) (4.66.1)\n",
            "Requirement already satisfied: xxhash in /opt/homebrew/lib/python3.11/site-packages (from datasets->ragas) (3.3.0)\n",
            "Requirement already satisfied: multiprocess in /opt/homebrew/lib/python3.11/site-packages (from datasets->ragas) (0.70.15)\n",
            "Requirement already satisfied: fsspec[http]<2023.9.0,>=2023.1.0 in /opt/homebrew/lib/python3.11/site-packages (from datasets->ragas) (2023.6.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /opt/homebrew/lib/python3.11/site-packages (from datasets->ragas) (0.16.4)\n",
            "Requirement already satisfied: packaging in /Users/mertbozkir/Library/Python/3.11/lib/python/site-packages (from datasets->ragas) (23.1)\n",
            "Collecting torch>=1.6.0\n",
            "  Using cached torch-2.0.1-cp311-none-macosx_11_0_arm64.whl (55.8 MB)\n",
            "Collecting torchvision\n",
            "  Using cached torchvision-0.15.2-cp311-cp311-macosx_11_0_arm64.whl (1.4 MB)\n",
            "Collecting scikit-learn\n",
            "  Using cached scikit_learn-1.3.1-cp311-cp311-macosx_12_0_arm64.whl (9.4 MB)\n",
            "Collecting scipy\n",
            "  Using cached scipy-1.11.3-cp311-cp311-macosx_12_0_arm64.whl (29.7 MB)\n",
            "Collecting nltk\n",
            "  Using cached nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
            "Collecting sentencepiece\n",
            "  Using cached sentencepiece-0.1.99-cp311-cp311-macosx_11_0_arm64.whl (1.2 MB)\n",
            "Requirement already satisfied: filelock in /opt/homebrew/lib/python3.11/site-packages (from transformers->ragas) (3.12.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /opt/homebrew/lib/python3.11/site-packages (from transformers->ragas) (2023.8.8)\n",
            "Requirement already satisfied: tokenizers<0.15,>=0.14 in /opt/homebrew/lib/python3.11/site-packages (from transformers->ragas) (0.14.0)\n",
            "Collecting safetensors>=0.3.1\n",
            "  Using cached safetensors-0.3.3-cp311-cp311-macosx_13_0_arm64.whl (406 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /opt/homebrew/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.288->ragas) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/homebrew/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.288->ragas) (3.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/homebrew/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.288->ragas) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/homebrew/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.288->ragas) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/homebrew/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.288->ragas) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /opt/homebrew/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.288->ragas) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /opt/homebrew/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.288->ragas) (1.3.1)\n",
            "Requirement already satisfied: idna>=2.8 in /opt/homebrew/lib/python3.11/site-packages (from anyio<4.0->langchain>=0.0.288->ragas) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /opt/homebrew/lib/python3.11/site-packages (from anyio<4.0->langchain>=0.0.288->ragas) (1.3.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/homebrew/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain>=0.0.288->ragas) (3.20.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/homebrew/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain>=0.0.288->ragas) (0.9.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /opt/homebrew/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain>=0.0.288->ragas) (2.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/lib/python3.11/site-packages (from requests<3,>=2->langchain>=0.0.288->ragas) (2.0.5)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/lib/python3.11/site-packages (from requests<3,>=2->langchain>=0.0.288->ragas) (2023.7.22)\n",
            "Requirement already satisfied: sympy in /opt/homebrew/lib/python3.11/site-packages (from torch>=1.6.0->sentence-transformers->ragas) (1.12)\n",
            "Collecting networkx\n",
            "  Using cached networkx-3.1-py3-none-any.whl (2.1 MB)\n",
            "Collecting jinja2\n",
            "  Using cached Jinja2-3.1.2-py3-none-any.whl (133 kB)\n",
            "Requirement already satisfied: click in /opt/homebrew/lib/python3.11/site-packages (from nltk->sentence-transformers->ragas) (8.1.7)\n",
            "Collecting joblib\n",
            "  Using cached joblib-1.3.2-py3-none-any.whl (302 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/mertbozkir/Library/Python/3.11/lib/python/site-packages (from pandas->datasets->ragas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /opt/homebrew/lib/python3.11/site-packages (from pandas->datasets->ragas) (2023.3.post1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /opt/homebrew/lib/python3.11/site-packages (from pandas->datasets->ragas) (2023.3)\n",
            "Collecting threadpoolctl>=2.0.0\n",
            "  Using cached threadpoolctl-3.2.0-py3-none-any.whl (15 kB)\n",
            "Collecting pillow!=8.3.*,>=5.3.0\n",
            "  Using cached Pillow-10.0.1-cp311-cp311-macosx_11_0_arm64.whl (3.3 MB)\n",
            "Requirement already satisfied: six>=1.5 in /opt/homebrew/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets->ragas) (1.16.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/homebrew/lib/python3.11/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain>=0.0.288->ragas) (1.0.0)\n",
            "Collecting MarkupSafe>=2.0\n",
            "  Using cached MarkupSafe-2.1.3-cp311-cp311-macosx_10_9_universal2.whl (17 kB)\n",
            "Requirement already satisfied: mpmath>=0.19 in /opt/homebrew/lib/python3.11/site-packages (from sympy->torch>=1.6.0->sentence-transformers->ragas) (1.3.0)\n",
            "Installing collected packages: sentencepiece, safetensors, threadpoolctl, scipy, pysbd, pydantic, pillow, networkx, MarkupSafe, joblib, scikit-learn, nltk, jinja2, torch, transformers, torchvision, sentence-transformers, ragas\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 2.4.2\n",
            "    Uninstalling pydantic-2.4.2:\n",
            "      Successfully uninstalled pydantic-2.4.2\n",
            "Successfully installed MarkupSafe-2.1.3 jinja2-3.1.2 joblib-1.3.2 networkx-3.1 nltk-3.8.1 pillow-10.0.1 pydantic-1.10.13 pysbd-0.3.4 ragas-0.0.16 safetensors-0.3.3 scikit-learn-1.3.1 scipy-1.11.3 sentence-transformers-2.2.2 sentencepiece-0.1.99 threadpoolctl-3.2.0 torch-2.0.1 torchvision-0.15.2 transformers-4.34.0\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install ragas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "1eBoHaf5t4w8"
      },
      "outputs": [],
      "source": [
        "from ragas.metrics import (\n",
        "    answer_relevancy,\n",
        "    faithfulness,\n",
        "    context_recall,\n",
        "    context_precision,\n",
        ")\n",
        "from ragas.metrics.critique import harmfulness\n",
        "from ragas import evaluate\n",
        "\n",
        "def create_ragas_dataset(rag_pipeline, eval_dataset):\n",
        "  rag_dataset = []\n",
        "  for row in tqdm(eval_dataset):\n",
        "    answer = rag_pipeline({\"query\" : row[\"question\"]})\n",
        "    rag_dataset.append(\n",
        "        {\"question\" : row[\"question\"],\n",
        "         \"answer\" : answer[\"result\"],\n",
        "         \"contexts\" : [context.page_content for context in answer[\"source_documents\"]],\n",
        "         \"ground_truths\" : [row[\"ground_truth\"]]\n",
        "         }\n",
        "    )\n",
        "  rag_df = pd.DataFrame(rag_dataset)\n",
        "  rag_eval_dataset = Dataset.from_pandas(rag_df)\n",
        "  return rag_eval_dataset\n",
        "\n",
        "def evaluate_ragas_dataset(ragas_dataset):\n",
        "  result = evaluate(\n",
        "    ragas_dataset,\n",
        "    metrics=[\n",
        "        context_precision,\n",
        "        faithfulness,\n",
        "        answer_relevancy,\n",
        "        context_recall,\n",
        "    ],\n",
        "  )\n",
        "  return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4c4Jd8G_lXY"
      },
      "source": [
        "Lets create our dataset first:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R7oXgcjkuopO",
        "outputId": "62d20b9f-fb45-4c31-b818-523e3a23561d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [00:50<00:00,  5.10s/it]\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "\n",
        "basic_qa_ragas_dataset = create_ragas_dataset(\n",
        "    qa_chain,\n",
        "    eval_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Obbgw3im_n01"
      },
      "source": [
        "Save it for later:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "79e167d43da549c292c2573c782581b2",
            "700f1814c1a14134b2d12a54d018416d",
            "eeb1186495bb4580af9e99ba60c3ddd7",
            "9362cbc2937c40eb821d05afd7cdef77",
            "4fc6ddfce74a4957bdb03822d9130cb9",
            "ad146de916d1474ca3bf252d1c68ca6f",
            "6b55cf5bf0d14e7ba1384b1faad82f62",
            "36183c3078ac4be8a0fd48de8c00ff09",
            "21238add60ea411e937a445c154efaa7",
            "3ce331032ea44ea2add34ae1c04002c5",
            "18dbdffae47e407da2824cccb96bb27b"
          ]
        },
        "id": "6FG8x4i3yZ2B",
        "outputId": "0d4f62b3-5f4f-4121-8a81-cff3f1c3f37d"
      },
      "outputs": [],
      "source": [
        "basic_qa_ragas_dataset.to_csv(\n",
        "    ### YOUR CODE HERE\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A5I_d_RT_pFr"
      },
      "source": [
        "And finally - evaluate how it did!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ywp3Rwavy9pc",
        "outputId": "4934d132-a653-42ef-b6f9-cc2e8148baf4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading (…)lve/main/config.json: 100%|██████████| 647/647 [00:00<00:00, 2.85MB/s]\n",
            "Downloading pytorch_model.bin: 100%|██████████| 57.4M/57.4M [00:10<00:00, 5.26MB/s]\n",
            "Downloading (…)okenizer_config.json: 100%|██████████| 517/517 [00:00<00:00, 2.54MB/s]\n",
            "Downloading (…)solve/main/vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 875kB/s]\n",
            "Downloading (…)cial_tokens_map.json: 100%|██████████| 112/112 [00:00<00:00, 1.03MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "evaluating with [context_relevancy]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/1 [00:00<?, ?it/s]WARNING:root:The 'context_relevancy' metric is going to be deprecated soon! Please use the 'context_precision' metric instead. It is a drop-in replacement just a simple search and replace should work.\n",
            "100%|██████████| 1/1 [00:37<00:00, 37.25s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "evaluating with [faithfulness]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [01:48<00:00, 108.23s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "evaluating with [answer_relevancy]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:24<00:00, 24.76s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "evaluating with [context_recall]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:55<00:00, 55.29s/it]\n"
          ]
        }
      ],
      "source": [
        "basic_qa_result = evaluate_ragas_dataset(\n",
        "    basic_qa_ragas_dataset\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m4oYnKTn15gY",
        "outputId": "35fe5193-6ff0-47a3-b467-a82bcedbd487"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'ragas_score': 0.0799, 'context_relevancy': 0.0215, 'faithfulness': 0.7800, 'answer_relevancy': 0.9903, 'context_recall': 0.8250}"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "basic_qa_result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SwhBxlxYAdno"
      },
      "source": [
        "### Testing Other Retrievers\n",
        "\n",
        "Now we can test our how changing our Retriever impacts our RAGAS evaluation!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "qnfy4VNkzZi2"
      },
      "outputs": [],
      "source": [
        "def create_qa_chain(retriever):\n",
        "  primary_qa_llm = ChatOpenAI(model_name=\"gpt-3.5-turbo-16k\", temperature=0)\n",
        "\n",
        "  created_qa_chain = RetrievalQA.from_chain_type(\n",
        "      primary_qa_llm,\n",
        "      retriever=retriever,\n",
        "      return_source_documents=True\n",
        "  )\n",
        "\n",
        "  return created_qa_chain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOPp4Xq7AvEx"
      },
      "source": [
        "#### Parent Document Retriever\n",
        "\n",
        "One of the easier ways we can imagine improving a retriever is to embed our documents into small chunks, and then retrieve a significant amount of additional context that \"surrounds\" the found context.\n",
        "\n",
        "You can read more about this method [here](https://python.langchain.com/docs/modules/data_connection/retrievers/parent_document_retriever)!\n",
        "\n",
        "- [`ParentDocumentRetriever`](https://api.python.langchain.com/en/latest/retrievers/langchain.retrievers.parent_document_retriever.ParentDocumentRetriever.html?highlight=parentdocumentretriever#langchain-retrievers-parent-document-retriever-parentdocumentretriever)\n",
        "- [`InMemoryStore`](https://api.python.langchain.com/en/latest/retrievers/langchain.retrievers.parent_document_retriever.ParentDocumentRetriever.html?highlight=parentdocumentretriever#langchain-retrievers-parent-document-retriever-parentdocumentretriever)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "67I6QJAJ0Un7"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers import ParentDocumentRetriever\n",
        "from langchain.storage import InMemoryStore\n",
        "\n",
        "parent_splitter = RecursiveCharacterTextSplitter(chunk_size=2000)\n",
        "child_splitter = RecursiveCharacterTextSplitter(chunk_size=400)\n",
        "\n",
        "vectorstore = Chroma(\n",
        "    collection_name=\"split_parents\", \n",
        "    embedding_function= OpenAIEmbeddings()\n",
        ")\n",
        "\n",
        "store = InMemoryStore()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "zfk5RYUt00Pw"
      },
      "outputs": [],
      "source": [
        "parent_document_retriever = ParentDocumentRetriever(\n",
        "    vectorstore=vectorstore,\n",
        "    docstore=store,\n",
        "    child_splitter=child_splitter,\n",
        "    parent_splitter=parent_splitter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "68c1t4o104AK"
      },
      "outputs": [],
      "source": [
        "parent_document_retriever.add_documents(base_docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTH0MDolBndm"
      },
      "source": [
        "Let's create, test, and then evaluate our new chain!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "KMjLfqOC09Iw"
      },
      "outputs": [],
      "source": [
        "parent_document_retriever_qa_chain = create_qa_chain(parent_document_retriever)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "Rv8bAHPN1H4P",
        "outputId": "be9bbf18-b0e6-4a29-9f08-34cd4aa5fb6a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'RAG stands for Retrieval-Augmented Generation. It is an approach that combines retrieval and generation models to enhance the performance of language models. In the RAG framework, a retrieval model is used to retrieve relevant information from external documents, and this information is then used by the generation model to generate responses or completions.'"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "parent_document_retriever_qa_chain({\"query\" : \"What is RAG?\"})[\"result\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OQJRIQmU1WTw",
        "outputId": "6d51a5d2-2c1c-4604-8030-43f432c4aa84"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [00:48<00:00,  4.81s/it]\n"
          ]
        }
      ],
      "source": [
        "pdr_qa_ragas_dataset = create_ragas_dataset(parent_document_retriever_qa_chain, eval_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "467c2b45d8dd4a67a229d66ff2fc9bc7",
            "293bbbe3717c4c0899792535eaff8acf",
            "c523276907e648e2ba79ac03be49576c",
            "c4f448cab4ec493d86f279f7207a5ce8",
            "a78ccb267c854f3cb093a132ff8256d9",
            "df9133aad3b047aaa69b7d1da7eb19a9",
            "bc674f2c0ec84cecb99bed69ff80c0bf",
            "84b05599d12849dba867dd219d36d212",
            "a93c4c0ec57646f1b84355a16f8ba947",
            "1538f80d733844e0a5f6c71ae2bbf948",
            "a4d900bf76d14cbd8da6df656ec21fa0"
          ]
        },
        "id": "d9vfKnCL1jtB",
        "outputId": "5f67fdf4-20fb-4d7a-cde2-07caf6f6cce1"
      },
      "outputs": [],
      "source": [
        "pdr_qa_ragas_dataset.to_csv(\n",
        "    ### YOUR CODE HERE\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qfB1H9S_1mW3",
        "outputId": "453628e0-10f4-4aa2-bc73-86552d400806"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "evaluating with [context_relevancy]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/1 [00:00<?, ?it/s]WARNING:root:The 'context_relevancy' metric is going to be deprecated soon! Please use the 'context_precision' metric instead. It is a drop-in replacement just a simple search and replace should work.\n",
            "100%|██████████| 1/1 [00:48<00:00, 48.80s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "evaluating with [faithfulness]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [01:54<00:00, 114.55s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "evaluating with [answer_relevancy]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:28<00:00, 28.29s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "evaluating with [context_recall]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:43<00:00, 43.88s/it]\n"
          ]
        }
      ],
      "source": [
        "pdr_qa_result = evaluate_ragas_dataset(pdr_qa_ragas_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5nFyYCdL2Nco",
        "outputId": "707a8e3e-c808-4f55-dd94-2279a168da5b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'ragas_score': 0.0990, 'context_relevancy': 0.0270, 'faithfulness': 0.9067, 'answer_relevancy': 0.9924, 'context_recall': 0.8000}"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pdr_qa_result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JaNk6o7_BqX8"
      },
      "source": [
        "#### Ensemble Retrieval\n",
        "\n",
        "There are a number of excellent options to retrieve documents - we'll be looking at an additional example today, which is called the EnsembleRetriever.\n",
        "\n",
        "The method this is using is outlined in [this paper](https://plg.uwaterloo.ca/~gvcormac/cormacksigir09-rrf.pdf).\n",
        "\n",
        "The brief explanation is:\n",
        "\n",
        "- We collect results from two different retrieval methods over the same corpus\n",
        "- We apply a reranking algorithm to rerank our source documents to be the most relevant without losing specific or potentially low-ranked information rich documents\n",
        "- We feed the top-k results into the LLM with our query as context.\n",
        "\n",
        "> HINT: Your weight list should be of type List[float] and the sum(List[float]) should be 1.\n",
        "\n",
        "We'll be leveraging the following tools:\n",
        "\n",
        "- [`BM25Retriever`](https://api.python.langchain.com/en/latest/retrievers/langchain.retrievers.bm25.BM25Retriever.html)\n",
        "- [`EnsembleRetriever`](https://api.python.langchain.com/en/latest/retrievers/langchain.retrievers.ensemble.EnsembleRetriever.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### High Level Diagram\n",
        "\n",
        "Leverages the [RRF](https://plg.uwaterloo.ca/~gvcormac/cormacksigir09-rrf.pdf) reranking algorithm to combine sparse and dense search results for increased effectiveness for relevant document retrieval.\n",
        "\n",
        "![image](https://i.imgur.com/mn4jXAz.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "zz7dl1GD5-L-"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting rank_bm25\n",
            "  Using cached rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)\n",
            "Requirement already satisfied: numpy in /opt/homebrew/lib/python3.11/site-packages (from rank_bm25) (1.26.0)\n",
            "Installing collected packages: rank_bm25\n",
            "Successfully installed rank_bm25-0.2.2\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install rank_bm25"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "Vs8wxT9b5pRA"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 1000, chunk_overlap = 200, length_function = len)\n",
        "docs = text_splitter.split_documents(base_docs\n",
        "    \n",
        ")\n",
        "\n",
        "bm25_retriever = BM25Retriever.from_documents(\n",
        "    docs\n",
        ")\n",
        "bm25_retriever.k = 4\n",
        "\n",
        "embedding = OpenAIEmbeddings()\n",
        "vectorstore = Chroma.from_documents(docs, embedding)\n",
        "chroma_retriever = vectorstore.as_retriever()\n",
        "\n",
        "ensemble_retriever = EnsembleRetriever(retrievers=[chroma_retriever, bm25_retriever])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "cv69YDpF6PrJ"
      },
      "outputs": [],
      "source": [
        "ensemble_retriever_qa_chain = create_qa_chain(ensemble_retriever)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "6lSszzrf6UmP",
        "outputId": "a72a74bf-be4d-492e-ebeb-bb6ee619cf53"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'RAG stands for Retrieval-Augmented Generation. It is a framework that combines retrieval-based models with generation-based models to enhance their performance in natural language processing tasks. The retrieval component helps improve contextual understanding and reduces hallucination, while the generation component allows for creative and fluent text generation.'"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ensemble_retriever_qa_chain({\"query\" : \"What is RAG?\"})[\"result\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "abUgTGDT6UrV",
        "outputId": "41eb67ab-06c4-495a-c401-7b0fc909e705"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [00:48<00:00,  4.89s/it]\n"
          ]
        }
      ],
      "source": [
        "ensemble_qa_ragas_dataset = create_ragas_dataset(ensemble_retriever_qa_chain, eval_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "f207234b51b647fd8219b5ffc9c39bd6",
            "971e27539ecb42718f3c31bf14d855fc",
            "6647629812ba4652b6b6c8f3400cb79f",
            "3e638938eb934c8db1f970675eb4a0d7",
            "6d228f73d99247048af148f154c7fed3",
            "5476ebd8683747e387ef875cfeed2ee5",
            "59f0fe93f0e5478098ababace0670e80",
            "a5d5c3a0cbd14e97aed63260d032c41f",
            "394d86f3b82e414193e96d49da77a0d5",
            "8ddc0f7dfdb24fc6932dfc589cf056d9",
            "91b8e7e80ffb4897983b607f3165033c"
          ]
        },
        "id": "bGHipYsf7phk",
        "outputId": "c890d308-6b66-4f01-9919-4e3970283321"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Creating CSV from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 186.30ba/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "70941"
            ]
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ensemble_qa_ragas_dataset.to_csv(\"ensemble_qa_ragas_dataset.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ozo0jkvx7r1d",
        "outputId": "6356da39-9983-4f12-f8ef-9fce34aa9573"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "evaluating with [context_relevancy]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/1 [00:00<?, ?it/s]WARNING:root:The 'context_relevancy' metric is going to be deprecated soon! Please use the 'context_precision' metric instead. It is a drop-in replacement just a simple search and replace should work.\n",
            "100%|██████████| 1/1 [00:44<00:00, 44.04s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "evaluating with [faithfulness]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [01:29<00:00, 89.57s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "evaluating with [answer_relevancy]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:19<00:00, 19.44s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "evaluating with [context_recall]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:39<00:00, 39.85s/it]\n"
          ]
        }
      ],
      "source": [
        "ensemble_qa_result = evaluate_ragas_dataset(ensemble_qa_ragas_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hvabdcbh793a",
        "outputId": "40263758-cb97-42c0-864a-ab9d37e8bd92"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'ragas_score': 0.0800, 'context_relevancy': 0.0214, 'faithfulness': 0.8667, 'answer_relevancy': 0.9636, 'context_recall': 0.8833}"
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ensemble_qa_result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O4vXVWqiCcSI"
      },
      "source": [
        "### Conclusion\n",
        "\n",
        "Observe your results in a table!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g0Tu_GM_ESC5"
      },
      "outputs": [],
      "source": [
        "### YOUR CODE HERE"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1538f80d733844e0a5f6c71ae2bbf948": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "18dbdffae47e407da2824cccb96bb27b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "21238add60ea411e937a445c154efaa7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "285688f9e00548388c923b0bc2bc38fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4316578e814246358ad08c55d80e44df",
            "placeholder": "​",
            "style": "IPY_MODEL_9ddeea42439a49fd9fe2c310e8facb07",
            "value": "Creating CSV from Arrow format: 100%"
          }
        },
        "293bbbe3717c4c0899792535eaff8acf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_df9133aad3b047aaa69b7d1da7eb19a9",
            "placeholder": "​",
            "style": "IPY_MODEL_bc674f2c0ec84cecb99bed69ff80c0bf",
            "value": "Creating CSV from Arrow format: 100%"
          }
        },
        "349dbc81cb3843caba834cd940e4d41c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "36183c3078ac4be8a0fd48de8c00ff09": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "394d86f3b82e414193e96d49da77a0d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3ce331032ea44ea2add34ae1c04002c5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3e638938eb934c8db1f970675eb4a0d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8ddc0f7dfdb24fc6932dfc589cf056d9",
            "placeholder": "​",
            "style": "IPY_MODEL_91b8e7e80ffb4897983b607f3165033c",
            "value": " 1/1 [00:00&lt;00:00, 22.85ba/s]"
          }
        },
        "4316578e814246358ad08c55d80e44df": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "467c2b45d8dd4a67a229d66ff2fc9bc7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_293bbbe3717c4c0899792535eaff8acf",
              "IPY_MODEL_c523276907e648e2ba79ac03be49576c",
              "IPY_MODEL_c4f448cab4ec493d86f279f7207a5ce8"
            ],
            "layout": "IPY_MODEL_a78ccb267c854f3cb093a132ff8256d9"
          }
        },
        "4fc6ddfce74a4957bdb03822d9130cb9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5476ebd8683747e387ef875cfeed2ee5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "59f0fe93f0e5478098ababace0670e80": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5d6474260ff648adbadf28e8e0839fa2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6647629812ba4652b6b6c8f3400cb79f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a5d5c3a0cbd14e97aed63260d032c41f",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_394d86f3b82e414193e96d49da77a0d5",
            "value": 1
          }
        },
        "6b55cf5bf0d14e7ba1384b1faad82f62": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6d228f73d99247048af148f154c7fed3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "700f1814c1a14134b2d12a54d018416d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ad146de916d1474ca3bf252d1c68ca6f",
            "placeholder": "​",
            "style": "IPY_MODEL_6b55cf5bf0d14e7ba1384b1faad82f62",
            "value": "Creating CSV from Arrow format: 100%"
          }
        },
        "79e167d43da549c292c2573c782581b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_700f1814c1a14134b2d12a54d018416d",
              "IPY_MODEL_eeb1186495bb4580af9e99ba60c3ddd7",
              "IPY_MODEL_9362cbc2937c40eb821d05afd7cdef77"
            ],
            "layout": "IPY_MODEL_4fc6ddfce74a4957bdb03822d9130cb9"
          }
        },
        "7f925c5a99194d4daec387f3847337b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ff4ca4f11c3843809b0fb2c194b58327",
            "placeholder": "​",
            "style": "IPY_MODEL_8d4ac912eaf94f6c8e013977133a4906",
            "value": " 1/1 [00:00&lt;00:00, 19.62ba/s]"
          }
        },
        "84b05599d12849dba867dd219d36d212": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8d4ac912eaf94f6c8e013977133a4906": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8ddc0f7dfdb24fc6932dfc589cf056d9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8df3b60cdb35424da81299c607e17d0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5d6474260ff648adbadf28e8e0839fa2",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_db3e3d58eeda4567964682c366157086",
            "value": 1
          }
        },
        "91b8e7e80ffb4897983b607f3165033c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9362cbc2937c40eb821d05afd7cdef77": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3ce331032ea44ea2add34ae1c04002c5",
            "placeholder": "​",
            "style": "IPY_MODEL_18dbdffae47e407da2824cccb96bb27b",
            "value": " 1/1 [00:00&lt;00:00, 20.82ba/s]"
          }
        },
        "9459643d14994e269ec3897e5b071029": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_285688f9e00548388c923b0bc2bc38fd",
              "IPY_MODEL_8df3b60cdb35424da81299c607e17d0f",
              "IPY_MODEL_7f925c5a99194d4daec387f3847337b8"
            ],
            "layout": "IPY_MODEL_349dbc81cb3843caba834cd940e4d41c"
          }
        },
        "971e27539ecb42718f3c31bf14d855fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5476ebd8683747e387ef875cfeed2ee5",
            "placeholder": "​",
            "style": "IPY_MODEL_59f0fe93f0e5478098ababace0670e80",
            "value": "Creating CSV from Arrow format: 100%"
          }
        },
        "9ddeea42439a49fd9fe2c310e8facb07": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a4d900bf76d14cbd8da6df656ec21fa0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a5d5c3a0cbd14e97aed63260d032c41f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a78ccb267c854f3cb093a132ff8256d9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a93c4c0ec57646f1b84355a16f8ba947": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ad146de916d1474ca3bf252d1c68ca6f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bc674f2c0ec84cecb99bed69ff80c0bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c4f448cab4ec493d86f279f7207a5ce8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1538f80d733844e0a5f6c71ae2bbf948",
            "placeholder": "​",
            "style": "IPY_MODEL_a4d900bf76d14cbd8da6df656ec21fa0",
            "value": " 1/1 [00:00&lt;00:00, 27.01ba/s]"
          }
        },
        "c523276907e648e2ba79ac03be49576c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_84b05599d12849dba867dd219d36d212",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a93c4c0ec57646f1b84355a16f8ba947",
            "value": 1
          }
        },
        "db3e3d58eeda4567964682c366157086": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "df9133aad3b047aaa69b7d1da7eb19a9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eeb1186495bb4580af9e99ba60c3ddd7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_36183c3078ac4be8a0fd48de8c00ff09",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_21238add60ea411e937a445c154efaa7",
            "value": 1
          }
        },
        "f207234b51b647fd8219b5ffc9c39bd6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_971e27539ecb42718f3c31bf14d855fc",
              "IPY_MODEL_6647629812ba4652b6b6c8f3400cb79f",
              "IPY_MODEL_3e638938eb934c8db1f970675eb4a0d7"
            ],
            "layout": "IPY_MODEL_6d228f73d99247048af148f154c7fed3"
          }
        },
        "ff4ca4f11c3843809b0fb2c194b58327": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

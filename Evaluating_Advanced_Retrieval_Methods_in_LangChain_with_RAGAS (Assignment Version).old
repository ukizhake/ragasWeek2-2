{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wa8ykQk92aLX"
      },
      "source": [
        "# Evaluation with RAGAS and Advanced Retrieval Methods Using LangChain\n",
        "\n",
        "In the following notebook we'll discuss a major component of LLM Ops:\n",
        "\n",
        "- Evaluation\n",
        "\n",
        "We're going to be leveraging the [RAGAS]() framework for our evaluations today as it's becoming a standard method of evaluating (at least directionally) RAG systems.\n",
        "\n",
        "We're also going to discuss a few more powerful Retrieval Systems that can potentially improve the quality of our generations!\n",
        "\n",
        "Let's start as we always do: Grabbing our dependencies!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5BN13TZlSCv4",
        "outputId": "5731c52b-311b-4191-b550-551e745c47df"
      },
      "outputs": [],
      "source": [
        "!pip install -U -q langchain openai ragas arxiv pymupdf chromadb wandb tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: Arxiv in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (1.4.8)\n",
            "Requirement already satisfied: ArxivLoader in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (1.0.2)\n",
            "Requirement already satisfied: feedparser in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from Arxiv) (6.0.10)\n",
            "Requirement already satisfied: bs4 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from ArxivLoader) (0.0.1)\n",
            "Requirement already satisfied: lxml in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from ArxivLoader) (4.9.3)\n",
            "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from ArxivLoader) (1.25.1)\n",
            "Requirement already satisfied: pandas in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from ArxivLoader) (2.1.0)\n",
            "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from ArxivLoader) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from ArxivLoader) (4.65.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from bs4->ArxivLoader) (4.12.2)\n",
            "Requirement already satisfied: sgmllib3k in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from feedparser->Arxiv) (1.0.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/ukizhake/Library/Python/3.11/lib/python/site-packages (from pandas->ArxivLoader) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pandas->ArxivLoader) (2023.3)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pandas->ArxivLoader) (2023.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->ArxivLoader) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->ArxivLoader) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->ArxivLoader) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->ArxivLoader) (2023.7.22)\n",
            "Requirement already satisfied: six>=1.5 in /Users/ukizhake/Library/Python/3.11/lib/python/site-packages (from python-dateutil>=2.8.2->pandas->ArxivLoader) (1.16.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from beautifulsoup4->bs4->ArxivLoader) (2.4.1)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install Arxiv ArxivLoader "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Lhqp5rUThG-",
        "outputId": "98908c24-9570-4766-99e1-3d91dad465ba"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "import openai\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DV_BOewX8CW0"
      },
      "source": [
        "### Data Collection\n",
        "\n",
        "We're going to be using papers from Arxiv as our context today.\n",
        "\n",
        "We can collect these documents rather straightforwardly with the `ArxivLoader` document loader from LangChain.\n",
        "\n",
        "Let's grab and load 5 documents.\n",
        "\n",
        "- [`ArxivLoader`](https://api.python.langchain.com/en/latest/document_loaders/langchain.document_loaders.arxiv.ArxivLoader.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pymupdf in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (1.23.4)\n",
            "Requirement already satisfied: PyMuPDFb==1.23.3 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pymupdf) (1.23.3)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install pymupdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DTDNFXaBSO2j",
        "outputId": "d57f54ac-010a-4f17-b990-3d3e7a2eee8c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain.document_loaders import ArxivLoader \n",
        "\n",
        "base_docs = ArxivLoader(query=\"Retrieval Augmented Generation\", load_max_docs=5).load()\n",
        "len(base_docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nNPAWPgNSyGP",
        "outputId": "bce2ebbf-201a-4993-9824-1f46352a3de6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'Published': '2022-02-13', 'Title': 'A Survey on Retrieval-Augmented Text Generation', 'Authors': 'Huayang Li, Yixuan Su, Deng Cai, Yan Wang, Lemao Liu', 'Summary': 'Recently, retrieval-augmented text generation attracted increasing attention\\nof the computational linguistics community. Compared with conventional\\ngeneration models, retrieval-augmented text generation has remarkable\\nadvantages and particularly has achieved state-of-the-art performance in many\\nNLP tasks. This paper aims to conduct a survey about retrieval-augmented text\\ngeneration. It firstly highlights the generic paradigm of retrieval-augmented\\ngeneration, and then it reviews notable approaches according to different tasks\\nincluding dialogue response generation, machine translation, and other\\ngeneration tasks. Finally, it points out some important directions on top of\\nrecent methods to facilitate future research.'}\n",
            "{'Published': '2023-05-11', 'Title': 'Active Retrieval Augmented Generation', 'Authors': 'Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, Graham Neubig', 'Summary': 'Despite the remarkable ability of large language models (LMs) to comprehend\\nand generate language, they have a tendency to hallucinate and create factually\\ninaccurate output. Augmenting LMs by retrieving information from external\\nknowledge resources is one promising solution. Most existing\\nretrieval-augmented LMs employ a retrieve-and-generate setup that only\\nretrieves information once based on the input. This is limiting, however, in\\nmore general scenarios involving generation of long texts, where continually\\ngathering information throughout the generation process is essential. There\\nhave been some past efforts to retrieve information multiple times while\\ngenerating outputs, which mostly retrieve documents at fixed intervals using\\nthe previous context as queries. In this work, we provide a generalized view of\\nactive retrieval augmented generation, methods that actively decide when and\\nwhat to retrieve across the course of the generation. We propose\\nForward-Looking Active REtrieval augmented generation (FLARE), a generic\\nretrieval-augmented generation method which iteratively uses a prediction of\\nthe upcoming sentence to anticipate future content, which is then utilized as a\\nquery to retrieve relevant documents to regenerate the sentence if it contains\\nlow-confidence tokens. We test FLARE along with baselines comprehensively over\\n4 long-form knowledge-intensive generation tasks/datasets. FLARE achieves\\nsuperior or competitive performance on all tasks, demonstrating the\\neffectiveness of our method. Code and datasets are available at\\nhttps://github.com/jzbjyb/FLARE.'}\n",
            "{'Published': '2023-08-08', 'Title': 'Hybrid Retrieval-Augmented Generation for Real-time Composition Assistance', 'Authors': 'Xuchao Zhang, Menglin Xia, Camille Couturier, Guoqing Zheng, Saravan Rajmohan, Victor Ruhle', 'Summary': \"Retrieval augmented models show promise in enhancing traditional language\\nmodels by improving their contextual understanding, integrating private data,\\nand reducing hallucination. However, the processing time required for retrieval\\naugmented large language models poses a challenge when applying them to tasks\\nthat require real-time responses, such as composition assistance.\\n  To overcome this limitation, we propose the Hybrid Retrieval-Augmented\\nGeneration (HybridRAG) framework that leverages a hybrid setting that combines\\nboth client and cloud models. HybridRAG incorporates retrieval-augmented memory\\ngenerated asynchronously by a Large Language Model (LLM) in the cloud. By\\nintegrating this retrieval augmented memory, the client model acquires the\\ncapability to generate highly effective responses, benefiting from the LLM's\\ncapabilities. Furthermore, through asynchronous memory integration, the client\\nmodel is capable of delivering real-time responses to user requests without the\\nneed to wait for memory synchronization from the cloud. Our experiments on\\nWikitext and Pile subsets show that HybridRAG achieves lower latency than a\\ncloud-based retrieval-augmented LLM, while outperforming client-only models in\\nutility.\"}\n",
            "{'Published': '2023-05-27', 'Title': 'Augmentation-Adapted Retriever Improves Generalization of Language Models as Generic Plug-In', 'Authors': 'Zichun Yu, Chenyan Xiong, Shi Yu, Zhiyuan Liu', 'Summary': \"Retrieval augmentation can aid language models (LMs) in knowledge-intensive\\ntasks by supplying them with external information. Prior works on retrieval\\naugmentation usually jointly fine-tune the retriever and the LM, making them\\nclosely coupled. In this paper, we explore the scheme of generic retrieval\\nplug-in: the retriever is to assist target LMs that may not be known beforehand\\nor are unable to be fine-tuned together. To retrieve useful documents for\\nunseen target LMs, we propose augmentation-adapted retriever (AAR), which\\nlearns LM's preferences obtained from a known source LM. Experiments on the\\nMMLU and PopQA datasets demonstrate that our AAR trained with a small source LM\\nis able to significantly improve the zero-shot generalization of larger target\\nLMs ranging from 250M Flan-T5 to 175B InstructGPT. Further analysis indicates\\nthat the preferences of different LMs overlap, enabling AAR trained with a\\nsingle source LM to serve as a generic plug-in for various target LMs. Our code\\nis open-sourced at https://github.com/OpenMatch/Augmentation-Adapted-Retriever.\"}\n",
            "{'Published': '2023-02-07', 'Title': 'Augmenting Zero-Shot Dense Retrievers with Plug-in Mixture-of-Memories', 'Authors': 'Suyu Ge, Chenyan Xiong, Corby Rosset, Arnold Overwijk, Jiawei Han, Paul Bennett', 'Summary': 'In this paper we improve the zero-shot generalization ability of language\\nmodels via Mixture-Of-Memory Augmentation (MoMA), a mechanism that retrieves\\naugmentation documents from multiple information corpora (\"external memories\"),\\nwith the option to \"plug in\" new memory at inference time. We develop a joint\\nlearning mechanism that trains the augmentation component with latent labels\\nderived from the end retrieval task, paired with hard negatives from the memory\\nmixture. We instantiate the model in a zero-shot dense retrieval setting by\\naugmenting a strong T5-based retriever with MoMA. Our model, MoMA, obtains\\nstrong zero-shot retrieval accuracy on the eighteen tasks included in the\\nstandard BEIR benchmark. It outperforms systems that seek generalization from\\nincreased model parameters and computation steps. Our analysis further\\nillustrates the necessity of augmenting with mixture-of-memory for robust\\ngeneralization, the benefits of augmentation learning, and how MoMA utilizes\\nthe plug-in memory at inference time without changing its parameters. We plan\\nto open source our code.'}\n"
          ]
        }
      ],
      "source": [
        "for doc in base_docs:\n",
        "  print(doc.metadata)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7ht6bJX9PAY"
      },
      "source": [
        "### Creating an Index\n",
        "\n",
        "Let's use a naive index creation strategy of just using `RecursiveCharacterTextSplitter` on our documents and embedding each into our `VectorStore` using `OpenAIEmbeddings()`.\n",
        "\n",
        "Let's use a rather generic 500 character chunk size.\n",
        "\n",
        "- [`RecursiveCharacterTextSplitter()`](https://api.python.langchain.com/en/latest/text_splitter/langchain.text_splitter.RecursiveCharacterTextSplitter.html)\n",
        "- [`Chroma`](https://api.python.langchain.com/en/latest/vectorstores/langchain.vectorstores.chroma.Chroma.html?highlight=chroma#langchain.vectorstores.chroma.Chroma)\n",
        "- [`OpenAIEmbeddings()`](https://api.python.langchain.com/en/latest/embeddings/langchain.embeddings.openai.OpenAIEmbeddings.html?highlight=openaiembeddings#langchain-embeddings-openai-openaiembeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: Arxiv in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (1.4.8)\n",
            "Requirement already satisfied: feedparser in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from Arxiv) (6.0.10)\n",
            "Requirement already satisfied: sgmllib3k in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from feedparser->Arxiv) (1.0.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install Arxiv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: chromadb in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (0.4.13)\n",
            "Requirement already satisfied: requests>=2.28 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from chromadb) (2.31.0)\n",
            "Requirement already satisfied: pydantic>=1.9 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from chromadb) (1.10.12)\n",
            "Requirement already satisfied: chroma-hnswlib==0.7.3 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from chromadb) (0.7.3)\n",
            "Requirement already satisfied: fastapi>=0.95.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from chromadb) (0.99.1)\n",
            "Requirement already satisfied: uvicorn[standard]>=0.18.3 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from chromadb) (0.23.2)\n",
            "Requirement already satisfied: posthog>=2.4.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from chromadb) (3.0.2)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from chromadb) (4.7.1)\n",
            "Requirement already satisfied: pulsar-client>=3.1.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from chromadb) (3.3.0)\n",
            "Requirement already satisfied: onnxruntime>=1.14.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from chromadb) (1.16.0)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from chromadb) (0.13.3)\n",
            "Requirement already satisfied: pypika>=0.48.9 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from chromadb) (0.48.9)\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from chromadb) (4.65.0)\n",
            "Requirement already satisfied: overrides>=7.3.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from chromadb) (7.4.0)\n",
            "Requirement already satisfied: importlib-resources in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from chromadb) (6.1.0)\n",
            "Requirement already satisfied: bcrypt>=4.0.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from chromadb) (4.0.1)\n",
            "Requirement already satisfied: typer>=0.9.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from chromadb) (0.9.0)\n",
            "Requirement already satisfied: numpy>=1.22.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from chromadb) (1.25.1)\n",
            "Requirement already satisfied: starlette<0.28.0,>=0.27.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from fastapi>=0.95.2->chromadb) (0.27.0)\n",
            "Requirement already satisfied: coloredlogs in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from onnxruntime>=1.14.1->chromadb) (23.5.26)\n",
            "Requirement already satisfied: packaging in /Users/ukizhake/Library/Python/3.11/lib/python/site-packages (from onnxruntime>=1.14.1->chromadb) (23.1)\n",
            "Requirement already satisfied: protobuf in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from onnxruntime>=1.14.1->chromadb) (4.23.4)\n",
            "Requirement already satisfied: sympy in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from onnxruntime>=1.14.1->chromadb) (1.12)\n",
            "Requirement already satisfied: six>=1.5 in /Users/ukizhake/Library/Python/3.11/lib/python/site-packages (from posthog>=2.4.0->chromadb) (1.16.0)\n",
            "Requirement already satisfied: monotonic>=1.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from posthog>=2.4.0->chromadb) (1.6)\n",
            "Requirement already satisfied: backoff>=1.10.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from posthog>=2.4.0->chromadb) (2.2.1)\n",
            "Requirement already satisfied: python-dateutil>2.1 in /Users/ukizhake/Library/Python/3.11/lib/python/site-packages (from posthog>=2.4.0->chromadb) (2.8.2)\n",
            "Requirement already satisfied: certifi in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pulsar-client>=3.1.0->chromadb) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests>=2.28->chromadb) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests>=2.28->chromadb) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests>=2.28->chromadb) (2.0.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from typer>=0.9.0->chromadb) (8.1.5)\n",
            "Requirement already satisfied: h11>=0.8 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.14.0)\n",
            "Requirement already satisfied: httptools>=0.5.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.0)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (6.0.1)\n",
            "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.17.0)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.20.0)\n",
            "Requirement already satisfied: websockets>=10.4 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (11.0.3)\n",
            "Requirement already satisfied: anyio<5,>=3.4.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from starlette<0.28.0,>=0.27.0->fastapi>=0.95.2->chromadb) (3.7.1)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from anyio<5,>=3.4.0->starlette<0.28.0,>=0.27.0->fastapi>=0.95.2->chromadb) (1.3.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install chromadb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "xne8P5dQTUiR"
      },
      "outputs": [],
      "source": [
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 1000,\n",
        "    chunk_overlap  = 100,\n",
        "    length_function = len\n",
        "    ### YOUR CODE HERE\n",
        ")\n",
        "\n",
        "docs = text_splitter.split_documents(base_docs)### YOUR CODE HERE\n",
        "\n",
        "vectorstore = Chroma.from_documents(\n",
        "    documents=docs,\n",
        "    embedding=OpenAIEmbeddings())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cnRzYx4c_2mZ",
        "outputId": "977c1b18-d6de-4d78-8a3f-78398398fd1c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "357"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WyUh8EVI_6TZ",
        "outputId": "30ba5ab0-64c3-46a8-c2fd-2b2d3c9b63cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "998\n"
          ]
        }
      ],
      "source": [
        "print(max([len(chunk.page_content) for chunk in docs]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYHnPaXl-cvJ"
      },
      "source": [
        "### Setting Up our Basic QA Chain\n",
        "\n",
        "Now we can instantiate our basic `RetrievalQA` chain, let's retrieve the top `k=3` documents.\n",
        "\n",
        "- [`RetrievalQA`](https://api.python.langchain.com/en/latest/chains/langchain.chains.retrieval_qa.base.RetrievalQA.html?highlight=retrievalqa#langchain-chains-retrieval-qa-base-retrievalqa)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "-TsjUWjbUfbW"
      },
      "outputs": [],
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "primary_qa_llm = ChatOpenAI(\n",
        "    model_name=\"gpt-3.5-turbo-16k\", \n",
        "    temperature=0\n",
        ")\n",
        "\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    primary_qa_llm,### YOUR CODE HERE\n",
        "    retriever=vectorstore.as_retriever(),### YOUR CODE HERE\n",
        "    return_source_documents=True### YOUR CODE HERE\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zO69de-F-oMD"
      },
      "source": [
        "Let's test it out!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2FS5NxC6UyU2",
        "outputId": "a4e54555-6201-41c0-9cef-2468894bab2d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RAG stands for Retrieval-Augmented Generation. It is a framework that combines retrieval models and language generation models to improve the performance of language models on edge devices. The framework leverages cloud-generated memory augmentation to enhance the capabilities of small language models while operating asynchronously.\n"
          ]
        }
      ],
      "source": [
        "query = \"What is RAG?\"\n",
        "\n",
        "result = qa_chain({\"query\" : query})\n",
        "\n",
        "print(result[\"result\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wyazkAIu85KL"
      },
      "source": [
        "### Ground Truth Dataset Creation Using GPT-3.5-turbo and GPT-4\n",
        "\n",
        "The next section might take you a long time to run, so the evaluation dataset is provided.\n",
        "\n",
        "The basic idea is that we can use LangChain to create questions based on our contexts, and then answer those questions.\n",
        "\n",
        "Let's look at how that works in the code!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "WB8gSL4CZoSC"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "texts = base_docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "V24T_gpPatAO"
      },
      "outputs": [],
      "source": [
        "from langchain.output_parsers import ResponseSchema\n",
        "from langchain.output_parsers import StructuredOutputParser\n",
        "\n",
        "question_schema = ResponseSchema(\n",
        "    name=\"question\",\n",
        "    description=\"a question about the context.\"\n",
        ")\n",
        "\n",
        "question_response_schemas = [\n",
        "    question_schema,\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "ebbmazGrdPap"
      },
      "outputs": [],
      "source": [
        "question_output_parser = StructuredOutputParser.from_response_schemas(question_response_schemas)\n",
        "format_instructions = question_output_parser.get_format_instructions()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "oPqC1_MXdRuh"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "qa_template = \"\"\"\\\n",
        "You are a University Professor creating a test for advanced students. For each context, create a question that is specific to the context. Avoid creating generic or general questions.\n",
        "\n",
        "question: a question about the context.\n",
        "\n",
        "Format the output as JSON with the following keys:\n",
        "question\n",
        "\n",
        "context: {context}\n",
        "\"\"\"\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_template(template=qa_template)\n",
        "\n",
        "messages = prompt_template.format_messages(\n",
        "    context=texts[0],\n",
        "    format_instructions=format_instructions\n",
        ")\n",
        "\n",
        "response = primary_qa_llm(messages)\n",
        "output_dict = question_output_parser.parse(response.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aKFw9kyZd7eB",
        "outputId": "cf125b12-403b-4620-c5d5-a5be28f72dc3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question\n",
            "What are the limitations of retrieval-augmented text generation?\n"
          ]
        }
      ],
      "source": [
        "for k, v in output_dict.items():\n",
        "  print(k)\n",
        "  print(v)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dtASDdhLfd89",
        "outputId": "d2a942b6-cfa2-49d2-b0e8-860c509e417f"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "Zolpr3CYeEYm"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/2 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[HumanMessage(content=\"You are a University Professor creating a test for advanced students. For each context, create a question that is specific to the context. Avoid creating generic or general questions.\\n\\nquestion: a question about the context.\\n\\nFormat the output as JSON with the following keys:\\nquestion\\n\\ncontext: page_content='A Survey on Retrieval-Augmented Text Generation\\\\nHuayang Li♥,∗\\\\nYixuan Su♠,∗\\\\nDeng Cai♦,∗\\\\nYan Wang♣,∗\\\\nLemao Liu♣,∗\\\\n♥Nara Institute of Science and Technology\\\\n♠University of Cambridge\\\\n♦The Chinese University of Hong Kong\\\\n♣Tencent AI Lab\\\\nli.huayang.lh6@is.naist.jp, ys484@cam.ac.uk\\\\nthisisjcykcd@gmail.com, brandenwang@tencent.com\\\\nlemaoliu@gmail.com\\\\nAbstract\\\\nRecently, retrieval-augmented text generation\\\\nattracted increasing attention of the compu-\\\\ntational linguistics community.\\\\nCompared\\\\nwith conventional generation models, retrieval-\\\\naugmented text generation has remarkable ad-\\\\nvantages and particularly has achieved state-of-\\\\nthe-art performance in many NLP tasks. This\\\\npaper aims to conduct a survey about retrieval-\\\\naugmented text generation. It ﬁrstly highlights\\\\nthe generic paradigm of retrieval-augmented\\\\ngeneration, and then it reviews notable ap-\\\\nproaches according to different tasks including\\\\ndialogue response generation, machine trans-\\\\nlation, and other generation tasks. Finally, it\\\\npoints out some promising directions on top of\\\\nrecent methods to facilitate future research.\\\\n1\\\\nIntroduction\\\\nRetrieval-augmented text generation, as a new\\\\ntext generation paradigm that fuses emerging deep\\\\nlearning technology and traditional retrieval tech-\\\\nnology, has achieved state-of-the-art (SOTA) per-\\\\nformance in many NLP tasks and attracted the at-\\\\ntention of the computational linguistics community\\\\n(Weston et al., 2018; Dinan et al., 2018; Cai et al.,\\\\n2021). Compared with generation-based counter-\\\\npart, this new paradigm has some remarkable ad-\\\\nvantages: 1) The knowledge is not necessary to be\\\\nimplicitly stored in model parameters, but is explic-\\\\nitly acquired in a plug-and-play manner, leading\\\\nto great scalibility; 2) Instead of generating from\\\\nscratch, the paradigm generating text from some re-\\\\ntrieved human-written reference, which potentially\\\\nalleviates the difﬁculty of text generation.\\\\nThis paper aims to review many representative\\\\napproaches for retrieval-augmented text generation\\\\ntasks including dialogue response generation (We-\\\\nston et al., 2018), machine translation (Gu et al.,\\\\n2018) and others (Hashimoto et al., 2018). We\\\\n∗All authors contributed equally.\\\\nﬁrstly present the generic paradigm of retrieval-\\\\naugmented generation as well as three key com-\\\\nponents under this paradigm, which are retrieval\\\\nsources, retrieval metrics and generation models.\\\\nThen, we introduce notable methods about\\\\nretrieval-augmented generation, which are orga-\\\\nnized with respect to different tasks. Speciﬁcally,\\\\non the dialogue response generation task, exem-\\\\nplar/template retrieval as an intermediate step has\\\\nbeen shown beneﬁcial to informative response gen-\\\\neration (Weston et al., 2018; Wu et al., 2019; Cai\\\\net al., 2019a,b). In addition, there has been growing\\\\ninterest in knowledge-grounded generation explor-\\\\ning different forms of knowledge such as knowl-\\\\nedge bases and external documents (Dinan et al.,\\\\n2018; Zhou et al., 2018; Lian et al., 2019; Li et al.,\\\\n2019; Qin et al., 2019; Wu et al., 2021; Zhang et al.,\\\\n2021). On the machine translation task, we summa-\\\\nrize the early work on how the retrieved sentences\\\\n(called translation memory) are used to improve\\\\nstatistical machine translation (SMT) (Koehn et al.,\\\\n2003) models (Simard and Isabelle, 2009; Koehn\\\\nand Senellart, 2010) and in particular, we inten-\\\\nsively highlight several popular methods to inte-\\\\ngrating translation memory to NMT models (Gu\\\\net al., 2018; Zhang et al., 2018; Xu et al., 2020;\\\\nHe et al., 2021). We also review the applications\\\\nof retrieval-augmented generation in other genera-\\\\ntion tasks such as abstractive summarization (Peng\\\\net al., 2019), code generation (Hashimoto et al.,\\\\n2018), paraphrase (Kazemnejad et al., 2020; Su\\\\net al., 2021b), and knowledge-intensive generation\\\\n(Lewis et al., 2020b). Finally, we also point out\\\\nsome promising directions on retrieval-augmented\\\\ngeneration to push forward the future research.\\\\n2\\\\nRetrieval-Augmented Paradigm\\\\nIn this section, we ﬁrst give a general formulation\\\\nof retrieval-augmented text generation. Then, we\\\\ndiscuss three major components of the retrieval-\\\\naugmented generation paradigm, including the re-\\\\narXiv:2202.01110v2  [cs.CL]  13 Feb 2022\\\\nInput\\\\nSources \\\\n(Sec. 2.2):\\\\nTraining \\\\nCorpus\\\\nExternal Data\\\\nUnsupervised \\\\nData\\\\nMetrics\\\\n(Sec. 2.3):\\\\nSparse-vector \\\\nRetrieval\\\\nDense-vector \\\\nRetrieval\\\\nTask-specific \\\\nRetrieval\\\\nRetrieval Memory\\\\nGeneration Model\\\\nSec. 4: Machine \\\\nTranslation\\\\nSec. 5: Other \\\\nTasks\\\\nData \\\\nAugmentation\\\\nAttention \\\\nMechanism\\\\nSkeleton & \\\\nTemplates\\\\nInformation Retrieval\\\\nTasks:\\\\nSec. 3: Dialogue \\\\nGeneration\\\\nModels \\\\n(Sec 2.4):\\\\nOutput\\\\nFigure 1: The overview of this survey.\\\\ntrieval source, retrieval metric and integration meth-\\\\nods.\\\\n2.1\\\\nFormulation\\\\nMost text generation tasks can be formulated as a\\\\nmapping from input sequence x to output sequence\\\\ny : y = f(x). For instance, x and y could be the\\\\ndialogue history and the corresponding response\\\\nfor dialogue response generation, the text in the\\\\nsource language and the translation in the target\\\\nlanguage for machine translation, and so on.\\\\nRecently, some researchers suggest to endow\\\\nmodels the capability to access external memory\\\\nvia some information retrieval techniques, so that\\\\nthey can acquire more information in the generation\\\\nprocess (Gu et al., 2018; Weston et al., 2018; Cai\\\\net al., 2019b). The retrieval-augmented generation\\\\ncan be further formulated as:\\\\ny = f(x, z)\\\\n(1)\\\\nwhere z = {⟨xr, yr⟩} is a set of relevant instances\\\\nretrieved from the original training set or external\\\\ndatasets. The main idea of this paradigm is that yr\\\\nmay beneﬁt the response generation, if xr (or yr)\\\\nis similar (or relevant) to the input x. It is worth\\\\nnoting that xr = ∅ when unsupervised retrieval\\\\nsources are used. In general, the retrieval mem-\\\\nory can be retrieved from three kinds of sources:\\\\nthe training corpus, external datasets in the same\\\\nformat with the training corpus, and large-scale\\\\nunsupervised corpus (§2.2). Metrics that evaluate\\\\nthe relevance between text are varied as well, in\\\\n§2.3 we divided them into three categories: sparse-\\\\nvector retrieval, dense-vector retrieval, and training-\\\\nbased retrieval. Finally, how to integrate the re-\\\\ntrieval memory to the generation model is also sig-\\\\nniﬁcant, we also introduce some popular integra-\\\\ntion approaches in §2.4.\\\\n2.2\\\\nRetrieval Sources\\\\nTraining Corpus\\\\nMost previous studies search\\\\nthe external memory from its training corpus (Song\\\\net al., 2016; Gu et al., 2018; Weston et al., 2018).\\\\nIn the inference time, retrieved examples with high\\\\nrelevant scores could be regarded as extra refer-\\\\nences and reduce model’s uncertainty in generation.\\\\nThe main motivation of those works is to to store\\\\nknowledge not only in the model parameters but\\\\nalso in an explicit and accessible form, making the\\\\nmodel be able to re-access it during inference.\\\\nExternal Data\\\\nSome researchers also propose to\\\\nretrieval relevant samples from external datasets\\\\n(Su et al., 2021c; Xiao et al., 2021). In these stud-\\\\nies, the retrieval pool is different with the training\\\\ncorpus, which can further provide additional infor-\\\\nmation that are not contained in the training corpus.\\\\nThis is especially beneﬁcial for applications such\\\\nas domain adaptation and knowledge update. For\\\\nexample, Khandelwal et al. (2020a); Zheng et al.\\\\n(2021a) employ the in-domain dataset as the exter-\\\\nnal memory to achieve fast domain adaptation for\\\\nmachine translation.\\\\nUnsupervised Data\\\\nOne limitation for previous\\\\ntwo sources is that the datasets have to be super-\\\\nvised datasets consisting of aligned input-output\\\\npairs. For machine translation, Cai et al. (2021) pro-\\\\npose a cross-lingual retriever to directly retrieve tar-\\\\nget sentence from unsupervised corpus (i.e., mono-\\\\nlingual corpus in the target language). The main\\\\nidea is aligning source-side sentences and the corre-\\\\nsponding target-side translations in a dense vector\\\\nspace, i.e., aligning x and yr when xr is absent.\\\\nAs a result, the retriever directly connects the dots\\\\nbetween the source-side input and target-side trans-\\\\nlations, enabling monolingual data in the target\\\\nlanguage to be used alone as memories.\\\\n2.3\\\\nRetrieval Metrics\\\\nSparse-vector Retrieval\\\\nGiven an input se-\\\\nquence x and a retrieval corpus, retrieval model\\\\naims to retrieve a set of relevant examples z =\\\\n{⟨xr, yr⟩} from the corpus. When a supervised\\\\ncorpus is used, {⟨xr, yr⟩} is retrieved by measur-\\\\ning the similarity between x and xr. For simi-\\\\nlarity measurement, sparse-vector retrieval meth-\\\\nods such as TF-IDF and BM25 (Robertson and\\\\nZaragoza, 2009) are widely used. They match key-\\\\nwords efﬁciently with an inverted index.\\\\nDense-vector Retrieval\\\\nHowever, these meth-\\\\nods prefer examples with similar surfaces, and may\\\\nfail to retrieve examples that are only semantically\\\\nrelevant. To alleviate above problem, some stud-\\\\nies (Cao and Xiong, 2018) attempt to retrieve in\\\\ndense-vector space instead of the lexical overlap.\\\\nRecent work (Lee et al., 2019) makes use of pre-\\\\ntrained language models, which encodes the text to\\\\nlow-dimensional dense vectors via BERT-based en-\\\\ncoders. The retrieval score are computed via inner\\\\nproducts between vectors.\\\\nTask-speciﬁc\\\\nRetrieval\\\\nSimilarity-based\\\\nre-\\\\ntrieval is based on a simple heuristic. That is, the\\\\nmore xr resembles with x, the more likely xr\\\\nand yr will help the generation. However, the\\\\nmost similar one by universal textual similarity\\\\ndoes not necessarily serve the best for downstream\\\\nmodels.\\\\nIdeally, the retrieval metric would be\\\\nlearned from the data in a task-dependent way: we\\\\nwish to consider a memory only if it can indeed\\\\nboost the quality of ﬁnal generation. To this end,\\\\nCai et al. (2021) propose to unify the memory\\\\nretriever and its downstream generation model\\\\ninto a learnable whole. Such memory retrieval is\\\\nend-to-end optimized for task-speciﬁc objectives.\\\\n2.4\\\\nIntegration\\\\nData Augmentation\\\\nThere are several ways to\\\\nintegrate the retrieved external memory in gener-\\\\nation. One straightforward way is data augmen-\\\\ntation, which constructs some augmented inputs\\\\nby concatenating spans from {⟨xr, yr⟩} with the\\\\noriginal input x. By training on the augmented\\\\ninputs, a generation model implicitly leans how\\\\nto integrate the retrieved information. Despite the\\\\nsimplicity, this kind of methods works efﬁciently\\\\nin lots of tasks (Song et al., 2016; Weston et al.,\\\\n2018; Bulte and Tezcan, 2019).\\\\nAttention\\\\nMechanisms\\\\nAnother\\\\nintegration\\\\nmethod\\\\nis\\\\nbased\\\\non\\\\nattention\\\\nmechanisms\\\\n(Bahdanau et al., 2014). The main idea of this\\\\nfashion is adopting additional encoders (in various\\\\narchitectures) to encode retrieved target sentences,\\\\nand integrate them through attention (Cao and\\\\nXiong, 2018; Gu et al., 2018; Bapna and Firat,\\\\n2019). Since the attention mechanism is becoming\\\\n(Bahdanau et al., 2014; Vaswani et al., 2017) a\\\\nkey module in lots of NLP models, integrating\\\\nretrieved memory through attention becomes a\\\\nvery nature and efﬁcient way.\\\\nSkeleton Extraction\\\\nIn the previous two meth-\\\\nods, the downstream generation model learns how\\\\nto ﬁlter out irrelevant or even harmful informa-\\\\ntion from the retrieved examples implicitly. There\\\\nalso exist some works that try to explicitly extract\\\\nuseful information, i.e., skeleton extraction, from\\\\nthe retrieved memory (Cai et al., 2019a; Wu et al.,\\\\n2019; Cai et al., 2019b). For example, one skeleton\\\\nshould be a part of a whole utterance with irrelevant\\\\ncontent masked, and the generation model only in-\\\\ntegrate this skeleton in the generation process.\\\\n3\\\\nDialogue Response Generation\\\\nBackground\\\\nDialogue systems can be grouped\\\\ninto two categories: chit-chat systems and task-\\\\noriented systems. While task-oriented dialogue\\\\nsystems are designed to accomplish speciﬁc user\\\\ntasks such as air tickets booking, chit-chat dialogue\\\\nsystems aim at giving a meaningful and ﬂuent re-\\\\nsponse for any dialogue history in the open domain.\\\\nDialogue response generation in chit-chat dialogue\\\\nsystem is challenging partly due to the diversity\\\\nof possible responses to a single dialogue history\\\\n(i.e., the one-to-many problem). The dialogue his-\\\\ntory alone cannot decide a meaningful and speciﬁc\\\\nresponse. Also, external knowledge that is not\\\\npresent in the dialogue history are often necessary\\\\nfor avoiding safe but boring responses. We focus\\\\non recent efforts tackling the challenges to develop\\\\nchit-chat dialogue systems.\\\\nMost modern chit-chat dialogue systems can\\\\nbe categorized into two classes, namely, retrieval-\\\\nbased models and generation-based models. The\\\\nretrieval-based models (Ji et al., 2014; Hu et al.,\\\\n2014) directly copy an existing response from cu-\\\\nrated dialogue corpora (i.e., the retrieval pool)\\\\nwhen receiving a response request. The retrieved\\\\nresponses are often informative and grammatical\\\\nas they are collected from real-world conversa-\\\\ntions and possibly post-edited by a human. How-\\\\never, such systems perform poorly when a given\\\\ndialogue history is substantially different from\\\\nthose in the retrieval pool. On the other hand,\\\\nthe generation-based models (Shang et al., 2015;\\\\nVinyals and Le, 2015; Li et al., 2016a) generate\\\\na new utterance from scratch. Those generation-\\\\nbased models have better generalization capacity\\\\nwhen handling unseen dialogue contexts. Never-\\\\ntheless, the generated utterances are inclined to be\\\\ndull and non-informative (e.g., “I don’t know”, “I\\\\nthink so”, “Me too” etc.) (Li et al., 2016a).\\\\nShallow Integration\\\\nAs discussed, retrieval-\\\\nbased models may give informative but inappro-\\\\npriate responses while generation-based models\\\\noften do the opposite. It is desirable to combine the\\\\nbest of both worlds. Early work (Qiu et al., 2017)\\\\nattempts to re-rank the output from both models.\\\\nFor a deep integration, Song et al. (2016) and Yang\\\\net al. (2019) extend the standard SEQ2SEQ encoder-\\\\ndecoder model (Bahdanau et al., 2014) with an ex-\\\\ntra encoder for encoding the retrieval result. The\\\\noutput of the extra encoder, along with the output\\\\nfrom the original encoder for dialogue history, is\\\\nused to feed the decoder. Weston et al. (2018) use\\\\na single encoder that takes the concatenation of\\\\nthe original dialogue history and the retrieved as\\\\ninput. Wu et al. (2019) note that the retrieved infor-\\\\nmation should be used in awareness of the context\\\\ndifference, and further proposed to construct an\\\\nedit vector by explicitly encoding the lexical differ-\\\\nences between the input dialogue history and the\\\\nretrieved dialogue history. Pandey et al. (2018) fur-\\\\nther propose to weight different training instances\\\\nby context similarity.\\\\nDeep Integration\\\\nTo prevent the inﬂow of er-\\\\nroneous information, Cai et al. (2019a) propose\\\\na general framework that ﬁrst extracts a skeleton\\\\nfrom the retrieved response and then generates the\\\\nresponse based on the extracted skeleton. This\\\\nframework is also adopted for stylistic response\\\\ngeneration (Su et al., 2021c). Gupta et al. (2021)\\\\nsuggest to use the semantic structure of an exem-\\\\nplar response, instead of the tokens of the exem-\\\\nplar response, to guide generation. Despite their\\\\ndifferences, a common issue is that the genera-\\\\ntion model easily learns to ignore the retrieved re-\\\\nsponse entirely and collapses to a vanilla seq2seq\\\\nmodel. This happens with improper training in-\\\\nstances. Due to the one-to-many nature, it hap-\\\\npens frequently that a retrieved response (extracted\\\\nskeleton) is suitable for responding to the query,\\\\nbut inconsistent with the current target response.\\\\nEarlier studies (Weston et al., 2018; Wu et al.,\\\\n2019; Cai et al., 2019a) alleviate the above prob-\\\\nlems by putting hard constraints on the data (e.g.,\\\\ndiscarding data with low similarity of the retrieved\\\\nresponse and the target response), which, however,\\\\ngreatly reduces the amount of usable data. Cai\\\\net al. (2019b) employ a random mechanism for\\\\ngenerating the skeletons used for training, which\\\\nextract skeletons from the corresponding responses\\\\nwith some deliberate disturbance. Paranjape et al.\\\\n(2021) propose to model the retriever after the pos-\\\\nterior distribution of retrieval given the input and\\\\nthe target output and train it jointly with the stan-\\\\ndard retriever and the generator by maximizing the\\\\nevidence lower bound (ELBo) in expectation over\\\\nretrieval.\\\\nKnowledge-Enhanced Generation\\\\nThe afore-\\\\nmentioned work demonstrates that retrieval-based\\\\ndialogue systems can be used for building bet-\\\\nter generation-based models. In general, this is\\\\ndone by conditioning the generation on some re-\\\\ntrieved responses. More traditionally, to infuse\\\\nthe response with external knowledge, the retrieval\\\\npool is not necessarily a dialogue corpus. In fact,\\\\nknowledge-grounded dialogue response generation\\\\nexploring different forms of knowledge such as\\\\nknowledge bases and external documents (Dinan\\\\net al., 2018; Zhou et al., 2018; Lian et al., 2019;\\\\nLi et al., 2019; Qin et al., 2019; Wu et al., 2021;\\\\nZhang et al., 2021; Komeili et al., 2021) has been\\\\nactively explored.\\\\nLimitations\\\\nWe note that there are three major\\\\nlimitations in existing work for dialogue response\\\\ngeneration. First, current methods only use one\\\\nretrieved response for generation. It can be more\\\\nbeneﬁcial to combine multiple retrieval responses.\\\\nHowever, this can be difﬁcult due to the one-to-\\\\nmany nature of dialogue response generation. Sec-\\\\nond, current methods use universal relevance score\\\\nfor retrieval. It can be more effective if we can\\\\nuse more customized retrieval metric especially\\\\nfor controlled dialogue response generation (e.g.,\\\\npersona, emotion, etc). Third, the retrieval pool\\\\nof existing methods is limited to dialogue corpora\\\\n(context-response pairs) or documents. It might\\\\nbe useful to enlarge the retrieval pool by including\\\\nmore corpora in other domains or in other modali-\\\\nties. As discussed, there leaves plenty of possible\\\\ndirections to explore in the future.\\\\n4\\\\nMachine Translation\\\\nRetrieval augmented translation originates from hu-\\\\nman translation scenarios (Somers, 2003). When\\\\ntranslating ˆy from an input source sentence x, a hu-\\\\nman translator typically involves a search engine to\\\\nretrieve similar sentences {⟨xr, yr⟩} from a bilin-\\\\ngual database. Such a technique called translation\\\\nmemory is helpful to improve the translation qual-\\\\nity and efﬁciency for human translators (Dillon\\\\nand Fraser, 2006). As the development of ma-\\\\nchine translation techniques, there is a surge of\\\\ninterests in improving machine translation models\\\\nwith translation memory. In the rest of this section,\\\\nwe will review translation memory for both statisti-\\\\ncal machine translation (SMT) and neural machine\\\\ntranslation (NMT).\\\\n4.1\\\\nTranslation Memory in SMT\\\\nGenerally, SMT includes three key components in\\\\na pipeline manner such as phrase table extraction,\\\\nparameter tuning and decoding (Koehn et al., 2003;\\\\nChiang, 2007). As a result, many efforts have been\\\\nmade to make use of translation memory (TM) on\\\\ntop of each component.\\\\nConstrained Decoding with TM\\\\nConstrained\\\\ndecoding is the most straightforward way to in-\\\\ntegrating TM into SMT (Smith and Clark, 2009;\\\\nKoehn and Senellart, 2010; Zhechev and Van Gen-\\\\nabith, 2010; Ma et al., 2011). Its basic idea is\\\\nto reuse the useful segments in yr while trans-\\\\nlate other segments by SMT. Speciﬁcally, the ap-\\\\nproach consists of three steps: 1) identify the un-\\\\nmatched segments in both xr and x through the\\\\nedit-distance algorithm; 2) identify the unmatched\\\\nsegments in yr, each of which is aligned to one\\\\nunmatched segment in xr by a word alignment\\\\nalgorithm; 3) decode each unmatched segment in\\\\nx by SMT and then use the result to replace its\\\\ncorresponding unmatched segment in yr. Li et al.\\\\n(2016b) further extend this approach from sentence\\\\nlevel to phrase level. The advantage in constrained\\\\ndecoding is that it does not require to change the\\\\ntranslation model (including phrase table and pa-\\\\nrameters) and can be applied in a plug-and-play\\\\nway. This approach is successful when x is highly\\\\nsimilar to xr; otherwise its performance is de-\\\\ngraded largely, because it explicitly isolates TM\\\\nmatching and SMT decoding and reuses the results\\\\nin xr or not in a deterministic way.\\\\nPhrase Table Aggregation with TM\\\\nThere are\\\\nalso notable efforts to augment the phrase table\\\\nfor SMT by extracting translation rules from the\\\\nretrieved bilingual sentences {⟨xr, yr⟩}.\\\\nThen\\\\nthey re-tune the parameters for the SMT model\\\\nwhich makes use of translation knowledge from\\\\n{⟨xr, yr⟩} in a implicit way when translating x.\\\\nFor example, Biçici and Dymetman (2008); Simard\\\\nand Isabelle (2009) directly combine the extracted\\\\ntranslation rules into the phrase table in a shallow\\\\ncombination way. They introduce an additional fea-\\\\nture to indicate that whether translation rule is from\\\\n{⟨xr, yr⟩} or not and then train all feature weights\\\\nwith MERT (Och, 2003). One characteristic of\\\\nthese work is that a translation rule extracted from\\\\n{⟨xr, yr⟩} which can not exactly match any seg-\\\\nments in x is useless even if it may contain some\\\\nuseful words in its target side. To remedy this ob-\\\\nservation, Wang et al. (2013, 2014) resort to a deep\\\\ncombination way to using the extracted translation\\\\nrules. For each rule in the phrase table, it designs\\\\na generative model to reward the rules which are\\\\nsimilar to those extracted from {⟨xr, yr⟩}. Then\\\\nthis generative model is used as a feature in the log-\\\\nlinear based SMT model whose weight is tuned\\\\ntogether with other features by MERT. In addition,\\\\nLi et al. (2014) employ a similar way to reward\\\\nthe rules but it relies on a discriminative model\\\\nwhich is easy to integrate potential features from\\\\n{⟨xr, yr⟩}.\\\\nParameter Tuning with TM\\\\nUnlike the above\\\\ntwo research lines, Liu et al. (2012, 2014) make use\\\\nof translation memory only in tuning parameters.\\\\nTo be speciﬁc, when translating an input sentence\\\\nx, they ﬁrstly retrieve many similar bilingual sen-\\\\ntences {⟨xr, yr⟩}, and then tune the parameters on\\\\ntop of the retrieved sentences as well as a given de-\\\\nvelopment dataset in a sentence-wise manner, i.e.,\\\\nit performs an independent tuning for each input\\\\nsentence. To improve the efﬁciency of each tuning\\\\nstep, it propose a local update on top of {⟨xr, yr⟩}\\\\nfrom a baseline model.\\\\nDespite the successes of translation memory in\\\\nSMT, there are still some limitations for the above\\\\nthree kinds of methods. Firstly, all these methods\\\\nemploy fuzzy score for retrieval which is highly de-\\\\npendent on word matching and thus can not recall\\\\nsuch examples which are similar in word seman-\\\\ntics but different in surface form. Secondly, these\\\\nmethods integrate the retrieved examples into a\\\\nmodule of SMT in the ways which can not make\\\\nfull use of the knowledge in retrieved examples.\\\\nFor example, the integration ways in the ﬁrst two\\\\nkinds (constrained decoding and phrase table ag-\\\\ngregation) are heuristic and not optimized towards\\\\ntranslation quality; the parameter tuning method\\\\nﬁne-tunes few parameters for log-linear based SMT\\\\nwhich are not enough to preserve sufﬁcient knowl-\\\\nedge from retrieved examples. Thirdly, since SMT\\\\nperforms in a pipeline manner, it is intractable to\\\\njointly optimize retrieval metrics as well as SMT\\\\nmodels. Consequently, all these methods adopt an\\\\noff-the-shelf metric for retrieval, leading to sub-\\\\noptimal performance.\\\\n4.2\\\\nTranslation Memory in NMT\\\\nTranslation memory has been widely explored in\\\\nNeural Machine Translation (NMT). Depending\\\\non when retrieval is involved, we can categorize\\\\nprevious works into two classes: 1) an NMT model\\\\nleans how to cooperate with the retrieval model in\\\\nthe training phase; 2) an NMT model is only aware\\\\nof the retrieved data in the inference phase.\\\\nInference Phase\\\\nThe key point of literature in\\\\nthis line is to reward some target words based on\\\\nwords in yr in the inference process. Thus, a de-\\\\ncision can be made based on both the distribution\\\\nof generation model and the additional reward of\\\\nretrieval model. Some previous works propose to\\\\nreward target words based on the sentence-level\\\\nsimilarity between x and xr, and the word align-\\\\nment between xr and yr. Given the input sentence\\\\nx, Zhang et al. (2018) try to assign target words\\\\nin ˆy with higher rewards, when they appear in yr\\\\nand the aligned source words are in both xr and\\\\nx. He et al. (2019) follow a similar framework\\\\nand consider the position information of those tar-\\\\nget words when rewarding. Those works reward\\\\nthe target words in an explicit way, however, the\\\\none-sentence-one-model approach (Li et al., 2016c;\\\\nTurchi et al., 2017) propose to reward target word\\\\nimplicitly. For each testing input x, their approach\\\\nwill ﬁrst ﬁnetune the translation model on retrieved\\\\nmemory {⟨xr, yr⟩} and then translate x.\\\\nOthers try to reward target words based on token-\\\\nlevel similarity score. Most works in this line are\\\\nbased on the dense retriever (Khandelwal et al.,\\\\n2020a), e.g., faiss. Khandelwal et al. (2020a) build\\\\na key-value datastore, where key h(xr, yr\\\\n<t) is the\\\\nhidden state at each time step when translating yr\\\\nfrom xr, and value is its golden-truth target word\\\\nyr\\\\nt. Therefore, in the inference time, they can use\\\\nthe h(x, ˆy<t) as query and reward target words\\\\nwith similar hidden representations in the datas-\\\\ntore. Although this method achieves signiﬁcant\\\\nperformance gain, one drawback of it is the high la-\\\\ntency. To address this issue, Meng et al. (2021) use\\\\nsome heuristics, e.g., pre-ﬁltering, to avoid search-\\\\ning on the entire datastore. The reward score of\\\\nprevious works is got from some non-parametric\\\\napproaches, however, Zheng et al. (2021a) propose\\\\na light-weight network to learn the reward score.\\\\nSince dense retrieval has the potential of cross-\\\\nlingual retrieval, Zheng et al. (2021b) use a similar\\\\napproach to achieve unsupervised domain adapta-\\\\ntion, where a main change is to create the datastore\\\\nbased on synthetic sources sentence and the real\\\\ntarget sentences.\\\\nTraining Phase\\\\nDifferent from those model-\\\\nagnostic approaches, previous works in this line\\\\naim to train the generation model to learn how\\\\nto cooperate with the retrieval model. It is also\\\\nworth noting that most works in this line adopt\\\\nthe sentence-level retrieval, when integrating the\\\\nretrieval information in the training process. To\\\\nachieve its goal, Bulte and Tezcan (2019) and\\\\nHossain et al. (2020) propose a data augmenta-\\\\ntion method to integrate the retrieved information,\\\\nwhere x is concatenated with yr before feeding\\\\ninto the model . Following the data augmentation\\\\napproach, Xu et al. (2020) propose more matching\\\\nmethods to determine including which retrieved\\\\nexample in the source is better.\\\\nThere also exist some works that propose new\\\\narchitectures to integrate the retrieval information.\\\\nUnder the RNN-based framework, Cao and Xiong\\\\n(2018) and Gu et al. (2018) use the gating and at-\\\\ntention mechanism to incorporate the retrieved tar-\\\\nget sentences. When Transformer (Vaswani et al.,\\\\n2017) becomes the backbone of NMT, some works\\\\nalso use additional transformer encoders to en-\\\\ncode retrieved target sentences, and integrate them\\\\nthrough attention mechanism (Bapna and Firat,\\\\n2019; Cao et al., 2019). Xia et al. (2019) repre-\\\\nsent the retrieved target sentences in a different\\\\ndata structure, i.e., a graph structure, and integrate\\\\nit through attention mechanism. He et al. (2021)\\\\npropose a light-weight method to encode the re-\\\\ntrieved target sentences and leverage the alignment\\\\ninformation to ﬁlter out irrelevant information. Dif-\\\\nferent from previous works that rely on bilingual\\\\nmemories, Cai et al. (2021) propose a framework\\\\nthat can retrieve the most similar target sentence in\\\\na monolingual dataset, using a source sentence as\\\\nquery.\\\\nLimitations\\\\nIn the section of SMT, we have\\\\nshowed some limitations of the retrieval augmented\\\\napproaches. There also exist some limitations in\\\\nthe line of NMT. First, the information used for\\\\nderiving reward scores is limited. The similarity\\\\nbetween an input and retrieved examples is the\\\\nprimary feature to derive reward scores.\\\\nHow-\\\\never, some information, e.g., frequencies of words\\\\nand context, may also be beneﬁcial for integrating\\\\nthe translation memory. Second, it remains to be\\\\nan open question that when should we use the re-\\\\ntrieved information and when not. In the inference\\\\nphase, approaches tend to integrate the translation\\\\nmemory excessively, e.g., at each time step, which\\\\nnot only reduces the translation efﬁciency but may\\\\nalso dampen the ﬂuency of generated results.\\\\n5\\\\nOther Tasks\\\\nIn addition to dialogue system and machine trans-\\\\nlation, retrieval-augmented generation techniques\\\\nhave shown to be beneﬁcial in many other tasks. In\\\\nthe following, we highlight several key tasks that\\\\napply retrieval-augmented generation approaches.1\\\\nLanguage Modelling\\\\nIt has been shown that\\\\nproperly leveraging information from retrieval\\\\nmemory could improve the performance of large\\\\npre-trained language model. To build a more accu-\\\\nrate language model, Khandelwal et al. (2020b) pro-\\\\npose to incorporate a soft memory module into the\\\\nsystem. Speciﬁcally, an index is built by caching\\\\nthe hidden states of the training corpus. Then, the\\\\nlanguage model accesses the index via k-NN search\\\\nand displays a greatly improved performance. As\\\\nanother example, Guu et al. (2020) propose a new\\\\nparadigm that applies retrieval-augmented tech-\\\\nnique into the pre-training of generative language\\\\nmodel. During learning, they train a neural se-\\\\nlector that dynamically samples a relevant text to\\\\nguide the reconstruction of a corrupted input se-\\\\nquence. In this way, the pre-trained model deliv-\\\\ners better results by explicitly grounding on the\\\\nretrieval memory. Lewis et al. (2020a) combine\\\\nlanguage model pre-training with a paraphrasing\\\\n1Here, we focus on tasks other than question answering.\\\\nWe refer readers interested in QA to Chen and Yih (2020).\\\\napproach. During learning, an input sequence to\\\\nthe model is ﬁrst corrupted. In the meantime, a set\\\\nof multi-lingual texts are retrieved based on which\\\\nthe model learns to reconstruct the original input\\\\nsequence. Recently, Borgeaud et al. (2021) pro-\\\\npose RETRO, a large pre-trained language model\\\\nenhanced with retrieved documents, and obtained\\\\ncomparable performances with GPT-3 using 25×\\\\nfewer parameters.\\\\nSummarization\\\\nText summarization is another\\\\nresearch\\\\narea\\\\nthat\\\\nbeneﬁts\\\\nfrom\\\\nretrieval-\\\\naugmented text generation.\\\\nPeng et al. (2019)\\\\npropose an adaptive decoding framework which\\\\nﬁrst retrieves an exemplar document given the\\\\nsource document. Then, the summarization of the\\\\nsource document is derived through an adaptive\\\\ngeneration process based on the retrieved template.\\\\nDifferent from Peng et al. (2019), Cao et al.\\\\n(2018) and Hossain et al. (2020) introduce an\\\\nintermediate re-ranking stage into the generation\\\\npipeline.\\\\nSpeciﬁcally, before generating the\\\\ndocument summary, the retrieval documents are\\\\nﬁrst re-ranked based on their similarity scores\\\\nwith respect to the source document. Then, the\\\\ndocument summarization is produced by re-writing\\\\nthe selected templates.\\\\nParaphrase Generation\\\\nTo address the lack of\\\\nquality as well as diversity in the generation of para-\\\\nphrases, Kazemnejad et al. (2020) propose a gen-\\\\neration framework which ﬁrst retrieves a sentence\\\\nthat is similar to input sentence. Then, based on\\\\nthe retrieved sentence, a neural editor produces the\\\\nresulting paraphrased sentence. Chen et al. (2019)\\\\ninvestigate a different aspect of paraphrasing, i.e.\\\\nhow to control the linguistic syntax displayed in\\\\nthe generated text. To achieve this goal, Chen et al.\\\\n(2019) propose to ﬁrst extract a sentential exem-\\\\nplar that serves as the syntax template. A neural\\\\nmodel then generates the paraphrase with desired\\\\nlinguistic syntax following the retrieved exemplar.\\\\nText Style Transfer\\\\nTo improve the quality of\\\\ngenerated text, Li et al. (2018) propose a retrieval-\\\\naugmented framework which ﬁrst retrieves texts\\\\nthat are similar to the input based on lexical-level\\\\nsimilarity. Then, the retrieved tokens that are irrel-\\\\nevant to the source are deleted, and the output is\\\\nderived from the edited template. Xiao et al. (2021)\\\\nalso adopte this framework by incorporating re-\\\\ntrieval information from two sources (i.e. sparse\\\\nand dense memories) and obtained an improved\\\\nmodel performance.\\\\nData-to-Text Generation\\\\nRecently, retrieval-\\\\naugmented generation has been adapted to the task\\\\nof data-to-text generation. To bridge the gap be-\\\\ntween the structured data and natural language\\\\ntext, Su et al. (2021a) propose a novel retrieval-\\\\naugmented framework.\\\\nSpeciﬁcally, given the\\\\nsource data, a set of candidate texts are ﬁrst re-\\\\ntrieved from a large unlabelled corpus. Then, a\\\\nneural selector is applied to measure the similari-\\\\nties between the source data and candidate texts,\\\\nand extract a set of more ﬁne-grained prototypes\\\\nfrom the candidates. Lastly, a generation model\\\\ntakes the prototypes as input to produce the text\\\\nthat describes the given structured data.\\\\nWhile retrieval-augmented generation has been\\\\nwidely explored in the NLP community, we sug-\\\\ngest that future research could extend this approach\\\\nto tasks that involve data from multiple modali-\\\\nties. For instance, with recent advancements in\\\\nimage-text retrieval (Jia et al., 2021; Radford et al.,\\\\n2021), the structural gap between images and texts\\\\nis largely bridged. Some early studies (Zhang et al.,\\\\n2020) have shown that information retrieved from\\\\nimages could improve the performance of neural\\\\nmachine translation model. Naturally, such meth-\\\\nods could be extended to other multi-modal tasks,\\\\nsuch as image captioning (Karpathy and Li, 2015).\\\\nA similar idea could also be applied to tasks be-\\\\nyond images, such as speech-to-text transcription\\\\n(Gales and Young, 2007).\\\\n6\\\\nFuture Directions\\\\nDespite the current success of retrieval augmented\\\\ntext generation, there is still a long way to go as\\\\ndiscussed in previous sections. We highlight some\\\\ndirections to facilitate the future research as fol-\\\\nlows:\\\\nRetrieval Sensitivity\\\\nThe performance of re-\\\\ntrieval augmented text generation is very sensitive\\\\nto the retrieval quality, i.e., the similarity between\\\\nthe query and the retrieved examples. Currently, re-\\\\ntrieval augmented text generation models perform\\\\nwell when the retrieved examples are very simi-\\\\nlar to the query. However, they are even worse\\\\nthan the generation models without retrieval when\\\\nthe retrieval examples are less similar. Therefore,\\\\nit would be important to exploit new methods to\\\\naddress such an issue on similarity.\\\\nRetrieval Efﬁciency\\\\nGenerally, if one enlarges\\\\nthe retrieval memory to some extent, it would be\\\\npossible to retrieve an example which is very simi-\\\\nlar to the query.Unfortunately, the downside is that\\\\nthe overall inference for the retrieval augmented\\\\ngeneration models is less efﬁcient due the consid-\\\\nerable retrieval overhead. In this sense, it is urgent\\\\nto consider some methods to trade off the retrieval\\\\nmemory size and retrieval efﬁciency, for example,\\\\ndata compression for the retrieval memory.\\\\nLocal vs. Global Optimization\\\\nTheoretically, it\\\\nseems promising to jointly learn retrieval metrics\\\\nand generation models. However, in practice, there\\\\nis an essential gap about the retrieval metric be-\\\\ntween the training and inference phrases. In the\\\\ntraining phase, the loss is locally back-propagated\\\\nto only a few retrieved examples while in the infer-\\\\nence phase the metric is globally conducted among\\\\nall examples in the memory. It would be interesting\\\\nto narrow such a gap when learning a better metric\\\\nfor generation tasks.\\\\nMulti-Modalities\\\\nWith recent advancement in\\\\nimage-text retrieval, directly associating images\\\\nwith relevant text becomes possible. This urges\\\\nresearchers to investigate the possibility of retrieval-\\\\nbased text generation in tasks that involve data from\\\\ndifferent modalities. One typical task is image\\\\ncaptioning. Beyond images, other tasks like speech-\\\\nto-text transcription could potentially beneﬁt from\\\\nretrieval-based generation methods as well.\\\\nDiverse & Controllable Retrieval\\\\nMost of the\\\\nexisting approaches adopt a universal metric for\\\\nretrieval, such as lexical similarities of sentences.\\\\nFuture work should explore how to use customized\\\\nmetrics for retrieval. This can be beneﬁcial for\\\\nmore controlled text generation. For example, in-\\\\nstances with emotions and styles may be more de-\\\\nsirable in the personalized dialogue generation, par-\\\\nallel data that contains speciﬁc terminologies is\\\\nmore helpful in machine translation, and so on. On\\\\nthe other hand, using a universal metric for retrieval\\\\nmay lead to the lack of diversity of the retrieval re-\\\\nsults. Collecting a diverse set of retrieval results\\\\ncan improve the coverage of useful information.\\\\nThus, considering multiple different metrics for re-\\\\ntrieval may lead to generation with higher quality\\\\nin the future.\\\\n7\\\\nConclusion\\\\nIn this paper, we surveyed recent approaches for\\\\nretrieval-augmented text generation. We reviewed\\\\nand summarized the development of different com-\\\\nponents of retrieval-augmented text generation in-\\\\ncluding retrieval metrics, retrieval sources, and in-\\\\ntegration paradigms. We gave in-depth discussions\\\\nwhen retrieval-augmented text generation comes to\\\\ndifferent applications including dialogue response\\\\ngeneration, machine translation, and other genera-\\\\ntion tasks. We also pointed out some future direc-\\\\ntions for retrieval-augmented text generation.\\\\nReferences\\\\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\\\\ngio. 2014.\\\\nNeural machine translation by jointly\\\\nlearning to align and translate.\\\\narXiv preprint\\\\narXiv:1409.0473.\\\\nAnkur Bapna and Orhan Firat. 2019. Non-parametric\\\\nadaptation for neural machine translation. In Pro-\\\\nceedings of the 2019 Conference of the North Amer-\\\\nican Chapter of the Association for Computational\\\\nLinguistics: Human Language Technologies, Vol-\\\\nume 1 (Long and Short Papers), pages 1921–1931.\\\\nErgun Biçici and Marc Dymetman. 2008.\\\\nDynamic\\\\ntranslation memory: Using statistical machine trans-\\\\nlation to improve translation memory fuzzy matches.\\\\nIn International Conference on Intelligent Text Pro-\\\\ncessing and Computational Linguistics, pages 454–\\\\n465. Springer.\\\\nSebastian Borgeaud, Arthur Mensch, Jordan Hoff-\\\\nmann, Trevor Cai, Eliza Rutherford, Katie Millican,\\\\nGeorge van den Driessche, Jean-Baptiste Lespiau,\\\\nBogdan Damoc, Aidan Clark, Diego de Las Casas,\\\\nAurelia Guy, Jacob Menick, Roman Ring, Tom Hen-\\\\nnigan, Saffron Huang, Loren Maggiore, Chris Jones,\\\\nAlbin Cassirer, Andy Brock, Michela Paganini, Ge-\\\\noffrey Irving, Oriol Vinyals, Simon Osindero, Karen\\\\nSimonyan, Jack W. Rae, Erich Elsen, and Laurent\\\\nSifre. 2021. Improving language models by retriev-\\\\ning from trillions of tokens. CoRR, abs/2112.04426.\\\\nBram Bulte and Arda Tezcan. 2019. Neural fuzzy re-\\\\npair: Integrating fuzzy matches into neural machine\\\\ntranslation. In Proceedings of the 57th Annual Meet-\\\\ning of the Association for Computational Linguistics,\\\\npages 1800–1809.\\\\nDeng Cai, Yan Wang, Wei Bi, Zhaopeng Tu, Xi-\\\\naojiang Liu, Wai Lam, and Shuming Shi. 2019a.\\\\nSkeleton-to-response: Dialogue generation guided\\\\nby retrieval memory.\\\\nIn Proceedings of the 2019\\\\nConference of the North American Chapter of the\\\\nAssociation for Computational Linguistics: Human\\\\nLanguage Technologies, Volume 1 (Long and Short\\\\nPapers), pages 1219–1228.\\\\nDeng Cai, Yan Wang, Wei Bi, Zhaopeng Tu, Xiao-\\\\njiang Liu, and Shuming Shi. 2019b.\\\\nRetrieval-\\\\nguided dialogue response generation via a matching-\\\\nto-generation framework.\\\\nIn Proceedings of the\\\\n2019 Conference on Empirical Methods in Natu-\\\\nral Language Processing and the 9th International\\\\nJoint Conference on Natural Language Processing\\\\n(EMNLP-IJCNLP), pages 1866–1875.\\\\nDeng Cai, Yan Wang, Huayang Li, Wai Lam, and\\\\nLemao Liu. 2021. Neural machine translation with\\\\nmonolingual translation memory. In Proceedings of\\\\nthe 59th Annual Meeting of the Association for Com-\\\\nputational Linguistics and the 11th International\\\\nJoint Conference on Natural Language Processing\\\\n(Volume 1: Long Papers), pages 7307–7318, Online.\\\\nAssociation for Computational Linguistics.\\\\nQian Cao, Shaohui Kuang, and Deyi Xiong. 2019.\\\\nLearning to reuse translations: Guiding neural ma-\\\\nchine translation with examples.\\\\narXiv preprint\\\\narXiv:1911.10732.\\\\nQian Cao and Deyi Xiong. 2018.\\\\nEncoding gated\\\\ntranslation memory into neural machine translation.\\\\nIn Proceedings of the 2018 Conference on Empiri-\\\\ncal Methods in Natural Language Processing, pages\\\\n3042–3047.\\\\nZiqiang Cao, Wenjie Li, Sujian Li, and Furu Wei.\\\\n2018. Retrieve, rerank and rewrite: Soft template\\\\nbased neural summarization. In Proceedings of the\\\\n56th Annual Meeting of the Association for Com-\\\\nputational Linguistics, ACL 2018, Melbourne, Aus-\\\\ntralia, July 15-20, 2018, Volume 1: Long Papers,\\\\npages 152–161. Association for Computational Lin-\\\\nguistics.\\\\nDanqi Chen and Wen-tau Yih. 2020.\\\\nOpen-domain\\\\nquestion answering. In Proceedings of the 58th An-\\\\nnual Meeting of the Association for Computational\\\\nLinguistics: Tutorial Abstracts, pages 34–37, On-\\\\nline. Association for Computational Linguistics.\\\\nMingda Chen, Qingming Tang, Sam Wiseman, and\\\\nKevin Gimpel. 2019. Controllable paraphrase gen-\\\\neration with a syntactic exemplar. In Proceedings of\\\\nthe 57th Conference of the Association for Compu-\\\\ntational Linguistics, ACL 2019, Florence, Italy, July\\\\n28- August 2, 2019, Volume 1: Long Papers, pages\\\\n5972–5984. Association for Computational Linguis-\\\\ntics.\\\\nDavid Chiang. 2007. Hierarchical phrase-based trans-\\\\nlation. computational linguistics, 33(2):201–228.\\\\nSarah Dillon and Janet Fraser. 2006. Translators and\\\\ntm: An investigation of translators’ perceptions of\\\\ntranslation memory adoption. Machine Translation,\\\\n20(2):67–79.\\\\nEmily Dinan, Stephen Roller, Kurt Shuster, Angela\\\\nFan, Michael Auli, and Jason Weston. 2018. Wizard\\\\nof wikipedia: Knowledge-powered conversational\\\\nagents. arXiv preprint arXiv:1811.01241.\\\\nMark J. F. Gales and Steve J. Young. 2007. The applica-\\\\ntion of hidden markov models in speech recognition.\\\\nFound. Trends Signal Process., 1(3):195–304.\\\\nJiatao Gu, Yong Wang, Kyunghyun Cho, and Vic-\\\\ntor OK Li. 2018. Search engine guided neural ma-\\\\nchine translation. In Proceedings of the AAAI Con-\\\\nference on Artiﬁcial Intelligence, volume 32.\\\\nPrakhar Gupta, Jeffrey Bigham, Yulia Tsvetkov, and\\\\nAmy Pavel. 2021. Controlling dialogue generation\\\\nwith semantic exemplars.\\\\nIn Proceedings of the\\\\n2021 Conference of the North American Chapter of\\\\nthe Association for Computational Linguistics: Hu-\\\\nman Language Technologies, pages 3018–3029, On-\\\\nline. Association for Computational Linguistics.\\\\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu-\\\\npat, and Ming-Wei Chang. 2020. REALM: retrieval-\\\\naugmented language model pre-training.\\\\nCoRR,\\\\nabs/2002.08909.\\\\nTatsunori B Hashimoto, Kelvin Guu, Yonatan Oren,\\\\nand Percy S Liang. 2018. A retrieve-and-edit frame-\\\\nwork for predicting structured outputs. In Advances\\\\nin Neural Information Processing Systems, pages\\\\n10052–10062.\\\\nQiuxiang He, Guoping Huang, Qu Cui, Li Li, and\\\\nLemao Liu. 2021. Fast and accurate neural machine\\\\ntranslation with translation memory.\\\\nIn Proceed-\\\\nings of the 59th Annual Meeting of the Association\\\\nfor Computational Linguistics and the 11th Interna-\\\\ntional Joint Conference on Natural Language Pro-\\\\ncessing (Volume 1: Long Papers), pages 3170–3180.\\\\nQiuxiang He, Guoping Huang, Lemao Liu, and Li Li.\\\\n2019. Word position aware translation memory for\\\\nneural machine translation.\\\\nIn CCF International\\\\nConference on Natural Language Processing and\\\\nChinese Computing, pages 367–379. Springer.\\\\nNabil Hossain, Marjan Ghazvininejad, and Luke Zettle-\\\\nmoyer. 2020.\\\\nSimple and effective retrieve-edit-\\\\nrerank text generation. In Proceedings of the 58th\\\\nAnnual Meeting of the Association for Computa-\\\\ntional Linguistics, pages 2532–2538.\\\\nBaotian Hu, Zhengdong Lu, Hang Li, and Qingcai\\\\nChen. 2014. Convolutional neural network architec-\\\\ntures for matching natural language sentences. In\\\\nNIPS, pages 2042–2050.\\\\nZongcheng Ji, Zhengdong Lu, and Hang Li. 2014. An\\\\ninformation retrieval approach to short text conver-\\\\nsation. arXiv preprint arXiv:1408.6988.\\\\nChao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana\\\\nParekh, Hieu Pham, Quoc V. Le, Yun-Hsuan Sung,\\\\nZhen Li, and Tom Duerig. 2021. Scaling up visual\\\\nand vision-language representation learning with\\\\nnoisy text supervision. In Proceedings of the 38th In-\\\\nternational Conference on Machine Learning, ICML\\\\n2021, 18-24 July 2021, Virtual Event, volume 139 of\\\\nProceedings of Machine Learning Research, pages\\\\n4904–4916. PMLR.\\\\nAndrej Karpathy and Fei-Fei Li. 2015. Deep visual-\\\\nsemantic alignments for generating image descrip-\\\\ntions. In IEEE Conference on Computer Vision and\\\\nPattern Recognition, CVPR 2015, Boston, MA, USA,\\\\nJune 7-12, 2015, pages 3128–3137. IEEE Computer\\\\nSociety.\\\\nAmirhossein Kazemnejad, Mohammadreza Salehi, and\\\\nMahdieh Soleymani Baghshah. 2020.\\\\nParaphrase\\\\ngeneration by learning how to edit from samples. In\\\\nProceedings of the 58th Annual Meeting of the Asso-\\\\nciation for Computational Linguistics, pages 6010–\\\\n6021, Online. Association for Computational Lin-\\\\nguistics.\\\\nUrvashi Khandelwal, Angela Fan, Dan Jurafsky, Luke\\\\nZettlemoyer,\\\\nand Mike Lewis. 2020a.\\\\nNear-\\\\nest neighbor machine translation.\\\\narXiv preprint\\\\narXiv:2010.00710.\\\\nUrvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke\\\\nZettlemoyer, and Mike Lewis. 2020b. Generaliza-\\\\ntion through memorization: Nearest neighbor lan-\\\\nguage models. In 8th International Conference on\\\\nLearning Representations, ICLR 2020, Addis Ababa,\\\\nEthiopia, April 26-30, 2020. OpenReview.net.\\\\nPhilipp Koehn, Franz J. Och, and Daniel Marcu. 2003.\\\\nStatistical phrase-based translation. In Proceedings\\\\nof the 2003 Human Language Technology Confer-\\\\nence of the North American Chapter of the Associa-\\\\ntion for Computational Linguistics, pages 127–133.\\\\nPhilipp Koehn and Jean Senellart. 2010. Convergence\\\\nof translation memory and statistical machine trans-\\\\nlation. In Proceedings of AMTA Workshop on MT\\\\nResearch and the Translation Industry, pages 21–31.\\\\nMojtaba Komeili, Kurt Shuster, and Jason Weston.\\\\n2021.\\\\nInternet-augmented dialogue generation.\\\\narXiv preprint arXiv:2107.07566.\\\\nKenton Lee, Ming-Wei Chang, and Kristina Toutanova.\\\\n2019.\\\\nLatent retrieval for weakly supervised\\\\nopen domain question answering.\\\\narXiv preprint\\\\narXiv:1906.00300.\\\\nMike Lewis, Marjan Ghazvininejad, Gargi Ghosh, Ar-\\\\nmen Aghajanyan, Sida Wang, and Luke Zettlemoyer.\\\\n2020a. Pre-training via paraphrasing. In Advances\\\\nin Neural Information Processing Systems 33: An-\\\\nnual Conference on Neural Information Processing\\\\nSystems 2020, NeurIPS 2020, December 6-12, 2020,\\\\nvirtual.\\\\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio\\\\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\\\\nrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rock-\\\\ntäschel, et al. 2020b.\\\\nRetrieval-augmented gen-\\\\neration for knowledge-intensive nlp tasks.\\\\narXiv\\\\npreprint arXiv:2005.11401.\\\\nJiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao,\\\\nand Bill Dolan. 2016a. A diversity-promoting ob-\\\\njective function for neural conversation models. In\\\\nNAACL, pages 110–119.\\\\nJuncen Li, Robin Jia, He He, and Percy Liang. 2018.\\\\nDelete, retrieve, generate: a simple approach to sen-\\\\ntiment and style transfer. In Proceedings of the 2018\\\\nConference of the North American Chapter of the\\\\nAssociation for Computational Linguistics: Human\\\\nLanguage Technologies, NAACL-HLT 2018, New\\\\nOrleans, Louisiana, USA, June 1-6, 2018, Volume\\\\n1 (Long Papers), pages 1865–1874. Association for\\\\nComputational Linguistics.\\\\nLiangyou Li, Andy Way, and Qun Liu. 2014.\\\\nA\\\\ndiscriminative framework of integrating translation\\\\nmemory features into smt.\\\\nIn Proceedings of the\\\\n11th Conference of the Association for Machine\\\\nTranslation in the Americas, volume 1, pages 249–\\\\n260.\\\\nLiangyou Li, Andy Way, and Qun Liu. 2016b. Phrase-\\\\nlevel combination of smt and tm using constrained\\\\nword lattice.\\\\nAssociation for Computational Lin-\\\\nguistics (ACL).\\\\nXiaoqing Li, Jiajun Zhang, and Chengqing Zong.\\\\n2016c. One sentence one model for neural machine\\\\ntranslation. arXiv preprint arXiv:1609.06490.\\\\nZekang Li, Cheng Niu, Fandong Meng, Yang Feng,\\\\nQian Li, and Jie Zhou. 2019.\\\\nIncremental trans-\\\\nformer with deliberation decoder for document\\\\ngrounded conversations. In Proceedings of the 57th\\\\nAnnual Meeting of the Association for Computa-\\\\ntional Linguistics, pages 12–21.\\\\nRongzhong Lian, Min Xie, Fan Wang, Jinhua Peng,\\\\nand Hua Wu. 2019. Learning to select knowledge\\\\nfor response generation in dialog systems.\\\\narXiv\\\\npreprint arXiv:1902.04911.\\\\nLemao Liu, Hailong Cao, Taro Watanabe, Tiejun Zhao,\\\\nMo Yu, and Conghui Zhu. 2012. Locally training\\\\nthe log-linear model for smt. In Proceedings of the\\\\n2012 Joint Conference on Empirical Methods in Nat-\\\\nural Language Processing and Computational Natu-\\\\nral Language Learning, pages 402–411.\\\\nLemao Liu, Tiejun Zhao, Taro Watanabe, Hailong Cao,\\\\nand Conghui Zhu. 2014. Discriminative training for\\\\nlog-linear based smt: Global or local methods. ACM\\\\nTransactions on Asian Language Information Pro-\\\\ncessing (TALIP), 13(4):1–25.\\\\nYanjun Ma, Yifan He, Andy Way, and Josef van Gen-\\\\nabith. 2011.\\\\nConsistent translation using discrim-\\\\ninative learning-a translation memory-inspired ap-\\\\nproach.\\\\nIn Proceedings of the 49th Annual Meet-\\\\ning of the Association for Computational Linguistics:\\\\nHuman Language Technologies, pages 1239–1248.\\\\nYuxian Meng, Xiaoya Li, Xiayu Zheng, Fei Wu, Xi-\\\\naofei Sun, Tianwei Zhang, and Jiwei Li. 2021.\\\\nFast nearest neighbor machine translation.\\\\narXiv\\\\npreprint arXiv:2105.14528.\\\\nFranz Josef Och. 2003. Minimum error rate training in\\\\nstatistical machine translation. In Proceedings of the\\\\n41st Annual Meeting of the Association for Compu-\\\\ntational Linguistics, pages 160–167, Sapporo, Japan.\\\\nAssociation for Computational Linguistics.\\\\nGaurav Pandey, Danish Contractor, Vineet Kumar, and\\\\nSachindra Joshi. 2018. Exemplar encoder-decoder\\\\nfor neural conversation generation. In ACL, pages\\\\n1329–1338.\\\\nAshwin Paranjape, Omar Khattab, Christopher Potts,\\\\nMatei Zaharia, and Christopher D Manning. 2021.\\\\nHindsight: Posterior-guided training of retrievers for\\\\nimproved open-ended generation.\\\\narXiv preprint\\\\narXiv:2110.07752.\\\\nHao Peng, Ankur P. Parikh, Manaal Faruqui, Bhuwan\\\\nDhingra, and Das Dipanjan. 2019. Text generation\\\\nwith exemplar-based adaptive decoding. In Proceed-\\\\nings of the Conference of the North American Chap-\\\\nter of the Association for Computational Linguistics:\\\\nHuman Language Technologies.\\\\nLianhui Qin, Michel Galley, Chris Brockett, Xiaodong\\\\nLiu, Xiang Gao, William B Dolan, Yejin Choi, and\\\\nJianfeng Gao. 2019. Conversing by reading: Con-\\\\ntentful neural conversation with on-demand machine\\\\nreading. In Proceedings of the 57th Annual Meet-\\\\ning of the Association for Computational Linguistics,\\\\npages 5427–5436.\\\\nMinghui Qiu, Feng-Lin Li, Siyu Wang, Xing Gao, Yan\\\\nChen, Weipeng Zhao, Haiqing Chen, Jun Huang,\\\\nand Wei Chu. 2017. Alime chat: A sequence to se-\\\\nquence and rerank based chatbot engine. In ACL,\\\\npages 498–503.\\\\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya\\\\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish\\\\nSastry, Amanda Askell, Pamela Mishkin, Jack Clark,\\\\nGretchen Krueger, and Ilya Sutskever. 2021. Learn-\\\\ning transferable visual models from natural lan-\\\\nguage supervision. In Proceedings of the 38th In-\\\\nternational Conference on Machine Learning, ICML\\\\n2021, 18-24 July 2021, Virtual Event, volume 139 of\\\\nProceedings of Machine Learning Research, pages\\\\n8748–8763. PMLR.\\\\nStephen Robertson and Hugo Zaragoza. 2009.\\\\nThe\\\\nprobabilistic relevance framework: BM25 and be-\\\\nyond. Now Publishers Inc.\\\\nLifeng Shang, Zhengdong Lu, and Hang Li. 2015. Neu-\\\\nral responding machine for short-text conversation.\\\\nIn ACL, pages 1577–1586.\\\\nMichel Simard and Pierre Isabelle. 2009. Phrase-based\\\\nmachine translation in a computer-assisted transla-\\\\ntion environment. Proceedings of the Twelfth Ma-\\\\nchine Translation Summit (MT Summit XII), pages\\\\n120–127.\\\\nJames Smith and Stephen Clark. 2009. Ebmt for smt:\\\\na new ebmt-smt hybrid. In Proceedings of the 3rd\\\\nInternational Workshop on Example-Based Machine\\\\nTranslation, pages 3–10. Citeseer.\\\\nHarold Somers. 2003.\\\\nTranslation memory systems.\\\\nBenjamins Translation Library, 35:31–48.\\\\nYiping Song, Rui Yan, Xiang Li, Dongyan Zhao, and\\\\nMing Zhang. 2016. Two are better than one: An en-\\\\nsemble of retrieval-and generation-based dialog sys-\\\\ntems. arXiv preprint arXiv:1610.07149.\\\\nYixuan Su, Zaiqiao Meng, Simon Baker, and Nigel Col-\\\\nlier. 2021a. Few-shot table-to-text generation with\\\\nprototype memory. In Findings of the Association\\\\nfor Computational Linguistics: EMNLP 2021, Vir-\\\\ntual Event / Punta Cana, Dominican Republic, 16-\\\\n20 November, 2021, pages 910–917. Association for\\\\nComputational Linguistics.\\\\nYixuan Su, David Vandyke, Simon Baker, Yan Wang,\\\\nand Nigel Collier. 2021b. Keep the primary, rewrite\\\\nthe secondary: A two-stage approach for paraphrase\\\\ngeneration. In Findings of the Association for Com-\\\\nputational Linguistics: ACL-IJCNLP 2021, pages\\\\n560–569, Online. Association for Computational\\\\nLinguistics.\\\\nYixuan Su, Yan Wang, Deng Cai, Simon Baker, Anna\\\\nKorhonen, and Nigel Collier. 2021c. PROTOTYPE-\\\\nTO-STYLE: dialogue generation with style-aware\\\\nediting on retrieval memory. IEEE ACM Trans. Au-\\\\ndio Speech Lang. Process., 29:2152–2161.\\\\nMarco Turchi, Matteo Negri, M Farajian, and Marcello\\\\nFederico. 2017. Continuous learning from human\\\\npost-edits for neural machine translation.\\\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\\\\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\\\\nKaiser, and Illia Polosukhin. 2017. Attention is all\\\\nyou need. In Advances in neural information pro-\\\\ncessing systems, pages 5998–6008.\\\\nOriol Vinyals and Quoc Le. 2015. A neural conversa-\\\\ntional model. In ICML (Deep Learning Workshop).\\\\nKun Wang, Chengqing Zong, and Keh-Yih Su. 2013.\\\\nIntegrating translation memory into phrase-based\\\\nmachine translation during decoding.\\\\nIn Proceed-\\\\nings of the 51st Annual Meeting of the Association\\\\nfor Computational Linguistics (Volume 1: Long Pa-\\\\npers), pages 11–21.\\\\nKun Wang, Chengqing Zong, and Keh-Yih Su. 2014.\\\\nDynamically integrating cross-domain translation\\\\nmemory into phrase-based machine translation dur-\\\\ning decoding.\\\\nIn Proceedings of COLING 2014,\\\\nthe 25th International Conference on Computational\\\\nLinguistics: Technical Papers, pages 398–408.\\\\nJason Weston, Emily Dinan, and Alexander Miller.\\\\n2018. Retrieve and reﬁne: Improved sequence gen-\\\\neration models for dialogue. In Proceedings of the\\\\n2018 EMNLP Workshop SCAI: The 2nd Interna-\\\\ntional Workshop on Search-Oriented Conversational\\\\nAI, pages 87–92.\\\\nYu Wu, Furu Wei, Shaohan Huang, Yunli Wang, Zhou-\\\\njun Li, and Ming Zhou. 2019. Response generation\\\\nby context-aware prototype editing. In Proceedings\\\\nof the AAAI Conference on Artiﬁcial Intelligence,\\\\nvolume 33, pages 7281–7288.\\\\nZeqiu Wu, Michel Galley, Chris Brockett, Yizhe Zhang,\\\\nXiang Gao, Chris Quirk, Rik Koncel-Kedziorski,\\\\nJianfeng Gao, Hannaneh Hajishirzi, Mari Ostendorf,\\\\net al. 2021. A controllable model of grounded re-\\\\nsponse generation. In Proceedings of the AAAI Con-\\\\nference on Artiﬁcial Intelligence, volume 35, pages\\\\n14085–14093.\\\\nMengzhou Xia, Guoping Huang, Lemao Liu, and\\\\nShuming Shi. 2019. Graph based translation mem-\\\\nory for neural machine translation. In Proceedings\\\\nof the AAAI Conference on Artiﬁcial Intelligence,\\\\nvolume 33, pages 7297–7304.\\\\nFei Xiao, Liang Pang, Yanyan Lan, Yan Wang, Huawei\\\\nShen, and Xueqi Cheng. 2021. Transductive learn-\\\\ning for unsupervised text style transfer. In Proceed-\\\\nings of the 2021 Conference on Empirical Methods\\\\nin Natural Language Processing, EMNLP 2021, Vir-\\\\ntual Event / Punta Cana, Dominican Republic, 7-11\\\\nNovember, 2021, pages 2510–2521. Association for\\\\nComputational Linguistics.\\\\nJitao Xu, Josep M Crego, and Jean Senellart. 2020.\\\\nBoosting neural machine translation with similar\\\\ntranslations.\\\\nIn Proceedings of the 58th Annual\\\\nMeeting of the Association for Computational Lin-\\\\nguistics, pages 1580–1590.\\\\nLiu Yang, Junjie Hu, Minghui Qiu, Chen Qu, Jian-\\\\nfeng Gao, W Bruce Croft, Xiaodong Liu, Yelong\\\\nShen, and Jingjing Liu. 2019.\\\\nA hybrid retrieval-\\\\ngeneration neural conversation model. In Proceed-\\\\nings of the 28th ACM international conference on in-\\\\nformation and knowledge management, pages 1341–\\\\n1350.\\\\nJingyi Zhang, Masao Utiyama, Eiichiro Sumita, Gra-\\\\nham Neubig, and Satoshi Nakamura. 2018. Guiding\\\\nneural machine translation with retrieved translation\\\\npieces. In Proceedings of the 2018 Conference of the\\\\nNorth American Chapter of the Association for Com-\\\\nputational Linguistics: Human Language Technolo-\\\\ngies, Volume 1 (Long Papers), pages 1325–1335.\\\\nYizhe Zhang, Siqi Sun, Xiang Gao, Yuwei Fang, Chris\\\\nBrockett, Michel Galley, Jianfeng Gao, and Bill\\\\nDolan. 2021.\\\\nJoint retrieval and generation train-\\\\ning for grounded text generation.\\\\narXiv preprint\\\\narXiv:2105.06597.\\\\nZhuosheng Zhang, Kehai Chen, Rui Wang, Masao\\\\nUtiyama, Eiichiro Sumita, Zuchao Li, and Hai Zhao.\\\\n2020.\\\\nNeural machine translation with universal\\\\nvisual representation. In 8th International Confer-\\\\nence on Learning Representations, ICLR 2020, Ad-\\\\ndis Ababa, Ethiopia, April 26-30, 2020. OpenRe-\\\\nview.net.\\\\nVentsislav Zhechev and Josef Van Genabith. 2010.\\\\nSeeding statistical machine translation with trans-\\\\nlation memory output through tree-based structural\\\\nalignment.\\\\nIn Proceedings of the 4th Workshop\\\\non Syntax and Structure in Statistical Translation,\\\\npages 43–51.\\\\nXin Zheng, Zhirui Zhang, Junliang Guo, Shujian\\\\nHuang, Boxing Chen, Weihua Luo, and Jiajun Chen.\\\\n2021a. Adaptive nearest neighbor machine transla-\\\\ntion. arXiv preprint arXiv:2105.13022.\\\\nXin Zheng, Zhirui Zhang, Shujian Huang, Boxing\\\\nChen, Jun Xie, Weihua Luo, and Jiajun Chen. 2021b.\\\\nNon-parametric unsupervised domain adaptation for\\\\nneural machine translation. In Findings of the As-\\\\nsociation for Computational Linguistics: EMNLP\\\\n2021, pages 4234–4241.\\\\nKangyan Zhou, Shrimai Prabhumoye, and Alan W\\\\nBlack. 2018. A dataset for document grounded con-\\\\nversations. arXiv preprint arXiv:1809.07358.\\\\n' metadata={'Published': '2022-02-13', 'Title': 'A Survey on Retrieval-Augmented Text Generation', 'Authors': 'Huayang Li, Yixuan Su, Deng Cai, Yan Wang, Lemao Liu', 'Summary': 'Recently, retrieval-augmented text generation attracted increasing attention\\\\nof the computational linguistics community. Compared with conventional\\\\ngeneration models, retrieval-augmented text generation has remarkable\\\\nadvantages and particularly has achieved state-of-the-art performance in many\\\\nNLP tasks. This paper aims to conduct a survey about retrieval-augmented text\\\\ngeneration. It firstly highlights the generic paradigm of retrieval-augmented\\\\ngeneration, and then it reviews notable approaches according to different tasks\\\\nincluding dialogue response generation, machine translation, and other\\\\ngeneration tasks. Finally, it points out some important directions on top of\\\\nrecent methods to facilitate future research.'}\\n\", additional_kwargs={}, example=False)] /n/n/n/n\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 50%|█████     | 1/2 [00:03<00:03,  3.81s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[HumanMessage(content='You are a University Professor creating a test for advanced students. For each context, create a question that is specific to the context. Avoid creating generic or general questions.\\n\\nquestion: a question about the context.\\n\\nFormat the output as JSON with the following keys:\\nquestion\\n\\ncontext: page_content=\\'Active Retrieval Augmented Generation\\\\nZhengbao Jiang1∗\\\\nFrank F. Xu1∗\\\\nLuyu Gao1∗\\\\nZhiqing Sun1∗\\\\nQian Liu2\\\\nJane Dwivedi-Yu3\\\\nYiming Yang1\\\\nJamie Callan1\\\\nGraham Neubig1\\\\n1Language Technologies Institute, Carnegie Mellon University\\\\n2Sea AI Lab\\\\n3Meta AI Research\\\\n{zhengbaj,fangzhex,luyug,zhiqings,gneubig}@cs.cmu.edu\\\\nAbstract\\\\nDespite the remarkable ability of large lan-\\\\nguage models (LMs) to comprehend and gen-\\\\nerate language, they have a tendency to hal-\\\\nlucinate and create factually inaccurate out-\\\\nput.\\\\nAugmenting LMs by retrieving infor-\\\\nmation from external knowledge resources\\\\nis one promising solution.\\\\nMost existing\\\\nretrieval-augmented LMs employ a retrieve-\\\\nand-generate setup that only retrieves informa-\\\\ntion once based on the input.\\\\nThis is lim-\\\\niting, however, in more general scenarios in-\\\\nvolving generation of long texts, where con-\\\\ntinually gathering information throughout the\\\\ngeneration process is essential.\\\\nThere have\\\\nbeen some past efforts to retrieve informa-\\\\ntion multiple times while generating outputs,\\\\nwhich mostly retrieve documents at ﬁxed inter-\\\\nvals using the previous context as queries. In\\\\nthis work, we provide a generalized view of\\\\nactive retrieval augmented generation, meth-\\\\nods that actively decide when and what to re-\\\\ntrieve across the course of the generation. We\\\\npropose Forward-Looking Active REtrieval\\\\naugmented generation (FLARE), a generic\\\\nretrieval-augmented generation method which\\\\niteratively uses a prediction of the upcoming\\\\nsentence to anticipate future content, which is\\\\nthen utilized as a query to retrieve relevant doc-\\\\numents to regenerate the sentence if it contains\\\\nlow-conﬁdence tokens. We test FLARE along\\\\nwith baselines comprehensively over 4 long-\\\\nform knowledge-intensive generation tasks/-\\\\ndatasets. FLARE achieves superior or compet-\\\\nitive performance on all tasks, demonstrating\\\\nthe effectiveness of our method.1\\\\n1\\\\nIntroduction\\\\nGenerative language models (LMs) (Brown et al.,\\\\n2020; Ouyang et al., 2022; OpenAI, 2023; Chowd-\\\\nhery et al., 2022; Zhang et al., 2022; Touvron et al.,\\\\n2023) have become a foundational component in\\\\n∗Lead contributors.\\\\n1Code and datasets are available at https://github.com/\\\\njzbjyb/FLARE.\\\\nmany natural language processing (NLP) systems\\\\nwith their remarkable ability to comprehend and\\\\ngenerate language. Although LMs have memorized\\\\nsome amount of world knowledge observed during\\\\ntraining (Petroni et al., 2019; Roberts et al., 2020;\\\\nJiang et al., 2020), they still tend to hallucinate\\\\nand create imaginary content (Maynez et al., 2020;\\\\nZhou et al., 2021; OpenAI, 2023). To address the\\\\nissue of hallucination, one promising direction is to\\\\naugment generation with retrieval, which involves\\\\naugmenting parametric LMs with non-parametric\\\\nretrieval components that can look up relevant in-\\\\nformation from external knowledge resources such\\\\nas document corpora (Lewis et al., 2020; Izacard\\\\nand Grave, 2021; Khandelwal et al., 2020; Izacard\\\\net al., 2022; Jiang et al., 2022; Shi et al., 2023).\\\\nRetrieval-augmented LMs commonly use a\\\\nretrieve-and-generate setup where they retrieve doc-\\\\numents based on the user’s input (e.g. questions\\\\nin question answering), and then generate a com-\\\\nplete answer conditioning on the retrieved docu-\\\\nments (Lewis et al., 2020; Izacard and Grave, 2021;\\\\nIzacard et al., 2022; Jiang et al., 2022; Shi et al.,\\\\n2023). These single-time retrieval-augmented LMs\\\\nhave been found to outperform purely paramet-\\\\nric LMs, particularly for short-form knowledge-\\\\nintensive generation tasks such as factoid QA\\\\n(Kwiatkowski et al., 2019; Joshi et al., 2017) and\\\\nfact checking (Thorne et al., 2018), where the in-\\\\nformation needs are clear in the user’s input, and\\\\nit is sufﬁcient to retrieve relevant knowledge once\\\\nsolely based on the input.\\\\nIn recent years, increasingly powerful large LMs\\\\nhave demonstrated abilities in more complex tasks\\\\nthat involve generating long-form output, such as\\\\nlong-form QA (Fan et al., 2019; Stelmakh et al.,\\\\n2022), open-domain summarization (Cohen et al.,\\\\n2021; Hayashi et al., 2021; Giorgi et al., 2022),\\\\nand (chain-of-thought; CoT) reasoning (Wei et al.,\\\\n2022; Ho et al., 2020; Geva et al., 2021; Hendrycks\\\\net al., 2020). In contrast to short-form generation,\\\\narXiv:2305.06983v1  [cs.CL]  11 May 2023\\\\nGenerate a summary about Joe Biden.\\\\nSearch results:   !!\\\\n[1]: …\\\\n[2]: …\\\\nJoe Biden (born November 20, 1942) is the 46th president of the United States.\\\\nJoe Biden (born November 20, 1942) is the 46th president of the United States.\\\\nHe graduated from the University of Delaware in 1965 with a Bachelor of Arts \\\\nin history and political science\\\\nJoe Biden attended the University of Pennsylvania, where he earned a law \\\\ndegree.\\\\nRetriever\\\\nInput\\\\nStep 1\\\\nSearch results:   !\"!\\\\n[1]: …\\\\n[2]: …\\\\n\"##\\\\n##\\\\n\"#$\\\\n#$\\\\n$\\\\nStep 2\\\\nJoe Biden announced his candidacy for the 2020 presidential election on April \\\\n25, 2019.\\\\nJoe Biden announced his candidacy for the 2020 presidential election on August \\\\n18, 2019.\\\\n\"#%\\\\n#%\\\\nStep 3\\\\nSearch results:   !\"\"\\\\n[1]: …\\\\n[2]: …\\\\nRetrieved\\\\ndocuments\\\\nLM\\\\nGeneration\\\\n$\\\\n%$\\\\n%%\\\\nFigure 1: An illustration of forward-looking active retrieval augmented generation (FLARE). Starting with the\\\\nuser input x and initial retrieval results Dx, FLARE iteratively generates a temporary next sentence (shown in\\\\ngray italic) and check whether it contains low-probability tokens (indicated with underline). If so (step 2 and 3),\\\\nthe system retrieves relevant documents and regenerates the sentence.\\\\nlong-form generation presents complex informa-\\\\ntion needs that are not always evident from the in-\\\\nput alone. Similar to how humans gradually gather\\\\ninformation as we create content such as papers,\\\\nessays, or books, long-form generation with LMs\\\\nwould require gathering multiple pieces of knowl-\\\\nedge throughout the generation process. For exam-\\\\nple in open-domain summarization (Giorgi et al.,\\\\n2022), the goal is to generate a summary about a\\\\nparticular topic by retrieving references from the\\\\nopen web. The initial retrieval based on the topic\\\\nname (e.g., Joe Biden) may not cover all aspects\\\\nand details. Therefore, it is crucial to retrieve ex-\\\\ntra information as needed during the generation\\\\nprocess, such as when generating a certain aspect\\\\n(e.g., the education history of Joe Biden) or a spe-\\\\nciﬁc detail (e.g., when did Joe Biden announce his\\\\ncandidacy for the 2020 presidential campaign).\\\\nSeveral attempts have been made to build sys-\\\\ntems that retrieve multiple times throughout genera-\\\\ntion. These attempts include methods that passively\\\\nutilize the past context (e.g., previous sentences or\\\\ntokens) to retrieve additional information at a ﬁxed\\\\ninterval (e.g., every sentence or every few tokens)\\\\n(Khandelwal et al., 2020; Borgeaud et al., 2022;\\\\nRam et al., 2023; Trivedi et al., 2022) which might\\\\nnot accurately reﬂect what LMs intend to gener-\\\\nate in the future or retrieve at inappropriate points.\\\\nSome works in multihop QA address multiple infor-\\\\nmation needs by decomposing the full question into\\\\nsub-questions, each of which is used to retrieve ex-\\\\ntra information (Press et al., 2022; Yao et al., 2022;\\\\nKhot et al., 2022; Khattab et al., 2022).\\\\nWe ask the following question in this paper: can\\\\nwe create a simple and generic retrieval-augmented\\\\nLM that actively decides when and what to retrieve\\\\nthroughout the generation process, and are appli-\\\\ncable to a variety of long-form generation tasks?\\\\nWe consider a new paradigm, active retrieval aug-\\\\nmented generation. Our hypothesis regarding when\\\\nto retrieve is that LMs should retrieve information\\\\nonly when they lack the required knowledge to\\\\navoid unnecessary or inappropriate retrieval that\\\\noccurs in passive retrieval-augmented LMs (Khan-\\\\ndelwal et al., 2020; Borgeaud et al., 2022; Ram\\\\net al., 2023; Trivedi et al., 2022). Given the obser-\\\\nvation that large LMs tend to be well-calibrated and\\\\nlow probability/conﬁdence often indicates a lack\\\\nof knowledge (Jiang et al., 2021; Kadavath et al.,\\\\n2022), we adopt an active retrieval strategy that\\\\nonly retrieves when LMs generate low-probability\\\\ntokens. When deciding what to retrieve, we argue\\\\nthat it is important to consider what LMs intend to\\\\ngenerate in the future, as the goal of active retrieval\\\\nis to beneﬁt future generations. Therefore, we pro-\\\\npose anticipating the future by generating a tempo-\\\\nrary next sentence, using it as a query to retrieve\\\\nrelevant documents, and then regenerating the next\\\\nsentence conditioning on the retrieved documents.\\\\nCombining the two aspects, we propose Forward-\\\\nLooking Active REtrieval augmented generation\\\\n(FLARE), as illustrated in Figure 1. FLARE iter-\\\\natively generates a temporary next sentence, use\\\\nit as the query to retrieve relevant documents if it\\\\ncontains low-probability tokens and regenerate the\\\\nnext sentence until reaches the end.\\\\nFLARE is applicable to any existing LMs at\\\\ninference time without additional training. Con-\\\\nsidering the impressive performance achieved by\\\\nGPT-3.5 (Ouyang et al., 2022) on a variety of\\\\ntasks, we examine the effectiveness of our meth-\\\\nods on text-davinci-003. We evaluate FLARE\\\\non 4 diverse tasks/datasets involving generating\\\\nlong outputs, including multihop QA (2WikiMul-\\\\ntihopQA), commonsense reasoning (StrategyQA),\\\\nlong-form QA (ASQA), and open-domain summa-\\\\nrization (WikiAsp) (Ho et al., 2020; Geva et al.,\\\\n2021; Stelmakh et al., 2022; Hayashi et al., 2021).\\\\nOver all tasks, FLARE achieves superior or com-\\\\npetitive performance compared to single-time and\\\\nmulti-time retrieval baselines, demonstrating the\\\\neffectiveness and generalizability of our method.\\\\n2\\\\nRetrieval-Augmented Generation\\\\nIn this section, we formally deﬁne single-time\\\\nretrieval-augmented generation and propose the\\\\nframework of active retrieval augmented generation\\\\nthat decides when and what to retrieve throughout\\\\nthe generation.\\\\n2.1\\\\nNotations and Deﬁnitions\\\\nGiven a user input x and a document corpus D =\\\\n{di}|D|\\\\ni=1 (such as all Wikipedia articles), the goal of\\\\nretrieval-augmented LMs is to generate the answer\\\\ny = [s1, s2, ..., sm] = [w1, w2, ..., wn] containing\\\\nm sentences or n tokens leveraging information\\\\nretrieved from the corpus.\\\\nIn retrieval-augmented LM, the LM typically\\\\npairs with a retriever that can retrieve a list of\\\\ndocuments Dq = ret(q) for a query q; the LM\\\\nconditions on both the user input x and retrieved\\\\ndocuments Dq to generate the answer. Since we\\\\nfocus on examining various methods of determin-\\\\ning when and what to retrieve, we follow exist-\\\\ning methods (Ram et al., 2023; Trivedi et al.,\\\\n2022) to prepend the retrieved documents before\\\\nthe user input to aid future generation for both\\\\nbaselines and our method for fair comparisons:\\\\ny = LM([Dq, x]), where [·, ·] is concatenation fol-\\\\nlowing the speciﬁed order.\\\\n2.2\\\\nSingle-time Retrieval-Augmented\\\\nGeneration\\\\nThe most common choice is to directly use the user\\\\ninput as the query for retrieval and generate the\\\\ncomplete answer at once y = LM([Dx, x]) (Chen\\\\net al., 2017; Guu et al., 2020; Lewis et al., 2020;\\\\nIzacard and Grave, 2021; Sachan et al., 2021; Lee\\\\net al., 2021; Jiang et al., 2022; Izacard et al., 2022;\\\\nShi et al., 2023).\\\\n2.3\\\\nActive Retrieval Augmented Generation\\\\nTo aid long-form generation with retrieval, we pro-\\\\npose active retrieval augmented generation. It is a\\\\ngeneric framework that actively decides when and\\\\nwhat to retrieve through the generation process,\\\\nresulting in the interleaving of retrieval and genera-\\\\ntion. Formally, at step t(t ≥ 1), the retrieval query\\\\nqt is formulated based on both the user input x and\\\\npreviously generated output y<t = [y0, ..., yt−1]:\\\\nqt = qry(x, y<t),\\\\nwhere qry(·) is the query formulation function. At\\\\nthe start of the generation (t = 1), the previous\\\\ngeneration is empty (y<1 = ∅), and the user input\\\\nis used as the initial query (q1 = x). Given the re-\\\\ntrieved documents Dqt, LMs continually generate\\\\nthe answer until the next retrieval is triggered or\\\\nreaches the end:\\\\nyt = LM([Dqt, x, y<t]),\\\\nwhere yt represents the generated tokens at the\\\\ncurrent step t, and the input to LMs is the concate-\\\\nnation of the retrieved documents Dqt, the user\\\\ninput x, and the previous generation y<t. At each\\\\nstep, we discard previously retrieved documents\\\\n∪t′<tDqt′ and only use the retrieved documents\\\\nfrom the current step to condition the next gener-\\\\nation to prevent reaching the input length limit of\\\\nLMs.\\\\n3\\\\nFLARE: Forward-Looking Active\\\\nREtrieval Augmented Generation\\\\nOur intuition is that (1) LMs should only retrieve\\\\ninformation when they do not have the necessary\\\\nknowledge to avoid unnecessary or inappropriate\\\\nretrieval, and (2) the retrieval queries should re-\\\\nﬂect the intents of future generations. Therefore,\\\\nWe propose two forward-looking active retrieval\\\\naugmented generation (FLARE) methods to im-\\\\nplement the active retrieval augmented generation\\\\nframework. Inspired by Toolformer (Schick et al.,\\\\n2023), the ﬁrst method prompts the LM to generate\\\\nretrieval queries when necessary while generating\\\\nthe answer using retrieval-encouraging instructions,\\\\ndenoted as FLAREinstruct. The second method di-\\\\nrectly uses the LM’s generation as search queries,\\\\ndenoted as FLAREdirect, which iteratively gener-\\\\nates the next sentence to gain insight into the future\\\\ntopic, and if uncertain tokens are present, retrieves\\\\nrelevant documents to regenerate the next sentence.\\\\n3.1\\\\nFLARE with Retrieval Instructions\\\\nA straightforward way of expressing information\\\\nneeds for retrieval is to generate “[Search(query)]”\\\\nwhen additional information is needed (Schick\\\\net al., 2023), e.g., “The colors on the ﬂag of\\\\nGhana have the following meanings. Red is for\\\\n[Search(Ghana ﬂag red meaning)] the blood of mar-\\\\ntyrs, ...” When working with GPT-3.5 models that\\\\noffer only API access, we elicit such behavior by\\\\nfew-shot prompting (Brown et al., 2020).\\\\nSpeciﬁcally, for a downstream task, we place\\\\nthe search-related instruction and exemplars at the\\\\nbeginning as skill 1, followed by the instruction and\\\\nexemplars of the downstream task as skill 2. Given\\\\na test case, we ask LMs to combine skills 1 and 2 to\\\\ngenerate search queries while performing the task.\\\\nThe structure of the prompt is shown in Prompt 3.1,\\\\nand further details can be found in Prompt C.1.\\\\nPrompt 3.1: retrieval instructions\\\\nSkill 1. An instruction to guide LMs to generate search\\\\nqueries.\\\\nSeveral search-related exemplars.\\\\nSkill 2.\\\\nAn instruction to guide LMs to perform a\\\\nspeciﬁc downstream task (e.g., multihop QA).\\\\nSeveral task-related exemplars.\\\\nAn instruction to guide LMs to combine skills 1\\\\nand 2 for the test case.\\\\nThe input of the test case.\\\\nAs shown in Figure 2, when the LM generates\\\\n“[Search(query)]” (shown in gray italic), we stop\\\\nthe generation and use the query terms to retrieve\\\\nrelevant documents, which are prepended before\\\\nthe user input to aid future generation until the next\\\\nsearch query is generated or reaches the end.\\\\nSearch results:   !!\\\\n[1]: …\\\\n[2]: …\\\\nJoe Biden attended\\\\nSearch results:   !\"!\\\\n[1]: …\\\\n[2]: …Search results:   !\"\"\\\\n[1]: …\\\\n[2]: …\\\\n[Search(Joe Biden University)]\\\\n[Search(Joe Biden degree)]\\\\nthe University of Pennsylvania, where he earned\\\\na law degree.\\\\nGenerate a summary about Joe Biden.\\\\nInput\\\\n$\\\\n&$\\\\n&#\\\\n%$\\\\n&%\\\\n%%\\\\nGeneration\\\\nRetriever\\\\n$\\\\n%$\\\\n%%\\\\nFigure 2: An illustration of forward-looking active re-\\\\ntrieval augmented generation with retrieval instructions\\\\n(FLAREinstruct). It iteratively generates search queries\\\\n(shown in gray italic) to retrieve relevant information\\\\nto aid future generations.\\\\nWe found that LMs can effectively combine the\\\\ntwo skills and generate meaningful search queries\\\\nwhile performing the task. However, there are\\\\ntwo issues: (1) LMs tend to generate fewer search\\\\nqueries than necessary. (2) Generating excessive\\\\nsearch queries can disrupt answer generation and\\\\nadversely affect performance. We address these\\\\nissues using two methods respectively. First, we\\\\nincrease the logit of the token “[” by 2.0 to improve\\\\nthe chances of LMs generating “[Search(query)]”.\\\\nSecond, whenever LMs generate a search query in\\\\nFigure 2), we use it to retrieve relevant information,\\\\npromptly remove it from the generation, and gen-\\\\nerate the next few tokens while forbidding “[” by\\\\nadding a large negative value to the logit of “[”.\\\\n3.2\\\\nDirect FLARE\\\\nSince we cannot ﬁne-tune black-box LMs, we\\\\nfound queries generated by FLAREinstruct through\\\\nretrieval instructions might not be reliable. There-\\\\nfore, we propose a more direct way of forward-\\\\nlooking active retrieval that uses the next sentence\\\\nto decide when and what to retrieve.\\\\n3.2.1\\\\nConﬁdence-based Active Retrieval\\\\nAs shown in Figure 1, at step t, we ﬁrst generate a\\\\ntemporary next sentence ˆst = LM([x, y<t]) with-\\\\nout conditioning on retrieved documents. Then we\\\\ndecide whether to trigger retrieval and formulate\\\\nqueries based on ˆst. If the LM is conﬁdent about ˆst,\\\\nwe accept it without retrieving additional informa-\\\\ntion; if not, we use ˆst to formulate search queries\\\\nqt to retrieve relevant documents, and then regen-\\\\nerate the next sentence st. The reason we utilize\\\\nsentences as the basis of our iteration is due to their\\\\nsigniﬁcance as semantic units that are neither too\\\\nshort nor too lengthy like phrases and paragraphs.\\\\nHowever, it is worth noting that our approach can\\\\nalso be employed using phrases, paragraphs, or\\\\nﬁxed-size windows as the basis.\\\\nSince LMs tend to be well-calibrated that low\\\\nprobability/conﬁdence often indicates a lack of\\\\nknowledge (Kadavath et al., 2022; Jiang et al.,\\\\n2021), we actively trigger retrieval if any token\\\\nof ˆst has a probability lower than a threshold\\\\nθ ∈ [0, 1]. θ = 0 means that retrieval is never\\\\ntriggered, while θ = 1 triggers retrieval for every\\\\nsentence.\\\\nyt =\\\\n�\\\\nˆst\\\\nif all tokens of ˆst have probs ≥ θ\\\\nst = LM([Dqt, x, y<t])\\\\notherwise\\\\nwhere the query qt is formulated based on ˆst.\\\\n3.2.2\\\\nConﬁdence-based Query Formulation\\\\nOne way to perform retrieval is to directly use the\\\\nnext sentence ˆst as the query qt. This shares a sim-\\\\nilar spirit with existing methods that use generated\\\\nhypothetical titles or paragraphs (Gao et al., 2022;\\\\nSun et al., 2022) from LMs instead of the origi-\\\\nnal input question as the retrieval query (Gao et al.,\\\\n2022; Mao et al., 2021). We transfer and generalize\\\\nsuch techniques to long-form generation scenarios\\\\nwhere active information access is essential.\\\\nEmpirically, we found retrieving with the next\\\\nsentence achieves signiﬁcantly better results than\\\\nwith the previous context, as to be shown later in\\\\nsubsection 6.2. However, it has a risk of perpetuat-\\\\ning errors contained in it. For example, if the LM\\\\nproduces the sentence “Joe Biden attended the Uni-\\\\nversity of Pennsylvania” instead of the correct fact\\\\nthat he attended the University of Delaware, using\\\\nthis erroneous sentence as a query could prompt the\\\\nretriever to retrieve irrelevant information, which\\\\ncould potentially mislead future generations. We\\\\npropose two simple methods to overcome this issue\\\\nas illustrated in Figure 3.\\\\nMasked sentences as implicit queries.\\\\nThe ﬁrst\\\\nmethod masks out low-conﬁdence tokens in ˆst with\\\\nprobabilities below a threshold β ∈ [0, 1], where a\\\\nhigher β results in more aggressive masking. This\\\\nremoves potential distractions from the sentence to\\\\nimprove retrieval accuracy.\\\\nJoe Biden attended the University of Pennsylvania, \\\\nwhere he earned a law degree.\\\\nAsk a question to which the answer is “the University of Pennsylvania”\\\\nAsk a question to which the answer is “a law degree”\\\\nWhat university did Joe Biden attend?\\\\nWhat degree did Joe Biden earn?\\\\nimplicit query \\\\nby masking\\\\nexplicit query by\\\\n question generation\\\\nJoe Biden attended  , where he earned  .\\\\nLM such as ChatGPT\\\\nFigure 3: Implicit the explicit query formulation. To-\\\\nkens with low probabilities are marked with underlines.\\\\nGenerated questions as explicit queries.\\\\nAn-\\\\nother method is to generate explicit questions that\\\\ntarget the low-conﬁdent span in ˆst. For example, if\\\\nthe LM is uncertain about “the University of Penn-\\\\nsylvania”, a question like “Which university did Joe\\\\nBiden attend?” can help retrieve relevant informa-\\\\ntion. Self-ask (Press et al., 2022) achieved this by\\\\nmanually inserting follow-up questions into down-\\\\nstream task exemplars as shown later in Prompt 4.1,\\\\nwhich requires task-speciﬁc annotation efforts. In-\\\\nstead, we developed a universal approach that gen-\\\\nerates questions for low-conﬁdence spans without\\\\nadditional annotation. Speciﬁcally, We ﬁrst extract\\\\nall spans from ˆst with probabilities below β. For\\\\neach extracted span z, we prompt gpt-3.5-turbo\\\\nto generate a question qt,z that can be answered\\\\nwith the span, using the following prompt:\\\\nPrompt 3.2: zero-shot question generation\\\\nUser input x.\\\\nGenerated output so far y≤t.\\\\nGiven the above passage, ask a question to which\\\\nthe answer is the term/entity/phrase “z”.\\\\nWe retrieve using each generated question and\\\\ninterleave the returned documents into a single\\\\nranking list to aid future generations. In summary,\\\\nqueries qt are formulated based on ˆst as follows:\\\\nqt =\\\\n�\\\\n∅\\\\nif all tokens of ˆst have probs ≥ θ\\\\nmask(ˆst) or qgen(ˆst)\\\\notherwise\\\\n3.3\\\\nImplementation Details\\\\nWe validate our method using one of the most ad-\\\\nvanced GPT-3.5 LMs text-davinci-003 by itera-\\\\ntively querying their API.2\\\\n2https://api.openai.com/v1/completions in April\\\\n2023.\\\\nThe initial query.\\\\nFLARE starts with the user\\\\ninput x as the initial query to retrieve documents\\\\nto generate the ﬁrst sentence ˆs1 = LM([Dx, x])\\\\nto bootstrap the iterative generation process. For\\\\nthe following steps, the temporary forward-looking\\\\nsentence is generated without retrieved documents.\\\\nSentence tokenization.\\\\nFor each step t, we gen-\\\\nerate 64 tokens which are longer than most sen-\\\\ntences, and use NLTK sentence tokenizer3 to ex-\\\\ntract the ﬁrst sentence and discard the rest.\\\\nDocument corpus and retrievers.\\\\nSince we fo-\\\\ncus on the integration of retrieval and generation in\\\\nthis paper, we use off-the-shelf retrievers that take\\\\nqueries as inputs and return a list of relevant docu-\\\\nments. For datasets that mainly rely on knowledge\\\\nfrom Wikipedia, we use the Wikipedia dump from\\\\nKarpukhin et al. (2020) where articles are divided\\\\ninto 100-token passages as the document corpus\\\\nand employ BM25 (Robertson and Zaragoza, 2009)\\\\nas the retriever. For datasets that rely on knowledge\\\\nfrom the open web, we use the Bing search engine\\\\nas our retriever.4\\\\nRetrieved document formatting.\\\\nMultiple re-\\\\ntrieved documents are linearized according to their\\\\nranking and then added to the beginning of the user\\\\ninput using the following format:\\\\nPrompt 3.3: document formatting\\\\nSearch results:\\\\n[1] Document 1\\\\n[2] Document 2\\\\n...\\\\nThe user input x\\\\nEfﬁciency\\\\nAs shown later in subsection 6.2, on\\\\naverage retrieval is triggered for 30% ∼ 60% of\\\\nsentences depending on downstream tasks.\\\\nIn\\\\ncomparision, KNN-LM (Khandelwal et al., 2020)\\\\nretrieves for every token, RETRO or IC-RALM\\\\n(Borgeaud et al., 2022; Ram et al., 2023) retriev-\\\\ners every 4∼32 tokens, and IRCoT (Trivedi et al.,\\\\n2022) retrieves for every sentence.\\\\nCompared\\\\nto single-time retrieval, however, interleaving re-\\\\ntrieval and generation with a naive implementation\\\\nindeed increases overheads, which we will discuss\\\\nin the limitation section (section 8).\\\\n3https://www.nltk.org/api/nltk.tokenize.\\\\nPunktSentenceTokenizer.html\\\\n4https://www.microsoft.com/en-us/bing/apis/\\\\nbing-web-search-api\\\\n4\\\\nMulti-time Retrieval Baselines\\\\nExisting passive multi-time retrieval-augmented\\\\nLMs (Khandelwal et al., 2020; Ram et al., 2023;\\\\nTrivedi et al., 2022; Press et al., 2022; Yao et al.,\\\\n2022) can also be formulated using our framework\\\\n(subsection 2.3). In this section, we formally in-\\\\ntroduce three baseline categories based on when\\\\nand what to retrieve. These baselines are not exact\\\\nreproductions of the corresponding paper because\\\\nmany design choices differ among previous works\\\\nwhich makes direct comparisons impossible. We\\\\nexcluded irrelevant designs and ensured that we\\\\nimplemented them using the same settings, with\\\\nthe only variation being when and what to retrieve.\\\\nPrevious-window\\\\napproaches trigger retrieval\\\\nevery l tokens, where l represents the window size.\\\\nGenerated tokens from the previous window are\\\\nused as the query:\\\\nqt = yt−1\\\\n(t ≥ 2),\\\\nyt = [w(t−1)l+1, ..., wtl].\\\\nSome existing methods in this category are RETRO\\\\n(Borgeaud et al., 2022), IC-RALM (Ram et al.,\\\\n2023), which retrieve every few tokens, and KNN-\\\\nLM (Khandelwal et al., 2020), which retrieves ev-\\\\nery token.5\\\\nPrevious-sentence\\\\napproaches trigger retrieval\\\\nevery sentence and use the previous sentence as the\\\\nquery:\\\\nqt = yt−1\\\\n(t ≥ 2),\\\\nyt = st.\\\\nIRCoT (Trivedi et al., 2022) belongs to this cate-\\\\ngory.\\\\nQuestion decomposition\\\\napproaches manually\\\\nannotated task-speciﬁc exemplars to guide LMs\\\\nto generate decomposed sub-questions while pro-\\\\nducing outputs. For example, self-ask (Press et al.,\\\\n2022), a method in this category, manually inserts\\\\nfollow-up questions in exemplars:\\\\n5Since KNN-LM uses the contextualized representation\\\\ncorresponding to the current decoding position to retrieve rel-\\\\nevant information which encodes all previous tokens. Strictly\\\\nspeaking, qt should be y<t.\\\\nPrompt 4.1: multihop QA with self-ask\\\\nQuestion: Who lived longer, Theodor Haecker or Harry\\\\nVaughan Watkins?\\\\nAre follow up questions needed here: Yes.\\\\nFollow up: How old was Theodor Haecker when he died?\\\\nIntermediate answer: Theodor Haecker was 65 years old\\\\nwhen he died.\\\\nFollow up: How old was Harry Vaughan Watkins when he\\\\ndied?\\\\nIntermediate answer: Harry Vaughan Watkins was 69 years\\\\nold when he died.\\\\nSo the ﬁnal answer is: Harry Vaughan Watkins.\\\\nFor the test case, retrieval is triggered dynami-\\\\ncally whenever the model generates a sub-question\\\\n(e.g., “follow up” in self-ask).\\\\nThe aforementioned three approaches are capa-\\\\nble of retrieving additional information while gen-\\\\nerating. However, they have notable drawbacks:\\\\n(1) ﬁxed-interval approaches use previously gener-\\\\nated tokens as queries which might not reﬂect what\\\\nLMs intend to generate in the future. (2) Retriev-\\\\ning information at a ﬁxed interval can be inefﬁcient\\\\nbecause it might occur at inappropriate points. (3)\\\\nQuestion decomposition approaches require task-\\\\nspeciﬁc prompt engineering, which restricts their\\\\ngeneralizability in new tasks.\\\\n5\\\\nExperimental Setup\\\\nWe evaluate the effectiveness of FLARE on 4 di-\\\\nverse knowledge-intensive tasks using few-shot in-\\\\ncontext learning (Radford et al., 2019; Brown et al.,\\\\n2020), as summarized in Table 6 of Appendix A.\\\\nTo ensure fair head-to-head comparisons, we com-\\\\npare the results of FLARE with baselines using\\\\nthe same setting, namely, the same in-context ex-\\\\nemplars, prompt format, retriever, and document\\\\ncorpus. We follow previous works (Trivedi et al.,\\\\n2022) to sub-sample at most 500 examples from\\\\neach dataset due to the cost of running experi-\\\\nments. The hyperparameters of FLARE are se-\\\\nlected based on the development set and listed in\\\\nTable 8. FLARE refers to FLAREdirect if not specif-\\\\nically stated. For previous-window approaches,\\\\nwe follow Ram et al. (2023) to use a window size\\\\nl = 16 in our experiments.\\\\n5.1\\\\nMultihop QA\\\\nDataset\\\\nThe goal of multihop QA is to answer\\\\ncomplex questions through a process of informa-\\\\ntion retrieval and reasoning (Yang et al., 2018; Ho\\\\net al., 2020). For instance, to answer “Why did\\\\nthe founder of Versus die?”, we must ﬁrst identify\\\\nwho founded Versus and subsequently determine\\\\nthe cause of their death. Multihop QA also uniﬁes\\\\ninto long-form generation when solved with the\\\\nstate-of-the-art CoT methods (Wei et al., 2022).\\\\nWe use 2WikiMultihopQA (Ho et al., 2020)\\\\nwhich contains 2-hop complex questions sourced\\\\nfrom Wikipedia articles that require composition,\\\\ncomparison, or inference.\\\\nSettings\\\\nWe follow Wang et al. (2022) to gen-\\\\nerate both the chain-of-thought reasoning process\\\\nand the ﬁnal answer. For the above case, the output\\\\nwe aim to generate is “The founder of Versus was\\\\nGianni Versace. Gianni Versace was shot and killed\\\\non the steps of his Miami Beach mansion on July\\\\n15, 1997. So the answer is shot.” We use 8 exem-\\\\nplars from Trivedi et al. (2022) listed in Prompt C.2\\\\nfor in-context learning, BM25 as the retriever, and\\\\nWikipedia articles as the retrieval corpus. Similar to\\\\nthe observation in Trivedi et al. (2022), we found\\\\nincorporating retrieval results for exemplars im-\\\\nproves the performance, we use the input x of each\\\\nexemplar to retrieve several documents and then\\\\nadd them using the format in Prompt 3.3. We found\\\\nincreasing the number of retrieval documents often\\\\nincreases performance. Therefore, we use the max-\\\\nimum number of documents that can ﬁt within the\\\\ninput length limit of text-davinci-003, which is\\\\n2 for 2WikiMultihopQA.\\\\nEvaluation\\\\nWe use regular expressions to extract\\\\nthe ﬁnal answer (i.e., “shot” in the above example)\\\\nfrom the generated output and compare it with the\\\\nreference answer using answer-level exact match\\\\n(EM), and token-level F1, precision, and recall.\\\\n5.2\\\\nCommonsense Reasoning\\\\nDataset\\\\nCommonsense reasoning requires sys-\\\\ntems to utilize both world and commonsense knowl-\\\\nedge to generate an answer (Talmor et al., 2019;\\\\nGeva et al., 2021). For example, to answer “Would\\\\na pear sink in water?”, we must have the common-\\\\nsense understanding that we need to consider their\\\\ndensity. We use StrategyQA (Geva et al., 2021) as\\\\nthe testbed which is a collection of crowdsourced\\\\nyes/no questions that require multi-step reasoning.\\\\nSettings\\\\nWe follow Wei et al. (2022) to generate\\\\nboth the chain-of-thought reasoning process and\\\\nthe ﬁnal yes/no answer, which for the above case is\\\\n“The density of a pear is about 0.6g/cm3, which is\\\\nless than water. Objects less dense than water ﬂoat.\\\\nThus, a pear would ﬂoat. So the ﬁnal answer is no.”\\\\nWe use 6 exemplars from Wei et al. (2022) listed\\\\nin Prompt C.3, BM25 over the Wikipedia corpus,\\\\nand 3 retrieved documents to run experiments.\\\\nEvaluation\\\\nWe extract the yes/no answer and\\\\nmatch it against the gold answer using exact match.\\\\n5.3\\\\nLong-form QA\\\\nDataset\\\\nLong-form QA aims to generate compre-\\\\nhensive answers to questions seeking complex in-\\\\nformation (Fan et al., 2019; Stelmakh et al., 2022).\\\\nThe following question “Where do the Philadelphia\\\\nEagles play their home games?” could be asking\\\\nabout the city, sports complex, or stadium of their\\\\nhome games. We use ASQA (Stelmakh et al., 2022)\\\\nas our testbed where inputs are ambiguous ques-\\\\ntions with multiple interpretations, and outputs are\\\\ncomprehensive answers covering all.\\\\nSetting\\\\nTo answer ambiguous questions, systems\\\\nmust ﬁrst identify possible interpretations and then\\\\nprovide answers for each of them, which for the\\\\nabove case is “We need to consider the different\\\\npossible locations or venues that could be con-\\\\nsidered the home ﬁeld of the Philadelphia Eagles.\\\\nThese include the city, the sports complex, or the\\\\nstadium. Therefore, this question has 3 interpreta-\\\\ntions and the answers are: (1) The city is Philadel-\\\\nphia. (2) The sports complex is the South Philadel-\\\\nphia Sports Complex. (3) The stadium is the Lin-\\\\ncoln Financial Field stadium.” We found that in\\\\nmany cases, it is challenging even for humans to\\\\nidentify which aspect of the original question is\\\\nambiguous. Therefore, we created another setting\\\\nwhere we provide a brief and generic hint to guide\\\\nLMs to stay on track when generating interpreta-\\\\ntions and corresponding answers. The hint for the\\\\nabove case is “This question is ambiguous in terms\\\\nof which speciﬁc location or venue is being referred\\\\nto.” For both the original setting (ASQA) and the\\\\nsetting with hints (ASQA-hint), we manually anno-\\\\ntate 8 exemplars (Prompt C.4 and C.6), use BM25\\\\nover the Wikipedia corpus, and 3 retrieved docu-\\\\nments to run experiments.\\\\nEvaluation\\\\nWe use all metrics from Stelmakh\\\\net al. (2022), including EM, soft match us-\\\\ning a RoBERTa-based QA model (Disambig-F1),\\\\nROUGE (Lin, 2004), and an overall score combin-\\\\ning Disambig-F1 and ROUGE (DR).\\\\n5.4\\\\nOpen-domain Summarization\\\\nDataset\\\\nThe goal of open-domain summarization\\\\nis to generate a comprehensive summary about a\\\\nspeciﬁc topic by gathering information from the\\\\nopen web (Giorgi et al., 2022), e.g., “Generate a\\\\nsummary about Echo School (Oregon) including\\\\nthe following aspects: academics, history”. We\\\\nuse WikiAsp (Hayashi et al., 2021) as our testbed\\\\nwhich aims to generate aspect-based summaries\\\\nabout entities from 20 domains in Wikipedia.\\\\nSetting\\\\nThe original WikiAsp dataset is designed\\\\nfor multi-document summarization and provides a\\\\nlist of references to systems. We converted it into\\\\nthe open-domain setting by removing the associ-\\\\nated references and instead gathering information\\\\nfrom the open web. For the above case, the output\\\\nwe aim to generate is “# Academics. In 2008, 91%\\\\nof the school’s seniors received their high school\\\\ndiploma... # History. The class of 2008 was the\\\\n100th class in the school’s history.” where # is used\\\\nto indicate aspects. We manually annotate 4 exem-\\\\nplars (Prompt C.8), and use the Bing search engine\\\\nto retrieve 5 documents from the open web.6\\\\nEvaluation\\\\nWe compare system outputs with the\\\\ngold summary using ROUGE, named entity-based\\\\nF1, and UniEval (Zhong et al., 2022) which mea-\\\\nsures factual consistency based on prediction proba-\\\\nbility of a ﬁne-tuned T5 model (Raffel et al., 2020).\\\\n6\\\\nExperimental Results\\\\nWe ﬁrst report overall results across 4 tasks/datasets\\\\nand compare the performance of FLARE with all\\\\nthe baselines introduced in section 4. We then\\\\nrun ablation experiments to study the efﬁcacy of\\\\nvarious design choices of our method.\\\\n6.1\\\\nComparison with Baselines\\\\nOverall results.\\\\nThe overall performance of\\\\nFLARE and baseline across all tasks/datasets are\\\\nreported in Figure 4. FLARE outperforms all base-\\\\nline on all tasks/datasets, indicating that FLARE\\\\nis a generic method that can effectively retrieve\\\\nadditional information throughout the generation.\\\\nAmong various tasks and datasets, multihop QA\\\\nshows the most signiﬁcant improvement. This is\\\\nlargely due to the task’s clear deﬁnition and speciﬁc\\\\nobjective of producing the ﬁnal answer through a\\\\n2-hop reasoning process, which makes it easier\\\\nfor LMs to generate on-topic output. In contrast,\\\\nASQA and WikiAsp are less clearly deﬁned and\\\\nmore open-ended, which increases the difﬁculty of\\\\n6To avoid leaking, we exclude several Wikipedia-related\\\\ndomains listed in Table 7 from Bing’s search results.\\\\n0.0\\\\n20.0\\\\n40.0\\\\n60.0\\\\n80.0\\\\n2WikiMultihopQA\\\\nStrategyQA\\\\nASQA\\\\nASQA-hint\\\\nWikiAsp\\\\nNo ret.\\\\nSingle-time ret.\\\\nPrevious-window ret.\\\\nForward-Looking Active REtrieval augmented generation (FLARE)\\\\nFigure 4: Comparision between FLARE and baselines across all tasks/datasets. We report the primary metric for\\\\neach dataset: EM for 2WikiMultihopQA, StrategyQA, and ASQA, and UniEval for WikiAsp.\\\\nboth generation and evaluation. The improvement\\\\non ASQA-hint is larger than that of ASQA because\\\\nidentifying ambiguous aspects is challenging even\\\\nfor humans in many cases, and providing a generic\\\\nhint helps LMs to stay on topic.\\\\nThorough comparisons with baselines.\\\\nThe\\\\nperformance of all baselines discussed in section 4\\\\non 2WikiMultihopQA are reported in Table 1.\\\\nFLARE outperforms all baselines by a large mar-\\\\ngin, which conﬁrms that forward-looking active re-\\\\ntrieval is highly effective. Most multi-time retrieval-\\\\naugmented approaches outperform single-time re-\\\\ntrieval but with different margins. The improve-\\\\nment of retrieving using the previous sentence is\\\\nrelatively small which we hypothesize is mainly\\\\nbecause the previous sentence often describes en-\\\\ntities or relations that differ from those in the next\\\\nsentence in 2WikiMultihopQA. While the previous-\\\\nwindow approach might use the ﬁrst half of a sen-\\\\ntence as queries to retrieve information potentially\\\\nhelpful for generating the second half. Among all\\\\nbaselines, the question decomposition approach\\\\n(Press et al., 2022) achieves the best performance.\\\\nThis is not surprising since the in-context exem-\\\\nplars manually annotated with decomposed sub-\\\\nquestions (Prompt 4.1) guide LMs to generate suit-\\\\nable sub-questions that align with the topic/intent\\\\nof future generations. FLARE outperforms this\\\\nbaseline, indicating that manual exemplar anno-\\\\ntation is not necessary for effective future-aware\\\\nretrieval. The gap between FLAREinstruct and ques-\\\\ntion decomposition is large, indicating that teaching\\\\nLMs to generate search queries using task-generic\\\\nretrieval instructions and exemplars is challenging.\\\\nWe report all metrics for the other datasets in Ta-\\\\nble 2. Again, FLARE outperforms baselines with\\\\nrespect to all metrics. Retrieval using the previ-\\\\nous window underperforms single-time retrieval on\\\\n%steps/sentences with retrieval\\\\n0.0\\\\n20.0\\\\n40.0\\\\n60.0\\\\n80.0\\\\n0.0\\\\n25.0\\\\n50.0\\\\n75.0\\\\n100.0\\\\n2WikiMultihopQA\\\\nStrategyQA\\\\nFigure 5: Performance (EM) of FLARE with respect\\\\nto the percentage of steps/sentences with retrieval on\\\\n2WikiMultihopQA and StrategyQA.\\\\nASQA, which we hypothesize is because the previ-\\\\nous window does not accurately reﬂect the user’s\\\\nfuture intent. Since we focus on evaluating the\\\\nfactuality of the generation, metrics with an empha-\\\\nsis on factual content (such as EM, Disambig-F1,\\\\nUniEval) are more reliable than metrics computed\\\\nover all tokens (ROUGE-L).\\\\n6.2\\\\nAblation Study\\\\nWe study the efﬁcacy of various design choices\\\\nthrough ablation experiments.\\\\nImportance of forward-looking retrieval.\\\\nWe\\\\nﬁrst validate our hypothesis that forward-looking\\\\nretrieval is indeed more powerful than past-context-\\\\nbased retrieval. We run ablation experiments on\\\\n2WikiMultihopQA and ASQA-hint datasets com-\\\\nparing retrieval using the previous versus the next\\\\nsentence, by ensuring both methods are identical\\\\nexcept for the query used for retrieval. Speciﬁ-\\\\ncally, both methods retrieve every sentence and\\\\ndirectly use the complete sentence (without mask-\\\\ning or question generation) for retrieval. As shown\\\\nin Table 3, on both datasets, using the next sentence\\\\nto retrieve is clearly better than using the previous\\\\nsentence, conﬁrming our hypothesis.\\\\nMethods\\\\nEM\\\\nF1\\\\nPrec.\\\\nRec.\\\\nNo retrieval\\\\n28.2\\\\n36.8\\\\n36.5\\\\n38.6\\\\nSingle-time retrieval\\\\n39.4\\\\n48.8\\\\n48.6\\\\n51.5\\\\nMulti-time retrieval\\\\nPrevious-window (Borgeaud et al., 2022; Ram et al., 2023)∗\\\\n43.2\\\\n52.3\\\\n51.7\\\\n54.5\\\\nPrevious-sentence (Trivedi et al., 2022)∗\\\\n39.0\\\\n49.2\\\\n48.9\\\\n51.8\\\\nQuestion decomposition (Press et al., 2022; Yao et al., 2022)∗\\\\n47.8\\\\n56.4\\\\n56.1\\\\n58.6\\\\nFLAREinstruct (ours)\\\\n42.4\\\\n49.8\\\\n49.1\\\\n52.5\\\\nFLAREdirect (ours)\\\\n51.0\\\\n59.7\\\\n59.1\\\\n62.6\\\\nTable 1: Comparisons between FLARE and baselines on 2WikiMultihopQA. ∗Reimplemented for fair compar-\\\\nisons.\\\\nDatasets\\\\nStrategyQA\\\\nASQA\\\\nASQA-hint\\\\nWikiAsp\\\\nMetrics\\\\nEM\\\\nEM D-F1 R-L DR\\\\nEM D-F1 R-L DR\\\\nUniEval E-F1 R-L\\\\nNo retrieval\\\\n72.9\\\\n33.8 24.2 33.3 28.4\\\\n40.1 32.5 36.4 34.4\\\\n47.1\\\\n14.1 26.4\\\\nSingle-time retrieval\\\\n68.6\\\\n40.0 27.1 34.0 30.4\\\\n43.2 34.8 37.4 36.0\\\\n52.4\\\\n17.4 26.9\\\\nMulti-time retrieval\\\\nPrevious-window\\\\n71.2\\\\n39.9 27.0 34.3 30.4\\\\n43.7 35.7 37.5 36.6\\\\n51.8\\\\n18.1 27.3\\\\nFLARE (ours)\\\\n77.3\\\\n41.3 28.2 34.3 31.1\\\\n46.2 36.7 37.7 37.2\\\\n53.4\\\\n18.9 27.6\\\\nTable 2: Comparison between FLARE and baselines on StrategyQA, ASQA, ASQA-hint, WikiAsp wrt. corre-\\\\nsponding metrics. D-F1 is Disambig-F1, R-L is ROUGE-L, and E-F1 is named entity-based F1.\\\\n2WikiMultihopQA\\\\nASQA-hint\\\\nEM\\\\nF1\\\\nPrec. Rec.\\\\nEM D-F1 R-L DR\\\\nPrevious 39.0 49.2 48.9 51.8\\\\n42.5 34.1 36.9 35.5\\\\nNext\\\\n48.8 57.6 57.1 60.5\\\\n45.9 35.7 37.5 36.6\\\\nTable 3: A head-to-head comparison between using the\\\\nprevious sentence and the next sentence for retrieval.\\\\nImportance of active retrieval.\\\\nNext, we inves-\\\\ntigate the relationship between performance and\\\\nthe active retrieval threshold θ. To alter our method\\\\nfrom not retrieving anything to retrieving every sen-\\\\ntence, we adjusted the conﬁdence threshold θ used\\\\nto determine when to trigger retrieval from 0 to\\\\n1. We calculate the percentage of steps/sentences\\\\nwhere retrieval is triggered for every threshold and\\\\ndisplay the performance based on the percentage\\\\nof retrieval. As shown in Figure 5, on 2WikiMul-\\\\ntihopQA, the performance plateaus when the re-\\\\ntrieval percentage exceeds 60%, indicating that re-\\\\ntrieval when LMs are conﬁdent is not necessary.\\\\nOn StrategyQA, the performance drops with a re-\\\\ntrieval percentage above 50%, suggesting that the\\\\nuse of high-conﬁdence sentences for retrieval can\\\\nintroduce noise and impede the original genera-\\\\nβ\\\\nEM\\\\nF1\\\\nPrec.\\\\nRec.\\\\n0.0\\\\n0.488\\\\n0.576\\\\n0.571\\\\n0.605\\\\n0.2\\\\n0.498\\\\n0.588\\\\n0.582\\\\n0.616\\\\n0.4\\\\n0.510\\\\n0.597\\\\n0.591\\\\n0.627\\\\n0.6\\\\n0.506\\\\n0.593\\\\n0.586\\\\n0.622\\\\nTable 4: Performance of FLARE with respect to the\\\\nmasking threshold β on 2WikiMultihopQA.\\\\nASQA-hint\\\\nWikiAsp\\\\nEM D-F1 R-L DR\\\\nUniEval E-F1 R-L\\\\nImplicit 45.7 36.9 37.7 37.3\\\\n53.4\\\\n18.8 27.7\\\\nExplicit 46.2 36.7 37.7 37.2\\\\n53.4\\\\n18.9 27.6\\\\nTable 5: A comparison between implicit and explicit\\\\nquery formulation methods in FLARE.\\\\ntion process. Depending on the tasks/datasets, we\\\\nfound on average triggering retrieval for 40%-60%\\\\nof sentences usually leads to a good performance.\\\\nEffectiveness of different query formulation\\\\nmethods\\\\nLast, we study implicit query forma-\\\\ntion by masking and explicit query formulation\\\\nthrough question generation. In Table 4, we com-\\\\npare the performance of FLARE with different\\\\nmasking thresholds β. Retrieving directly with the\\\\ncomplete sentence (β = 0) is worse than masking\\\\ntokens with low probabilities, conﬁrming our hy-\\\\npothesis that low-conﬁdence erroneous tokens can\\\\ndistract retrievers. We also compare implicit and\\\\nexplicit query formulation methods in Table 5. Per-\\\\nformances of both methods are similar, indicating\\\\nthat both methods can effectively reﬂect informa-\\\\ntion needs.\\\\n7\\\\nConclusion\\\\nTo aid long-form generation with retrieval aug-\\\\nmentation, we propose an active retrieval aug-\\\\nmented generation framework that decides when\\\\nand what to retrieve during generation. We imple-\\\\nment this framework with forward-looking active\\\\nretrieval that iteratively uses the upcoming sentence\\\\nto retrieve relevant information if it contains low-\\\\nconﬁdence tokens and regenerates the next sen-\\\\ntence. Experimental results on 4 tasks/datasets\\\\ndemonstrate the effectiveness of our methods. Fu-\\\\nture directions include better alternatives for active\\\\nretrieval and developing LM architectures for efﬁ-\\\\ncient active retrieval augmentation.\\\\n8\\\\nLimitation\\\\nWe also performed preliminary experiments on\\\\nWizard of Wikipedia (Dinan et al., 2019) and ELI5\\\\n(Fan et al., 2019), and found that FLARE did not\\\\nprovide signiﬁcant gains. Wizard of Wikipedia is\\\\na knowledge-intensive dialogue generation dataset\\\\nwhere the output is relatively short (∼20 tokens\\\\non average) so retrieving multiple disparate pieces\\\\nof information might not be necessary. ELI5 (Fan\\\\net al., 2019) is a long-form QA dataset requiring\\\\nin-depth answers to open-ended questions. Due to\\\\nissues mentioned in Krishna et al. (2021) such as\\\\ndifﬁculties of grounding generation in retrieval and\\\\nevaluation, both single-time retrieval and FLARE\\\\ndid not provide signiﬁcant gains over not using\\\\nretrieval. From an engineering perspective, inter-\\\\nleaving generation with retrieval with a naive im-\\\\nplementation increases both overheads and the cost\\\\nof generation. The LM needs to be activated mul-\\\\ntiple times (once for each retrieval) and a caching-\\\\nfree implementation will also require recomputing\\\\nthe previous activation each time after a retrieval.\\\\nThis issue can be potentially alleviated with spe-\\\\ncial architectural designs that encode the retrieved\\\\ndocuments Dqt and the input/generation (x/y<t)\\\\nindependently.\\\\nAcknowledgements\\\\nThis work was supported in part by a grant\\\\nfrom the Singapore Defence Science and Technol-\\\\nogy Agency and the IBM PhD Fellowship. We\\\\nthank Chunting Zhou, Amanda Bertsch, Uri Alon,\\\\nHiroaki Hayashi, Harsh Trivedi, Patrick Lewis,\\\\nKaixin Ma, Shuyan Zhou, and Songwei Ge for\\\\ntheir insightful discussions and help with the exper-\\\\niments.\\\\nReferences\\\\nSebastian Borgeaud, Arthur Mensch, Jordan Hoff-\\\\nmann, Trevor Cai, Eliza Rutherford, Katie Millican,\\\\nGeorge van den Driessche, Jean-Baptiste Lespiau,\\\\nBogdan Damoc, Aidan Clark, Diego de Las Casas,\\\\nAurelia Guy, Jacob Menick, Roman Ring, Tom Hen-\\\\nnigan, Saffron Huang, Loren Maggiore, Chris Jones,\\\\nAlbin Cassirer, Andy Brock, Michela Paganini, Ge-\\\\noffrey Irving, Oriol Vinyals, Simon Osindero, Karen\\\\nSimonyan, Jack W. Rae, Erich Elsen, and Laurent\\\\nSifre. 2022. Improving language models by retriev-\\\\ning from trillions of tokens. In International Confer-\\\\nence on Machine Learning, ICML 2022, 17-23 July\\\\n2022, Baltimore, Maryland, USA, volume 162 of\\\\nProceedings of Machine Learning Research, pages\\\\n2206–2240. PMLR.\\\\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\\\\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\\\\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\\\\nAskell,\\\\nSandhini Agarwal,\\\\nAriel Herbert-Voss,\\\\nGretchen Krueger, Tom Henighan, Rewon Child,\\\\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\\\\nClemens Winter, Christopher Hesse, Mark Chen,\\\\nEric Sigler, Mateusz Litwin, Scott Gray, Benjamin\\\\nChess, Jack Clark, Christopher Berner, Sam Mc-\\\\nCandlish, Alec Radford, Ilya Sutskever, and Dario\\\\nAmodei. 2020. Language models are few-shot learn-\\\\ners. In Advances in Neural Information Processing\\\\nSystems 33: Annual Conference on Neural Informa-\\\\ntion Processing Systems 2020, NeurIPS 2020, De-\\\\ncember 6-12, 2020, virtual.\\\\nDanqi Chen, Adam Fisch, Jason Weston, and Antoine\\\\nBordes. 2017. Reading wikipedia to answer open-\\\\ndomain questions. In Proceedings of the 55th An-\\\\nnual Meeting of the Association for Computational\\\\nLinguistics, ACL 2017, Vancouver, Canada, July 30 -\\\\nAugust 4, Volume 1: Long Papers, pages 1870–1879.\\\\nAssociation for Computational Linguistics.\\\\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\\\\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\\\\nPaul Barham, Hyung Won Chung, Charles Sutton,\\\\nSebastian Gehrmann, Parker Schuh, Kensen Shi,\\\\nSasha Tsvyashchenko, Joshua Maynez, Abhishek\\\\nRao, Parker Barnes, Yi Tay, Noam Shazeer, Vin-\\\\nodkumar Prabhakaran, Emily Reif, Nan Du, Ben\\\\nHutchinson, Reiner Pope, James Bradbury, Jacob\\\\nAustin, Michael Isard, Guy Gur-Ari, Pengcheng\\\\nYin, Toju Duke, Anselm Levskaya, Sanjay Ghe-\\\\nmawat, Sunipa Dev, Henryk Michalewski, Xavier\\\\nGarcia, Vedant Misra, Kevin Robinson, Liam Fe-\\\\ndus, Denny Zhou, Daphne Ippolito, David Luan,\\\\nHyeontaek Lim, Barret Zoph, Alexander Spiridonov,\\\\nRyan Sepassi, David Dohan, Shivani Agrawal, Mark\\\\nOmernick, Andrew M. Dai, Thanumalayan Sankara-\\\\nnarayana Pillai, Marie Pellat, Aitor Lewkowycz,\\\\nErica Moreira, Rewon Child, Oleksandr Polozov,\\\\nKatherine Lee, Zongwei Zhou, Xuezhi Wang, Bren-\\\\nnan Saeta, Mark Diaz, Orhan Firat, Michele Catasta,\\\\nJason Wei, Kathy Meier-Hellstern, Douglas Eck,\\\\nJeff Dean, Slav Petrov, and Noah Fiedel. 2022.\\\\nPalm: Scaling language modeling with pathways.\\\\nCoRR, abs/2204.02311.\\\\nNachshon Cohen, Oren Kalinsky, Yftah Ziser, and\\\\nAlessandro Moschitti. 2021.\\\\nWikisum:\\\\nCoher-\\\\nent summarization dataset for efﬁcient human-\\\\nevaluation. In Proceedings of the 59th Annual Meet-\\\\ning of the Association for Computational Linguis-\\\\ntics and the 11th International Joint Conference on\\\\nNatural Language Processing, ACL/IJCNLP 2021,\\\\n(Volume 2: Short Papers), Virtual Event, August 1-\\\\n6, 2021, pages 212–219. Association for Computa-\\\\ntional Linguistics.\\\\nEmily Dinan, Stephen Roller, Kurt Shuster, Angela\\\\nFan, Michael Auli, and Jason Weston. 2019. Wizard\\\\nof wikipedia: Knowledge-powered conversational\\\\nagents. In 7th International Conference on Learn-\\\\ning Representations, ICLR 2019, New Orleans, LA,\\\\nUSA, May 6-9, 2019. OpenReview.net.\\\\nAngela Fan, Yacine Jernite, Ethan Perez, David Grang-\\\\nier, Jason Weston, and Michael Auli. 2019. ELI5:\\\\nlong form question answering.\\\\nIn Proceedings of\\\\nthe 57th Conference of the Association for Compu-\\\\ntational Linguistics, ACL 2019, Florence, Italy, July\\\\n28- August 2, 2019, Volume 1: Long Papers, pages\\\\n3558–3567. Association for Computational Linguis-\\\\ntics.\\\\nLuyu Gao, Xueguang Ma, Jimmy Lin, and Jamie\\\\nCallan. 2022. Precise zero-shot dense retrieval with-\\\\nout relevance labels. CoRR, abs/2212.10496.\\\\nMor Geva, Daniel Khashabi, Elad Segal, Tushar Khot,\\\\nDan Roth, and Jonathan Berant. 2021. Did aristotle\\\\nuse a laptop? a question answering benchmark with\\\\nimplicit reasoning strategies. Transactions of the As-\\\\nsociation for Computational Linguistics, 9:346–361.\\\\nJohn M. Giorgi, Luca Soldaini, Bo Wang, Gary D.\\\\nBader, Kyle Lo, Lucy Lu Wang, and Arman Co-\\\\nhan. 2022.\\\\nExploring the challenges of open\\\\ndomain multi-document summarization.\\\\nCoRR,\\\\nabs/2212.10526.\\\\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu-\\\\npat, and Ming-Wei Chang. 2020. REALM: retrieval-\\\\naugmented language model pre-training.\\\\nCoRR,\\\\nabs/2002.08909.\\\\nHiroaki Hayashi, Prashant Budania, Peng Wang, Chris\\\\nAckerson, Raj Neervannan, and Graham Neubig.\\\\n2021. Wikiasp: A dataset for multi-domain aspect-\\\\nbased summarization. Trans. Assoc. Comput. Lin-\\\\nguistics, 9:211–225.\\\\nDan Hendrycks, Collin Burns, Steven Basart, Andy\\\\nZou, Mantas Mazeika, Dawn Song, and Jacob Stein-\\\\nhardt. 2020. Measuring massive multitask language\\\\nunderstanding. CoRR, abs/2009.03300.\\\\nXanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara,\\\\nand Akiko Aizawa. 2020. Constructing A multi-hop\\\\nQA dataset for comprehensive evaluation of reason-\\\\ning steps. In Proceedings of the 28th International\\\\nConference on Computational Linguistics, COLING\\\\n2020, Barcelona, Spain (Online), December 8-13,\\\\n2020, pages 6609–6625. International Committee on\\\\nComputational Linguistics.\\\\nGautier Izacard and Edouard Grave. 2021. Leveraging\\\\npassage retrieval with generative models for open\\\\ndomain question answering. In Proceedings of the\\\\n16th Conference of the European Chapter of the As-\\\\nsociation for Computational Linguistics: Main Vol-\\\\nume, EACL 2021, Online, April 19 - 23, 2021, pages\\\\n874–880. Association for Computational Linguis-\\\\ntics.\\\\nGautier Izacard, Patrick S. H. Lewis, Maria Lomeli,\\\\nLucas Hosseini, Fabio Petroni, Timo Schick, Jane\\\\nDwivedi-Yu, Armand Joulin, Sebastian Riedel, and\\\\nEdouard Grave. 2022.\\\\nFew-shot learning with\\\\nretrieval augmented language models.\\\\nCoRR,\\\\nabs/2208.03299.\\\\nZhengbao Jiang, Jun Araki, Haibo Ding, and Graham\\\\nNeubig. 2021. How can we know When language\\\\nmodels know? on the calibration of language mod-\\\\nels for question answering. Trans. Assoc. Comput.\\\\nLinguistics, 9:962–977.\\\\nZhengbao Jiang, Luyu Gao, Jun Araki, Haibo Ding,\\\\nZhiruo Wang, Jamie Callan, and Graham Neubig.\\\\n2022.\\\\nRetrieval as attention: End-to-end learning\\\\nof retrieval and reading within a single transformer.\\\\nCoRR, abs/2212.02027.\\\\nZhengbao Jiang, Frank F. Xu, Jun Araki, and Graham\\\\nNeubig. 2020.\\\\nHow can we know what language\\\\nmodels know.\\\\nTrans. Assoc. Comput. Linguistics,\\\\n8:423–438.\\\\nMandar Joshi, Eunsol Choi, Daniel S. Weld, and Luke\\\\nZettlemoyer. 2017. Triviaqa: A large scale distantly\\\\nsupervised challenge dataset for reading comprehen-\\\\nsion. In Proceedings of the 55th Annual Meeting of\\\\nthe Association for Computational Linguistics, ACL\\\\n2017, Vancouver, Canada, July 30 - August 4, Vol-\\\\nume 1: Long Papers, pages 1601–1611. Association\\\\nfor Computational Linguistics.\\\\nSaurav Kadavath, Tom Conerly, Amanda Askell, Tom\\\\nHenighan, Dawn Drain, Ethan Perez, Nicholas\\\\nSchiefer, Zac Hatﬁeld-Dodds, Nova DasSarma, Eli\\\\nTran-Johnson, Scott Johnston, Sheer El Showk,\\\\nAndy Jones, Nelson Elhage, Tristan Hume, Anna\\\\nChen, Yuntao Bai, Sam Bowman, Stanislav Fort,\\\\nDeep Ganguli, Danny Hernandez, Josh Jacobson,\\\\nJackson Kernion, Shauna Kravec, Liane Lovitt, Ka-\\\\nmal Ndousse, Catherine Olsson, Sam Ringer, Dario\\\\nAmodei, Tom Brown, Jack Clark, Nicholas Joseph,\\\\nBen Mann, Sam McCandlish, Chris Olah, and Jared\\\\nKaplan. 2022.\\\\nLanguage models (mostly) know\\\\nwhat they know. CoRR, abs/2207.05221.\\\\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick\\\\nS. H. Lewis, Ledell Wu, Sergey Edunov, Danqi\\\\nChen, and Wen-tau Yih. 2020. Dense passage re-\\\\ntrieval for open-domain question answering. In Pro-\\\\nceedings of the 2020 Conference on Empirical Meth-\\\\nods in Natural Language Processing, EMNLP 2020,\\\\nOnline, November 16-20, 2020, pages 6769–6781.\\\\nAssociation for Computational Linguistics.\\\\nUrvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke\\\\nZettlemoyer, and Mike Lewis. 2020.\\\\nGeneraliza-\\\\ntion through memorization: Nearest neighbor lan-\\\\nguage models. In 8th International Conference on\\\\nLearning Representations, ICLR 2020, Addis Ababa,\\\\nEthiopia, April 26-30, 2020. OpenReview.net.\\\\nOmar Khattab, Keshav Santhanam, Xiang Lisa Li,\\\\nDavid Hall, Percy Liang, Christopher Potts, and\\\\nMatei Zaharia. 2022.\\\\nDemonstrate-search-predict:\\\\nComposing retrieval and language models for\\\\nknowledge-intensive NLP. CoRR, abs/2212.14024.\\\\nTushar Khot, Harsh Trivedi, Matthew Finlayson, Yao\\\\nFu, Kyle Richardson, Peter Clark, and Ashish Sab-\\\\nharwal. 2022.\\\\nDecomposed prompting: A mod-\\\\nular approach for solving complex tasks.\\\\nCoRR,\\\\nabs/2210.02406.\\\\nKalpesh Krishna, Aurko Roy, and Mohit Iyyer. 2021.\\\\nHurdles to progress in long-form question answer-\\\\ning. In North American Association for Computa-\\\\ntional Linguistics.\\\\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\\\\nﬁeld, Michael Collins, Ankur P. Parikh, Chris Al-\\\\nberti, Danielle Epstein, Illia Polosukhin, Jacob De-\\\\nvlin, Kenton Lee, Kristina Toutanova, Llion Jones,\\\\nMatthew Kelcey, Ming-Wei Chang, Andrew M. Dai,\\\\nJakob Uszkoreit, Quoc Le, and Slav Petrov. 2019.\\\\nNatural questions: a benchmark for question answer-\\\\ning research.\\\\nTrans. Assoc. Comput. Linguistics,\\\\n7:452–466.\\\\nHaejun Lee, Akhil Kedia, Jongwon Lee, Ashwin Paran-\\\\njape, Christopher D. Manning, and Kyoung-Gu Woo.\\\\n2021. You only need one model for open-domain\\\\nquestion answering. CoRR, abs/2112.07381.\\\\nPatrick S. H. Lewis, Ethan Perez, Aleksandra Pik-\\\\ntus, Fabio Petroni, Vladimir Karpukhin, Naman\\\\nGoyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih,\\\\nTim Rocktäschel, Sebastian Riedel, and Douwe\\\\nKiela. 2020.\\\\nRetrieval-augmented generation for\\\\nknowledge-intensive NLP tasks.\\\\nIn Advances in\\\\nNeural Information Processing Systems 33: Annual\\\\nConference on Neural Information Processing Sys-\\\\ntems 2020, NeurIPS 2020, December 6-12, 2020,\\\\nvirtual.\\\\nChin-Yew Lin. 2004.\\\\nROUGE: A package for auto-\\\\nmatic evaluation of summaries. In Text Summariza-\\\\ntion Branches Out, pages 74–81, Barcelona, Spain.\\\\nAssociation for Computational Linguistics.\\\\nYuning Mao, Pengcheng He, Xiaodong Liu, Yelong\\\\nShen, Jianfeng Gao, Jiawei Han, and Weizhu Chen.\\\\n2021.\\\\nGeneration-augmented retrieval for open-\\\\ndomain question answering. In Proceedings of the\\\\n59th Annual Meeting of the Association for Compu-\\\\ntational Linguistics and the 11th International Joint\\\\nConference on Natural Language Processing, ACL/I-\\\\nJCNLP 2021, (Volume 1:\\\\nLong Papers), Virtual\\\\nEvent, August 1-6, 2021, pages 4089–4100. Associ-\\\\nation for Computational Linguistics.\\\\nJoshua Maynez, Shashi Narayan, Bernd Bohnet, and\\\\nRyan McDonald. 2020. On faithfulness and factu-\\\\nality in abstractive summarization. In Proceedings\\\\nof the 58th Annual Meeting of the Association for\\\\nComputational Linguistics, pages 1906–1919, On-\\\\nline. Association for Computational Linguistics.\\\\nOpenAI. 2023.\\\\nGPT-4 technical report.\\\\nCoRR,\\\\nabs/2303.08774.\\\\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-\\\\nroll L. Wainwright, Pamela Mishkin, Chong Zhang,\\\\nSandhini Agarwal, Katarina Slama, Alex Ray, John\\\\nSchulman, Jacob Hilton, Fraser Kelton, Luke Miller,\\\\nMaddie Simens, Amanda Askell, Peter Welinder,\\\\nPaul F. Christiano, Jan Leike, and Ryan Lowe.\\\\n2022. Training language models to follow instruc-\\\\ntions with human feedback. CoRR, abs/2203.02155.\\\\nFabio Petroni, Tim Rocktäschel, Sebastian Riedel,\\\\nPatrick S. H. Lewis, Anton Bakhtin, Yuxiang Wu,\\\\nand Alexander H. Miller. 2019.\\\\nLanguage mod-\\\\nels as knowledge bases?\\\\nIn Proceedings of the\\\\n2019 Conference on Empirical Methods in Natu-\\\\nral Language Processing and the 9th International\\\\nJoint Conference on Natural Language Processing,\\\\nEMNLP-IJCNLP 2019, Hong Kong, China, Novem-\\\\nber 3-7, 2019, pages 2463–2473. Association for\\\\nComputational Linguistics.\\\\nOﬁr Press, Muru Zhang, Sewon Min, Ludwig Schmidt,\\\\nNoah A Smith, and Mike Lewis. 2022. Measuring\\\\nand narrowing the compositionality gap in language\\\\nmodels. arXiv preprint arXiv:2210.03350.\\\\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\\\\nDario Amodei, and Ilya Sutskever. 2019. Language\\\\nmodels are unsupervised multitask learners. OpenAI\\\\nBlog, 1(8).\\\\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\\\\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\\\\nWei Li, and Peter J. Liu. 2020. Exploring the limits\\\\nof transfer learning with a uniﬁed text-to-text trans-\\\\nformer. J. Mach. Learn. Res., 21:140:1–140:67.\\\\nOri Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay,\\\\nAmnon Shashua, Kevin Leyton-Brown, and Yoav\\\\nShoham. 2023. In-context retrieval-augmented lan-\\\\nguage models. arXiv preprint arXiv:2302.00083.\\\\nAdam Roberts, Colin Raffel, and Noam Shazeer. 2020.\\\\nHow much knowledge can you pack into the pa-\\\\nrameters of a language model?\\\\nIn Proceedings of\\\\nthe 2020 Conference on Empirical Methods in Nat-\\\\nural Language Processing, EMNLP 2020, Online,\\\\nNovember 16-20, 2020, pages 5418–5426. Associ-\\\\nation for Computational Linguistics.\\\\nStephen E. Robertson and Hugo Zaragoza. 2009. The\\\\nprobabilistic relevance framework: BM25 and be-\\\\nyond. Found. Trends Inf. Retr., 3(4):333–389.\\\\nDevendra Singh Sachan, Siva Reddy, William L.\\\\nHamilton, Chris Dyer, and Dani Yogatama. 2021.\\\\nEnd-to-end training of multi-document reader and\\\\nretriever for open-domain question answering.\\\\nIn\\\\nAdvances in Neural Information Processing Systems\\\\n34: Annual Conference on Neural Information Pro-\\\\ncessing Systems 2021, NeurIPS 2021, December 6-\\\\n14, 2021, virtual, pages 25968–25981.\\\\nTimo\\\\nSchick,\\\\nJane\\\\nDwivedi-Yu,\\\\nRoberto\\\\nDessì,\\\\nRoberta Raileanu, Maria Lomeli, Luke Zettlemoyer,\\\\nNicola Cancedda, and Thomas Scialom. 2023. Tool-\\\\nformer: Language models can teach themselves to\\\\nuse tools.\\\\nWeijia Shi, Sewon Min, Michihiro Yasunaga, Min-\\\\njoon Seo, Rich James, Mike Lewis, Luke Zettle-\\\\nmoyer, and Wen-tau Yih. 2023. REPLUG: retrieval-\\\\naugmented black-box language models.\\\\nCoRR,\\\\nabs/2301.12652.\\\\nIvan Stelmakh, Yi Luan, Bhuwan Dhingra, and Ming-\\\\nWei Chang. 2022. ASQA: factoid questions meet\\\\nlong-form answers. In Proceedings of the 2022 Con-\\\\nference on Empirical Methods in Natural Language\\\\nProcessing, EMNLP 2022, Abu Dhabi, United Arab\\\\nEmirates, December 7-11, 2022, pages 8273–8288.\\\\nAssociation for Computational Linguistics.\\\\nZhiqing Sun, Xuezhi Wang, Yi Tay, Yiming Yang, and\\\\nDenny Zhou. 2022. Recitation-augmented language\\\\nmodels. CoRR, abs/2210.01296.\\\\nAlon Talmor, Jonathan Herzig, Nicholas Lourie, and\\\\nJonathan Berant. 2019. Commonsenseqa: A ques-\\\\ntion answering challenge targeting commonsense\\\\nknowledge. In Proceedings of the 2019 Conference\\\\nof the North American Chapter of the Association\\\\nfor Computational Linguistics: Human Language\\\\nTechnologies, NAACL-HLT 2019, Minneapolis, MN,\\\\nUSA, June 2-7, 2019, Volume 1 (Long and Short Pa-\\\\npers), pages 4149–4158. Association for Computa-\\\\ntional Linguistics.\\\\nJames\\\\nThorne,\\\\nAndreas\\\\nVlachos,\\\\nChristos\\\\nChristodoulopoulos,\\\\nand\\\\nArpit\\\\nMittal.\\\\n2018.\\\\nFEVER: a large-scale dataset for fact extraction\\\\nand veriﬁcation.\\\\nIn Proceedings of the 2018\\\\nConference of the North American Chapter of the\\\\nAssociation for Computational Linguistics: Human\\\\nLanguage Technologies, NAACL-HLT 2018, New\\\\nOrleans, Louisiana, USA, June 1-6, 2018, Volume\\\\n1 (Long Papers), pages 809–819. Association for\\\\nComputational Linguistics.\\\\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\\\\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\\\\nBaptiste Rozière, Naman Goyal, Eric Hambro,\\\\nFaisal Azhar, Aurélien Rodriguez, Armand Joulin,\\\\nEdouard Grave,\\\\nand Guillaume Lample. 2023.\\\\nLlama: Open and efﬁcient foundation language mod-\\\\nels. CoRR, abs/2302.13971.\\\\nHarsh Trivedi,\\\\nNiranjan Balasubramanian,\\\\nTushar\\\\nKhot, and Ashish Sabharwal. 2022.\\\\nInterleav-\\\\ning retrieval with chain-of-thought reasoning for\\\\nknowledge-intensive multi-step questions.\\\\nCoRR,\\\\nabs/2212.10509.\\\\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V.\\\\nLe, Ed H. Chi, and Denny Zhou. 2022.\\\\nSelf-\\\\nconsistency improves chain of thought reasoning in\\\\nlanguage models. CoRR, abs/2203.11171.\\\\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\\\\nBosma, Ed H. Chi, Quoc Le, and Denny Zhou. 2022.\\\\nChain of thought prompting elicits reasoning in large\\\\nlanguage models. CoRR, abs/2201.11903.\\\\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Ben-\\\\ngio, William W. Cohen, Ruslan Salakhutdinov, and\\\\nChristopher D. Manning. 2018. Hotpotqa: A dataset\\\\nfor diverse, explainable multi-hop question answer-\\\\ning.\\\\nIn Proceedings of the 2018 Conference on\\\\nEmpirical Methods in Natural Language Process-\\\\ning, Brussels, Belgium, October 31 - November 4,\\\\n2018, pages 2369–2380. Association for Computa-\\\\ntional Linguistics.\\\\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak\\\\nShafran, Karthik Narasimhan, and Yuan Cao. 2022.\\\\nReact: Synergizing reasoning and acting in language\\\\nmodels. CoRR, abs/2210.03629.\\\\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel\\\\nArtetxe, Moya Chen, Shuohui Chen, Christopher De-\\\\nwan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mi-\\\\nhaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel\\\\nSimig, Punit Singh Koura, Anjali Sridhar, Tianlu\\\\nWang, and Luke Zettlemoyer. 2022.\\\\nOpt: Open\\\\npre-trained transformer language models.\\\\nArXiv,\\\\nabs/2205.01068.\\\\nMing Zhong, Yang Liu, Da Yin, Yuning Mao, Yizhu\\\\nJiao, Pengfei Liu, Chenguang Zhu, Heng Ji, and\\\\nJiawei Han. 2022.\\\\nTowards a uniﬁed multi-\\\\ndimensional evaluator for text generation. In Pro-\\\\nceedings of the 2022 Conference on Empirical Meth-\\\\nods in Natural Language Processing, EMNLP 2022,\\\\nAbu Dhabi, United Arab Emirates, December 7-11,\\\\n2022, pages 2023–2038. Association for Computa-\\\\ntional Linguistics.\\\\nChunting Zhou, Graham Neubig, Jiatao Gu, Mona\\\\nDiab, Francisco Guzmán, Luke Zettlemoyer, and\\\\nMarjan Ghazvininejad. 2021.\\\\nDetecting halluci-\\\\nnated content in conditional neural sequence gener-\\\\nation. In Findings of the Association for Computa-\\\\ntional Linguistics: ACL-IJCNLP 2021, pages 1393–\\\\n1404, Online. Association for Computational Lin-\\\\nguistics.\\\\nA\\\\nDatasets and Settings\\\\nDatasets and experimental settings are summarized\\\\nin Table 6. Wikipedia-related domains excluded\\\\nfrom Bing’s search results are listed in Table 7.\\\\nB\\\\nHyperparameters\\\\nHyperparameters of FLARE on different datasets\\\\nare listed in Table 8.\\\\nC\\\\nPrompts and Few-shot exemplars\\\\nPrompts and exemplars of different tasks/datasets\\\\nare shown in Prompt C.1, C.2, C.3, C.4, C.6, and\\\\nC.8, respectively.\\\\nSettings\\\\n2WikiMultihopQA\\\\nStrategyQA\\\\nASQA\\\\nWikiAsp\\\\n(Ho et al., 2020)\\\\n(Geva et al., 2021)\\\\n(Stelmakh et al., 2022)\\\\n(Hayashi et al., 2021)\\\\nDataset statistics\\\\nTask\\\\nmultihop QA\\\\ncommonsense QA\\\\nlong-form QA\\\\nopen-domain summarization\\\\n#Examples\\\\n500\\\\n229\\\\n500\\\\n500\\\\nEvaluation settings\\\\nMetrics\\\\nEM, F1, Prec., Rec.\\\\nEM\\\\nEM, Disambig-F1, ROUGE, DR UniEval, entity-F1, ROUGE\\\\nRetrieval settings\\\\nCorpus\\\\nWikipedia\\\\nWikipedia\\\\nWikipedia\\\\nopen web\\\\nRetriever\\\\nBM25\\\\nBM25\\\\nBM25\\\\nBing\\\\nTop-k\\\\n2\\\\n3\\\\n3\\\\n5\\\\nPrompt format\\\\n#Exemplars\\\\n8\\\\n6\\\\n8\\\\n4\\\\nRet. for exemplars\\\\n\\\\x13\\\\n\\\\x17\\\\n\\\\x17\\\\n\\\\x17\\\\nTable 6: Statistics and experimental settings of different tasks/datasets.\\\\nwikipedia.org\\\\nwikiwand.com\\\\nwiki2.org\\\\nwikimedia.org\\\\nTable 7: Wikipedia-related domains excluded from Bing’s search results.\\\\nDataset\\\\nθ\\\\nβ\\\\nQuery formulation\\\\nCombine single- & multi-time retrieval\\\\n2WikiMultihopQA\\\\n0.8\\\\n0.4\\\\nimplicit\\\\n\\\\x17\\\\nStrategyQA\\\\n0.4\\\\n0.4\\\\nimplicit\\\\n\\\\x17\\\\nASQA & ASQA-hint\\\\n0.8\\\\n0.4\\\\nexplicit\\\\n\\\\x13\\\\nWikiAsp\\\\n0.8\\\\n0.4\\\\nexplicit\\\\n\\\\x13\\\\nTable 8: Statistics and experimental settings of different tasks/datasets.\\\\nPrompt C.1: retrieval instructions for 2WikiMultihopQA\\\\nSkill 1. Use the Search API to look up relevant information by writing “[Search(term)]” where “term” is the search term you\\\\nwant to look up. For example:\\\\nQuestion: But what are the risks during production of nanomaterials?\\\\nAnswer (with Search): [Search(nanomaterial production risks)] Some nanomaterials may give rise to various kinds of lung\\\\ndamage.\\\\nQuestion: The colors on the ﬂag of Ghana have the following meanings.\\\\nAnswer (with Search): Red is for [Search(Ghana ﬂag red meaning)] the blood of martyrs, green for forests, and gold for\\\\nmineral wealth.\\\\nQuestion: Metformin is the ﬁrst-line drug for what?\\\\nAnswer (with Search): [Search(Metformin ﬁrst-line drug)] patients with type 2 diabetes and obesity.\\\\nSkill 2.\\\\nAnswer questions by thinking step-by-step.\\\\nFirst, write out the reasoning steps, then draw the conclu-\\\\nsion. For example:\\\\nQuestion: When did the director of ﬁlm Hypocrite (Film) die?\\\\nAnswer (with step-by-step): The ﬁlm Hypocrite was directed by Miguel Morayta. Miguel Morayta died on 19 June 2013. So\\\\nthe answer is 19 June 2013.\\\\nQuestion: Are both Kurram Garhi and Trojkrsti located in the same country?\\\\nAnswer (with step-by-step): Kurram Garhi is located in the country of Pakistan. Trojkrsti is located in the country of\\\\nRepublic of Macedonia. Thus, they are not in the same country. So the answer is no.\\\\nQuestion: Do director of ﬁlm Coolie No. 1 (1995 Film) and director of ﬁlm The Sensational Trial have the same\\\\nnationality?\\\\nAnswer (with step-by-step): Coolie No. 1 (1995 ﬁlm) was directed by David Dhawan. The Sensational Trial was directed by\\\\nKarl Freund. David Dhawan’s nationality is India. Karl Freund’s nationality is Germany. Thus, they do not have the same\\\\nnationality. So the answer is no.\\\\nQuestion: Who is Boraqchin (Wife Of Ögedei)’s father-in-law?\\\\nAnswer (with step-by-step): Boraqchin is married to Ögedei Khan. Ögedei Khan’s father is Genghis Khan. Thus, Boraqchin’s\\\\nfather-in-law is Genghis Khan. So the answer is Genghis Khan.\\\\nQuestion: Who was born ﬁrst out of Martin Hodge and Ivania Martinich?\\\\nAnswer (with step-by-step): Martin Hodge was born on 4 February 1959. Ivania Martinich was born on 25 July 1995. Thus,\\\\nMartin Hodge was born ﬁrst. So the answer is Martin Hodge.\\\\nQuestion: When did the director of ﬁlm Laughter In Hell die?\\\\nAnswer (with step-by-step): The ﬁlm Laughter In Hell was directed by Edward L. Cahn. Edward L. Cahn died on August 25,\\\\n1963. So the answer is August 25, 1963.\\\\nQuestion: Which ﬁlm has the director died later, The Gal Who Took the West or Twenty Plus Two?\\\\nAnswer (with step-by-step): The ﬁlm Twenty Plus Two was directed by Joseph M. Newman.\\\\nThe Gal Who Took\\\\nthe West was directed by Frederick de Cordova. Joseph M. Newman died on January 23, 2006. Fred de Cordova\\\\ndied on September 15, 2001. Thus, the person to die later from the two is Twenty Plus Two. So the answer is Twenty Plus Two.\\\\nQuestion: Who is the grandchild of Krishna Shah (Nepalese Royal)?\\\\nAnswer (with step-by-step): Krishna Shah has a child named Rudra Shah. Rudra Shah has a child named Prithvipati Shah.\\\\nThus, Krishna Shah has a grandchild named Prithvipati Shah. So the answer is Prithvipati Shah.\\\\nNow, combine the aforementioned two skills.\\\\nFirst, write out the reasoning steps, then draw the conclusion,\\\\nwhere the reasoning steps should also utilize the Search API “[Search(term)]” whenever possible.\\\\nQuestion: Where did Minbyauk Thihapate’s wife die?\\\\nAnswer (with step-by-step & Search):\\\\nPrompt C.2: exemplars of 2WikiMultihopQA\\\\nQuestion: When did the director of ﬁlm Hypocrite (Film) die?\\\\nAnswer: The ﬁlm Hypocrite was directed by Miguel Morayta. Miguel Morayta died on 19 June 2013. So the answer is 19\\\\nJune 2013.\\\\nQuestion: Are both Kurram Garhi and Trojkrsti located in the same country?\\\\nAnswer: Kurram Garhi is located in the country of Pakistan. Trojkrsti is located in the country of Republic of Macedonia.\\\\nThus, they are not in the same country. So the answer is no.\\\\nQuestion: Do director of ﬁlm Coolie No. 1 (1995 Film) and director of ﬁlm The Sensational Trial have the same\\\\nnationality?\\\\nAnswer: Coolie No. 1 (1995 ﬁlm) was directed by David Dhawan. The Sensational Trial was directed by Karl Freund. David\\\\nDhawan’s nationality is India. Karl Freund’s nationality is Germany. Thus, they do not have the same nationality. So the\\\\nanswer is no.\\\\nQuestion: Who is Boraqchin (Wife Of Ögedei)’s father-in-law?\\\\nAnswer: Boraqchin is married to Ögedei Khan. Ögedei Khan’s father is Genghis Khan. Thus, Boraqchin’s father-in-law is\\\\nGenghis Khan. So the answer is Genghis Khan.\\\\nQuestion: Who was born ﬁrst out of Martin Hodge and Ivania Martinich?\\\\nAnswer: Martin Hodge was born on 4 February 1959. Ivania Martinich was born on 25 July 1995. Thus, Martin Hodge was\\\\nborn ﬁrst. So the answer is Martin Hodge.\\\\nQuestion: When did the director of ﬁlm Laughter In Hell die?\\\\nAnswer: The ﬁlm Laughter In Hell was directed by Edward L. Cahn. Edward L. Cahn died on August 25, 1963. So the\\\\nanswer is August 25, 1963.\\\\nQuestion: Which ﬁlm has the director died later, The Gal Who Took the West or Twenty Plus Two?\\\\nAnswer: The ﬁlm Twenty Plus Two was directed by Joseph M. Newman. The Gal Who Took the West was directed by\\\\nFrederick de Cordova. Joseph M. Newman died on January 23, 2006. Fred de Cordova died on September 15, 2001. Thus,\\\\nthe person to die later from the two is Twenty Plus Two. So the answer is Twenty Plus Two.\\\\nQuestion: Who is the grandchild of Krishna Shah (Nepalese Royal)?\\\\nAnswer: Krishna Shah has a child named Rudra Shah. Rudra Shah has a child named Prithvipati Shah. Thus, Krishna Shah\\\\nhas a grandchild named Prithvipati Shah. So the answer is Prithvipati Shah.\\\\nQuestion: Which country the director of ﬁlm Citizen Mavzik is from?\\\\nAnswer:\\\\nPrompt C.3: exemplars of StrategyQA\\\\nGenerate a yes or no answer to the following question.\\\\nQuestion: Do hamsters provide food for any animals?\\\\nAnswer: Hamsters are prey animals. Prey are food for predators. Thus, hamsters provide food for some animals. So the ﬁnal\\\\nanswer is yes.\\\\nGenerate a yes or no answer to the following question.\\\\nQuestion: Could Brooke Shields succeed at University of Pennsylvania?\\\\nAnswer: Brooke Shields went to Princeton University. Princeton University is about as academically rigorous as the Uni-\\\\nversity of Pennsylvania. Thus, Brooke Shields could also succeed at the University of Pennsylvania. So the ﬁnal answer is yes.\\\\nGenerate a yes or no answer to the following question.\\\\nQuestion: Hydrogen’s atomic number squared exceeds number of Spice Girls?\\\\nAnswer: Hydrogen has an atomic number of 1. 1 squared is 1. There are 5 Spice Girls. Thus, Hydrogen’s atomic number\\\\nsquared is less than 5. So the ﬁnal answer is no.\\\\nGenerate a yes or no answer to the following question.\\\\nQuestion: Is it common to see frost during some college commencements?\\\\nAnswer: College commencement ceremonies can happen in December, May, and June. December is in the winter, so there\\\\ncan be frost. Thus, there could be frost at some commencements. So the ﬁnal answer is yes.\\\\nGenerate a yes or no answer to the following question.\\\\nQuestion: Could a llama birth twice during War in Vietnam (1945-46)?\\\\nAnswer: The War in Vietnam was 6 months. The gestation period for a llama is 11 months, which is more than 6 months.\\\\nThus, a llama could not give birth twice during the War in Vietnam. So the ﬁnal answer is no.\\\\nGenerate a yes or no answer to the following question.\\\\nQuestion: Would a pear sink in water?\\\\nAnswer: The density of a pear is about 0.6g/cm3, which is less than water. Objects less dense than water ﬂoat. Thus, a pear\\\\nwould ﬂoat. So the ﬁnal answer is no.\\\\nGenerate a yes or no answer to the following question.\\\\nQuestion: Did Spiderman ﬁght against Falcon in the MCU?\\\\nAnswer:\\\\nPrompt C.4: exemplars of ASQA\\\\nGiven an ambiguous question, ﬁgure out its interpretations and answer them one by one.\\\\nQuestion: Who played bonnie in gone with the wind?\\\\nAnswer: This question is ambiguous in terms of which version or adaptation of Gone with the Wind is being referred to. In\\\\norder to ﬁgure out its interpretations, we need to consider different versions or adaptations of Gone with the Wind. Gone with\\\\nthe Wind has two versions or adaptations: the 1939 ﬁlm Gone with the Wind or the 2008 musical Gone with the Wind.\\\\nTherefore, this question has 2 interpretations: (1) Who played Bonnie in the 1939 ﬁlm Gone with the Wind? (2) Who played\\\\nBonnie in the 2008 musical Gone with the Wind? The answers to all interpretations are: (1) The 1939 ﬁlm Gone with the\\\\nWind’s character Bonnie was played by Eleanore Cammack \"Cammie\" King. (2) The 2008 musical Gone with the Wind’s\\\\ncharacter Bonnie was played by Leilah de Meza.\\\\nGiven an ambiguous question, ﬁgure out its interpretations and answer them one by one.\\\\nQuestion: What is the second largest city in the usa?\\\\nAnswer: This question is ambiguous in terms of the criteria being used to determine the second largest city in the USA.\\\\nIn order to ﬁgure out its interpretations, we need to consider different criteria to determine a city’s size. City size can be\\\\nmeasured by two criteria: population or area. Therefore, this question has 2 interpretations: (1) What is the second largest\\\\ncity in the USA by population? (2) What is the second largest city in the USA by area? The answers to all interpretations are:\\\\n(1) The second largest city in the USA by population is Los Angeles, California. (2) The second largest city in the USA by\\\\narea is Juneau, Alaska.\\\\nGiven an ambiguous question, ﬁgure out its interpretations and answer them one by one.\\\\nQuestion: When was bohemian rhapsody released as a single?\\\\nAnswer: This question is ambiguous in terms of which country’s release of the single is being referred to. In order to ﬁgure\\\\nout its interpretations, we need to consider different countries where Bohemian Rhapsody is released. Bohemian Rhapsody\\\\nwas released in the United Kingdom and in the United States on different dates. Therefore, this question has 2 interpretations:\\\\n(1) When was Bohemian Rhapsody released as a single in the United Kingdom? (2) When was Bohemian Rhapsody released\\\\nas a single in the United States? The answers to all interpretations are: (1) Bohemian Rhapsody was released as a single in the\\\\nUnited Kingdom on 31 October 1975. (2) Bohemian Rhapsody was released as a single in the United States on December 1975.\\\\nGiven an ambiguous question, ﬁgure out its interpretations and answer them one by one.\\\\nQuestion: Where do the philadelphia eagles play their home games?\\\\nAnswer: This question is ambiguous in terms of which speciﬁc location or venue is being referred to. In order to ﬁgure out its\\\\ninterpretations, we need to consider the different possible locations or venues that could be considered the home ﬁeld of the\\\\nPhiladelphia Eagles. These include the city, the sports complex, or the stadium. Therefore, this question has 3 interpretations:\\\\n(1) What city do the Philadelphia Eagles play their home games? (2) In what sports complex do the Philadelphia Eagles play\\\\ntheir home games? (3) What stadium do the Philadelphia Eagles play their home games? The answers to all interpretations\\\\nare: (1) Philadelphia Eagles play their home games in the city Philadelphia. (2) Philadelphia Eagles play their home games in\\\\nthe South Philadelphia Sports Complex. (3) Philadelphia Eagles play their home games in the Lincoln Financial Field stadium.\\\\nGiven an ambiguous question, ﬁgure out its interpretations and answer them one by one.\\\\nQuestion: When did xbox one come out in australia?\\\\nAnswer: This question is ambiguous in terms of which speciﬁc version of the Xbox One is being referred to. In order to\\\\nﬁgure out its interpretations, we need to consider the different versions of the Xbox One that have been released. Xbox One\\\\nhas two versions: the Xbox One video game console or the Xbox One X high-end model. Therefore, this question has 2\\\\ninterpretations: (1) When did the Xbox One release in Australia? (2) When did the Xbox One X release in Australia? The\\\\nanswers to all interpretations are: (1) The Xbox One video game console was released in Australia on November 22, 2013.\\\\n(2) The Xbox One X video game console was released in Australia on November 7, 2017.\\\\nGiven an ambiguous question, ﬁgure out its interpretations and answer them one by one.\\\\nQuestion: When does the movie summer of 84 come out?\\\\nAnswer: This question is ambiguous in terms of which release of the movie is being referred to. In order to ﬁgure out its\\\\ninterpretations, we need to consider different releases of the movie Summer of ’84. The movie Summer of ’84 is ﬁrst released\\\\nat the Sundance Festival before it’s released throughout the US. Therefore, this question has 2 interpretations: (1) When\\\\ndid the movie Summer of ’84 ﬁrst release at the Sundance Festival? (2) When did the movie Summer of ’84 ﬁrst release\\\\nthroughout the US? The answers to all interpretations are: (1) Summer of ’84 was released at the Sundance Festival on\\\\nJanuary 22, 2018. (2) Summer of ’84 was released throughout the US on August 10, 2018.\\\\nPrompt C.5: exemplars of ASQA (cont.)\\\\nGiven an ambiguous question, ﬁgure out its interpretations and answer them one by one.\\\\nQuestion: What was roy orbison’s ﬁrst number one hit?\\\\nAnswer: This question is ambiguous in terms of which speciﬁc chart or region is being referred to. In order to ﬁgure out its\\\\ninterpretations, we need to consider the different charts and regions where Roy Orbison’s music was popular. Roy Orbison is\\\\npopular in both the US Hot 100 and Canada, and the UK and Ireland. Therefore, this question has 2 interpretations: (1) What\\\\nwas Roy Orbison’s ﬁrst number one hit in the US Hot 100 and Canada? (2) What was Roy Orbison’s ﬁrst number one hit in\\\\nthe UK and Ireland? The answers to all interpretations are: (1) Running Scared was the ﬁrst number one hit for Roy Orbison\\\\nin the US Hot 100 and Canada. (2) Only the Lonely (Know the Way I Feel) was the ﬁrst number one hit for Roy Orbison in\\\\nthe UK and Ireland.\\\\nGiven an ambiguous question, ﬁgure out its interpretations and answer them one by one.\\\\nQuestion: What is the criminal’s name in the breakfast club?\\\\nAnswer: This question is ambiguous in terms of which speciﬁc name is being referred to - the character’s name or the actor’s\\\\nname. In order to ﬁgure out its interpretations, we need to consider both possibilities: the character’s name or the actor’s\\\\nname. Therefore, this question has 2 interpretations: (1) What is the criminal’s character name in The Breakfast Club? (2)\\\\nWhat is the the name of the actor who played the criminal in The Breakfast Club? The answers to all interpretations are: (1)\\\\nJohn Bender was the name of the criminal’s character in The Breakfast Club. (2) Judd Nelson was the actor of the criminal in\\\\nThe Breakfast Club.\\\\nGiven an ambiguous question, ﬁgure out its interpretations and answer them one by one.\\\\nQuestion: How many state parks are there in virginia?\\\\nAnswer:\\\\nPrompt C.6: exemplars of ASQA-hint\\\\nGiven an ambiguous question and a hint on which aspect of the question is ambiguous, ﬁgure out its interpretations and\\\\nanswer them one by one.\\\\nQuestion: Who played bonnie in gone with the wind?\\\\nHint: This question is ambiguous in terms of which version or adaptation of Gone with the Wind is being referred to.\\\\nAnswer: In order to ﬁgure out its interpretations, we need to consider different versions or adaptations of Gone with the Wind.\\\\nGone with the Wind has two versions or adaptations: the 1939 ﬁlm Gone with the Wind or the 2008 musical Gone with the\\\\nWind. Therefore, this question has 2 interpretations: (1) Who played Bonnie in the 1939 ﬁlm Gone with the Wind? (2) Who\\\\nplayed Bonnie in the 2008 musical Gone with the Wind? The answers to all interpretations are: (1) The 1939 ﬁlm Gone with\\\\nthe Wind’s character Bonnie was played by Eleanore Cammack \"Cammie\" King. (2) The 2008 musical Gone with the Wind’s\\\\ncharacter Bonnie was played by Leilah de Meza.\\\\nGiven an ambiguous question and a hint on which aspect of the question is ambiguous, ﬁgure out its interpreta-\\\\ntions and answer them one by one.\\\\nQuestion: What is the second largest city in the usa?\\\\nHint: This question is ambiguous in terms of the criteria being used to determine the second largest city in the USA.\\\\nAnswer: In order to ﬁgure out its interpretations, we need to consider different criteria to determine a city’s size. City size can\\\\nbe measured by two criteria: population or area. Therefore, this question has 2 interpretations: (1) What is the second largest\\\\ncity in the USA by population? (2) What is the second largest city in the USA by area? The answers to all interpretations are:\\\\n(1) The second largest city in the USA by population is Los Angeles, California. (2) The second largest city in the USA by\\\\narea is Juneau, Alaska.\\\\nGiven an ambiguous question and a hint on which aspect of the question is ambiguous, ﬁgure out its interpreta-\\\\ntions and answer them one by one.\\\\nQuestion: When was bohemian rhapsody released as a single?\\\\nHint: This question is ambiguous in terms of which country’s release of the single is being referred to.\\\\nAnswer: In order to ﬁgure out its interpretations, we need to consider different countries where Bohemian Rhapsody is\\\\nreleased. Bohemian Rhapsody was released in the United Kingdom and in the United States on different dates. Therefore,\\\\nthis question has 2 interpretations: (1) When was Bohemian Rhapsody released as a single in the United Kingdom? (2) When\\\\nwas Bohemian Rhapsody released as a single in the United States? The answers to all interpretations are: (1) Bohemian\\\\nRhapsody was released as a single in the United Kingdom on 31 October 1975. (2) Bohemian Rhapsody was released as a\\\\nsingle in the United States on December 1975.\\\\nGiven an ambiguous question and a hint on which aspect of the question is ambiguous, ﬁgure out its interpreta-\\\\ntions and answer them one by one.\\\\nQuestion: Where do the philadelphia eagles play their home games?\\\\nHint: This question is ambiguous in terms of which speciﬁc location or venue is being referred to.\\\\nAnswer: In order to ﬁgure out its interpretations, we need to consider the different possible locations or venues that could be\\\\nconsidered the home ﬁeld of the Philadelphia Eagles. These include the city, the sports complex, or the stadium. Therefore,\\\\nthis question has 3 interpretations: (1) What city do the Philadelphia Eagles play their home games? (2) In what sports\\\\ncomplex do the Philadelphia Eagles play their home games? (3) What stadium do the Philadelphia Eagles play their home\\\\ngames? The answers to all interpretations are: (1) Philadelphia Eagles play their home games in the city Philadelphia. (2)\\\\nPhiladelphia Eagles play their home games in the South Philadelphia Sports Complex. (3) Philadelphia Eagles play their\\\\nhome games in the Lincoln Financial Field stadium.\\\\nGiven an ambiguous question and a hint on which aspect of the question is ambiguous, ﬁgure out its interpreta-\\\\ntions and answer them one by one.\\\\nQuestion: When did xbox one come out in australia?\\\\nHint: This question is ambiguous in terms of which speciﬁc version of the Xbox One is being referred to.\\\\nAnswer: In order to ﬁgure out its interpretations, we need to consider the different versions of the Xbox One that have been\\\\nreleased. Xbox One has two versions: the Xbox One video game console or the Xbox One X high-end model. Therefore, this\\\\nquestion has 2 interpretations: (1) When did the Xbox One release in Australia? (2) When did the Xbox One X release\\\\nin Australia? The answers to all interpretations are: (1) The Xbox One video game console was released in Australia on\\\\nNovember 22, 2013. (2) The Xbox One X video game console was released in Australia on November 7, 2017.\\\\nGiven an ambiguous question and a hint on which aspect of the question is ambiguous, ﬁgure out its interpreta-\\\\ntions and answer them one by one.\\\\nQuestion: When does the movie summer of 84 come out?\\\\nHint: This question is ambiguous in terms of which release of the movie is being referred to.\\\\nAnswer: In order to ﬁgure out its interpretations, we need to consider different releases of the movie Summer of ’84. The\\\\nmovie Summer of ’84 is ﬁrst released at the Sundance Festival before it’s released throughout the US. Therefore, this question\\\\nhas 2 interpretations: (1) When did the movie Summer of ’84 ﬁrst release at the Sundance Festival? (2) When did the movie\\\\nSummer of ’84 ﬁrst release throughout the US? The answers to all interpretations are: (1) Summer of ’84 was released at the\\\\nSundance Festival on January 22, 2018. (2) Summer of ’84 was released throughout the US on August 10, 2018.\\\\nPrompt C.7: exemplars of ASQA-hint (cont.)\\\\nGiven an ambiguous question and a hint on which aspect of the question is ambiguous, ﬁgure out its interpretations and\\\\nanswer them one by one.\\\\nQuestion: What was roy orbison’s ﬁrst number one hit?\\\\nHint: This question is ambiguous in terms of which speciﬁc chart or region is being referred to.\\\\nAnswer: In order to ﬁgure out its interpretations, we need to consider the different charts and regions where Roy Orbison’s\\\\nmusic was popular. Roy Orbison is popular in both the US Hot 100 and Canada, and the UK and Ireland. Therefore, this\\\\nquestion has 2 interpretations: (1) What was Roy Orbison’s ﬁrst number one hit in the US Hot 100 and Canada? (2) What\\\\nwas Roy Orbison’s ﬁrst number one hit in the UK and Ireland? The answers to all interpretations are: (1) Running Scared\\\\nwas the ﬁrst number one hit for Roy Orbison in the US Hot 100 and Canada. (2) Only the Lonely (Know the Way I Feel) was\\\\nthe ﬁrst number one hit for Roy Orbison in the UK and Ireland.\\\\nGiven an ambiguous question and a hint on which aspect of the question is ambiguous, ﬁgure out its interpreta-\\\\ntions and answer them one by one.\\\\nQuestion: What is the criminal’s name in the breakfast club?\\\\nHint: This question is ambiguous in terms of which speciﬁc name is being referred to - the character’s name or the actor’s\\\\nname.\\\\nAnswer: In order to ﬁgure out its interpretations, we need to consider both possibilities: the character’s name or the actor’s\\\\nname. Therefore, this question has 2 interpretations: (1) What is the criminal’s character name in The Breakfast Club? (2)\\\\nWhat is the the name of the actor who played the criminal in The Breakfast Club? The answers to all interpretations are: (1)\\\\nJohn Bender was the name of the criminal’s character in The Breakfast Club. (2) Judd Nelson was the actor of the criminal in\\\\nThe Breakfast Club.\\\\nGiven an ambiguous question and a hint on which aspect of the question is ambiguous, ﬁgure out its interpreta-\\\\ntions and answer them one by one.\\\\nQuestion: How many state parks are there in virginia?\\\\nHint: This question is ambiguous in terms of the time frame or period being referred to.\\\\nAnswer: In order to ﬁgure out its interpretations,\\\\nPrompt C.8: exemplars of WikiAsp\\\\nGenerate a summary about Aslanhane Mosque including the following aspects: location, history with one aspect per line.\\\\n# Location\\\\nThe mosque is in the old quarter of ankara next to ankara castle. With an altitude of 947 metres (3,107 ft) it overlooks ankara\\\\nat 39°56’12\"N 32°51’55\"E.\\\\n# History\\\\nThe mosque is one of the oldest mosques in Turkey still standing. It was built during the reign of Mesud II of the Anatolian\\\\nSeljuks in 1290. Its architect was Ebubekir Mehmet. It was commissioned by two Ahi leaders named Hüsamettin and\\\\nHasaneddin. However, in 1330, it was repaired by another Ahi leader named ¸Serafettin after whom the mosque was named.\\\\nAfter several minor repairs the mosque was restored by the directorate general of foundations in 2010-2013 term.\\\\nGenerate a summary about Untold Legends:\\\\nThe Warrior’s Code including the following aspects:\\\\nreception,\\\\ngameplay, development with one aspect per line.\\\\n# Reception\\\\nThe game received \"mixed or average reviews\" according to video game review aggregator Metacritic.\\\\n# Gameplay\\\\nThe warrior’s code is a hack n’ slash action role-playing game, which concentrates on action-oriented combat.\\\\n# Development\\\\nAs a pre-order bonus, the game was shipped with a small action ﬁgure of the Guardian class.\\\\nGenerate a summary about Raid on St.\\\\nAugustine including the following aspects: aftermath, background with\\\\none aspect per line.\\\\n# Aftermath\\\\nOnce the English had gone Menéndez and the rest of the Spanish settlers returned to ﬁnd a smoldering ruins and very little\\\\nleft. He soon and begged for help from the viceroy of Cuba and the settlement took a while to build itself back up. The\\\\ndestroyed fort was replaced with the present day Castillo de San Marcos.\\\\n# Background\\\\nWar had already been unofﬁcially declared by Philip II of Spain after the Treaty of Nonsuch in which Elizabeth I had\\\\noffered her support to the rebellious Protestant Dutch rebels. The Queen through Francis Walsingham ordered Sir Francis\\\\nDrake to lead an expedition to attack the Spanish New World in a kind of preemptive strike. Sailing from Plymouth,\\\\nEngland, he struck ﬁrst at Santiago in November 1585 then across the Atlantic at the Spanish new world city of Santo\\\\nDomingo of which was captured and ransomed on 1 January 1586 and following that successfully attacked the important\\\\ncity of Cartagena on 19 February. Drake wanted to strike at another Spanish city on the Main before ﬁnally visiting and\\\\nreplenishing Sir Walter Raleigh’s new colony of Roanoke Colony on the American East Coast. Then after this he hoped\\\\nto make the Transatlantic crossing back to England. The ﬂeet headed north, and in late April Drake put into the Spanish\\\\nCuban mainland and his men dug wells in search of fresh water and gathered supplies to help counter an outbreak of\\\\ndysentery after which he moved on. The ﬂeet traveled north within sight of land on the Florida peninsula sailing past\\\\nthe West coast. On 27 May 1586 as they approached further north a small fort was spotted on the shore, with a small\\\\ninlet close by. This was the location of St Augustine, the most northerly town in Spain’s New World Empire, and the\\\\noldest permanent colonial settlement in North America. Drake knew of the place and was also aware of the fact that\\\\nthe spanish under Pedro Menéndez de Avilés had ordered all of the French Huguenot colonists that had tried to settle\\\\nin the area executed. Drake decided on one ﬁnal opportunity to raid and plunder, and a chance to avenge his fellow Protestants.\\\\nGenerate a summary about Lakewood (Livingston, Alabama) including the following aspects:\\\\narchitecture, his-\\\\ntory with one aspect per line.\\\\n# Architecture\\\\nThe house has a plan that is relatively rare in early Alabama architecture. The plan features a brick ground ﬂoor that is topped\\\\nby one-and-a-half-stories of wood-frame construction. The ground ﬂoor originally contained domestic spaces, with the\\\\nformal rooms on the principle ﬂoor and bedrooms on the upper ﬂoor. A central hallway is present on all levels. The facade is\\\\nﬁve bays wide, with central entrance doors on the ground and principle ﬂoors. The bays are divided by two-story Doric\\\\npilasters, with the middle third of the facade occupied by a two-tiered tetrastyle Doric portico. Two curved wrought iron\\\\nstaircases ascend from ground level to the front center of the upper portico, leading to the formal entrance.\\\\n# History\\\\nLakewood was built for Joseph lake, a native of North Carolina, by Hiram W. Bardwell, a master builder. Construction\\\\nwas completed in 1840. Located adjacent to the University of West Alabama, Julia Strudwick Tutwiler, a Lake relative,\\\\nperiodically resided in the house from 1881 to 1910 while she served as president of the university. It was then known as\\\\nLivingston Normal College. The house was extensively photographed by Alex Bush for the Historic American Buildings\\\\nSurvey in November and December 1936. Lakewood has continued to be owned by descendants of the Lake family to the\\\\ncurrent day. The house and its surviving 10 acres (4.0 ha) of grounds were listed on the Places in Peril in 2012 due to the\\\\nimmediate threat of its acquisition by developers.\\\\nGenerate a summary about Carlos Moedas including the following aspects: biography, early life, political career\\\\nwith one aspect per line.\\\\n\\' metadata={\\'Published\\': \\'2023-05-11\\', \\'Title\\': \\'Active Retrieval Augmented Generation\\', \\'Authors\\': \\'Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, Graham Neubig\\', \\'Summary\\': \\'Despite the remarkable ability of large language models (LMs) to comprehend\\\\nand generate language, they have a tendency to hallucinate and create factually\\\\ninaccurate output. Augmenting LMs by retrieving information from external\\\\nknowledge resources is one promising solution. Most existing\\\\nretrieval-augmented LMs employ a retrieve-and-generate setup that only\\\\nretrieves information once based on the input. This is limiting, however, in\\\\nmore general scenarios involving generation of long texts, where continually\\\\ngathering information throughout the generation process is essential. There\\\\nhave been some past efforts to retrieve information multiple times while\\\\ngenerating outputs, which mostly retrieve documents at fixed intervals using\\\\nthe previous context as queries. In this work, we provide a generalized view of\\\\nactive retrieval augmented generation, methods that actively decide when and\\\\nwhat to retrieve across the course of the generation. We propose\\\\nForward-Looking Active REtrieval augmented generation (FLARE), a generic\\\\nretrieval-augmented generation method which iteratively uses a prediction of\\\\nthe upcoming sentence to anticipate future content, which is then utilized as a\\\\nquery to retrieve relevant documents to regenerate the sentence if it contains\\\\nlow-confidence tokens. We test FLARE along with baselines comprehensively over\\\\n4 long-form knowledge-intensive generation tasks/datasets. FLARE achieves\\\\nsuperior or competitive performance on all tasks, demonstrating the\\\\neffectiveness of our method. Code and datasets are available at\\\\nhttps://github.com/jzbjyb/FLARE.\\'}\\n', additional_kwargs={}, example=False)] /n/n/n/n\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 50%|█████     | 1/2 [00:05<00:05,  5.43s/it]\n"
          ]
        },
        {
          "ename": "InvalidRequestError",
          "evalue": "This model's maximum context length is 16385 tokens. However, your messages resulted in 25572 tokens. Please reduce the length of the messages.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidRequestError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[1;32m/Users/ukizhake/Documents/LLM-Ops-Cohort-2-main-week2-thurs/Week 2/Thursday - Evaluating Advanced Retrieval Methods in LangChain with RAGAS/Evaluating_Advanced_Retrieval_Methods_in_LangChain_with_RAGAS (Assignment Version).ipynb Cell 26\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ukizhake/Documents/LLM-Ops-Cohort-2-main-week2-thurs/Week%202/Thursday%20-%20Evaluating%20Advanced%20Retrieval%20Methods%20in%20LangChain%20with%20RAGAS/Evaluating_Advanced_Retrieval_Methods_in_LangChain_with_RAGAS%20%28Assignment%20Version%29.ipynb#X34sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m messages \u001b[39m=\u001b[39m prompt_template\u001b[39m.\u001b[39mformat_messages(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ukizhake/Documents/LLM-Ops-Cohort-2-main-week2-thurs/Week%202/Thursday%20-%20Evaluating%20Advanced%20Retrieval%20Methods%20in%20LangChain%20with%20RAGAS/Evaluating_Advanced_Retrieval_Methods_in_LangChain_with_RAGAS%20%28Assignment%20Version%29.ipynb#X34sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     context\u001b[39m=\u001b[39mtext,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ukizhake/Documents/LLM-Ops-Cohort-2-main-week2-thurs/Week%202/Thursday%20-%20Evaluating%20Advanced%20Retrieval%20Methods%20in%20LangChain%20with%20RAGAS/Evaluating_Advanced_Retrieval_Methods_in_LangChain_with_RAGAS%20%28Assignment%20Version%29.ipynb#X34sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     format_instructions\u001b[39m=\u001b[39mformat_instructions\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ukizhake/Documents/LLM-Ops-Cohort-2-main-week2-thurs/Week%202/Thursday%20-%20Evaluating%20Advanced%20Retrieval%20Methods%20in%20LangChain%20with%20RAGAS/Evaluating_Advanced_Retrieval_Methods_in_LangChain_with_RAGAS%20%28Assignment%20Version%29.ipynb#X34sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ukizhake/Documents/LLM-Ops-Cohort-2-main-week2-thurs/Week%202/Thursday%20-%20Evaluating%20Advanced%20Retrieval%20Methods%20in%20LangChain%20with%20RAGAS/Evaluating_Advanced_Retrieval_Methods_in_LangChain_with_RAGAS%20%28Assignment%20Version%29.ipynb#X34sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mprint\u001b[39m(messages, \u001b[39m\"\u001b[39m\u001b[39m/n/n/n/n\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/ukizhake/Documents/LLM-Ops-Cohort-2-main-week2-thurs/Week%202/Thursday%20-%20Evaluating%20Advanced%20Retrieval%20Methods%20in%20LangChain%20with%20RAGAS/Evaluating_Advanced_Retrieval_Methods_in_LangChain_with_RAGAS%20%28Assignment%20Version%29.ipynb#X34sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m response \u001b[39m=\u001b[39m primary_qa_llm(messages)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ukizhake/Documents/LLM-Ops-Cohort-2-main-week2-thurs/Week%202/Thursday%20-%20Evaluating%20Advanced%20Retrieval%20Methods%20in%20LangChain%20with%20RAGAS/Evaluating_Advanced_Retrieval_Methods_in_LangChain_with_RAGAS%20%28Assignment%20Version%29.ipynb#X34sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ukizhake/Documents/LLM-Ops-Cohort-2-main-week2-thurs/Week%202/Thursday%20-%20Evaluating%20Advanced%20Retrieval%20Methods%20in%20LangChain%20with%20RAGAS/Evaluating_Advanced_Retrieval_Methods_in_LangChain_with_RAGAS%20%28Assignment%20Version%29.ipynb#X34sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m   output_dict \u001b[39m=\u001b[39m question_output_parser\u001b[39m.\u001b[39mparse(response\u001b[39m.\u001b[39mcontent)\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/langchain/chat_models/base.py:595\u001b[0m, in \u001b[0;36mBaseChatModel.__call__\u001b[0;34m(self, messages, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\n\u001b[1;32m    589\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    590\u001b[0m     messages: List[BaseMessage],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    593\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[1;32m    594\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m BaseMessage:\n\u001b[0;32m--> 595\u001b[0m     generation \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate(\n\u001b[1;32m    596\u001b[0m         [messages], stop\u001b[39m=\u001b[39;49mstop, callbacks\u001b[39m=\u001b[39;49mcallbacks, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    597\u001b[0m     )\u001b[39m.\u001b[39mgenerations[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m]\n\u001b[1;32m    598\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(generation, ChatGeneration):\n\u001b[1;32m    599\u001b[0m         \u001b[39mreturn\u001b[39;00m generation\u001b[39m.\u001b[39mmessage\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/langchain/chat_models/base.py:348\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    346\u001b[0m         \u001b[39mif\u001b[39;00m run_managers:\n\u001b[1;32m    347\u001b[0m             run_managers[i]\u001b[39m.\u001b[39mon_llm_error(e)\n\u001b[0;32m--> 348\u001b[0m         \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    349\u001b[0m flattened_outputs \u001b[39m=\u001b[39m [\n\u001b[1;32m    350\u001b[0m     LLMResult(generations\u001b[39m=\u001b[39m[res\u001b[39m.\u001b[39mgenerations], llm_output\u001b[39m=\u001b[39mres\u001b[39m.\u001b[39mllm_output)\n\u001b[1;32m    351\u001b[0m     \u001b[39mfor\u001b[39;00m res \u001b[39min\u001b[39;00m results\n\u001b[1;32m    352\u001b[0m ]\n\u001b[1;32m    353\u001b[0m llm_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_combine_llm_outputs([res\u001b[39m.\u001b[39mllm_output \u001b[39mfor\u001b[39;00m res \u001b[39min\u001b[39;00m results])\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/langchain/chat_models/base.py:338\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[39mfor\u001b[39;00m i, m \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(messages):\n\u001b[1;32m    336\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    337\u001b[0m         results\u001b[39m.\u001b[39mappend(\n\u001b[0;32m--> 338\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_generate_with_cache(\n\u001b[1;32m    339\u001b[0m                 m,\n\u001b[1;32m    340\u001b[0m                 stop\u001b[39m=\u001b[39;49mstop,\n\u001b[1;32m    341\u001b[0m                 run_manager\u001b[39m=\u001b[39;49mrun_managers[i] \u001b[39mif\u001b[39;49;00m run_managers \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    342\u001b[0m                 \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    343\u001b[0m             )\n\u001b[1;32m    344\u001b[0m         )\n\u001b[1;32m    345\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    346\u001b[0m         \u001b[39mif\u001b[39;00m run_managers:\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/langchain/chat_models/base.py:490\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    486\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    487\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAsked to cache, but no cache found at `langchain.cache`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    488\u001b[0m     )\n\u001b[1;32m    489\u001b[0m \u001b[39mif\u001b[39;00m new_arg_supported:\n\u001b[0;32m--> 490\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_generate(\n\u001b[1;32m    491\u001b[0m         messages, stop\u001b[39m=\u001b[39;49mstop, run_manager\u001b[39m=\u001b[39;49mrun_manager, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    492\u001b[0m     )\n\u001b[1;32m    493\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    494\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generate(messages, stop\u001b[39m=\u001b[39mstop, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/langchain/chat_models/openai.py:345\u001b[0m, in \u001b[0;36mChatOpenAI._generate\u001b[0;34m(self, messages, stop, run_manager, stream, **kwargs)\u001b[0m\n\u001b[1;32m    343\u001b[0m message_dicts, params \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_message_dicts(messages, stop)\n\u001b[1;32m    344\u001b[0m params \u001b[39m=\u001b[39m {\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs}\n\u001b[0;32m--> 345\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompletion_with_retry(\n\u001b[1;32m    346\u001b[0m     messages\u001b[39m=\u001b[39;49mmessage_dicts, run_manager\u001b[39m=\u001b[39;49mrun_manager, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparams\n\u001b[1;32m    347\u001b[0m )\n\u001b[1;32m    348\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_chat_result(response)\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/langchain/chat_models/openai.py:284\u001b[0m, in \u001b[0;36mChatOpenAI.completion_with_retry\u001b[0;34m(self, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[39m@retry_decorator\u001b[39m\n\u001b[1;32m    281\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_completion_with_retry\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m    282\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclient\u001b[39m.\u001b[39mcreate(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 284\u001b[0m \u001b[39mreturn\u001b[39;00m _completion_with_retry(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tenacity/__init__.py:289\u001b[0m, in \u001b[0;36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(f)\n\u001b[1;32m    288\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped_f\u001b[39m(\u001b[39m*\u001b[39margs: t\u001b[39m.\u001b[39mAny, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: t\u001b[39m.\u001b[39mAny) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m t\u001b[39m.\u001b[39mAny:\n\u001b[0;32m--> 289\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m(f, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tenacity/__init__.py:379\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    377\u001b[0m retry_state \u001b[39m=\u001b[39m RetryCallState(retry_object\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m, fn\u001b[39m=\u001b[39mfn, args\u001b[39m=\u001b[39margs, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[1;32m    378\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 379\u001b[0m     do \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49miter(retry_state\u001b[39m=\u001b[39;49mretry_state)\n\u001b[1;32m    380\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(do, DoAttempt):\n\u001b[1;32m    381\u001b[0m         \u001b[39mtry\u001b[39;00m:\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tenacity/__init__.py:314\u001b[0m, in \u001b[0;36mBaseRetrying.iter\u001b[0;34m(self, retry_state)\u001b[0m\n\u001b[1;32m    312\u001b[0m is_explicit_retry \u001b[39m=\u001b[39m fut\u001b[39m.\u001b[39mfailed \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(fut\u001b[39m.\u001b[39mexception(), TryAgain)\n\u001b[1;32m    313\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (is_explicit_retry \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mretry(retry_state)):\n\u001b[0;32m--> 314\u001b[0m     \u001b[39mreturn\u001b[39;00m fut\u001b[39m.\u001b[39;49mresult()\n\u001b[1;32m    316\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mafter \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mafter(retry_state)\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/_base.py:449\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[39mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    448\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39m==\u001b[39m FINISHED:\n\u001b[0;32m--> 449\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__get_result()\n\u001b[1;32m    451\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_condition\u001b[39m.\u001b[39mwait(timeout)\n\u001b[1;32m    453\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception:\n\u001b[1;32m    400\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 401\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception\n\u001b[1;32m    402\u001b[0m     \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    403\u001b[0m         \u001b[39m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    404\u001b[0m         \u001b[39mself\u001b[39m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tenacity/__init__.py:382\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(do, DoAttempt):\n\u001b[1;32m    381\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 382\u001b[0m         result \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    383\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m:  \u001b[39m# noqa: B902\u001b[39;00m\n\u001b[1;32m    384\u001b[0m         retry_state\u001b[39m.\u001b[39mset_exception(sys\u001b[39m.\u001b[39mexc_info())  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/langchain/chat_models/openai.py:282\u001b[0m, in \u001b[0;36mChatOpenAI.completion_with_retry.<locals>._completion_with_retry\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[39m@retry_decorator\u001b[39m\n\u001b[1;32m    281\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_completion_with_retry\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m--> 282\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclient\u001b[39m.\u001b[39;49mcreate(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/openai/api_resources/chat_completion.py:25\u001b[0m, in \u001b[0;36mChatCompletion.create\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m     24\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 25\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mcreate(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     26\u001b[0m     \u001b[39mexcept\u001b[39;00m TryAgain \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     27\u001b[0m         \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m time\u001b[39m.\u001b[39mtime() \u001b[39m>\u001b[39m start \u001b[39m+\u001b[39m timeout:\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/openai/api_resources/abstract/engine_api_resource.py:153\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m    128\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[1;32m    129\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams,\n\u001b[1;32m    137\u001b[0m ):\n\u001b[1;32m    138\u001b[0m     (\n\u001b[1;32m    139\u001b[0m         deployment_id,\n\u001b[1;32m    140\u001b[0m         engine,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    150\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams\n\u001b[1;32m    151\u001b[0m     )\n\u001b[0;32m--> 153\u001b[0m     response, _, api_key \u001b[39m=\u001b[39m requestor\u001b[39m.\u001b[39;49mrequest(\n\u001b[1;32m    154\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mpost\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    155\u001b[0m         url,\n\u001b[1;32m    156\u001b[0m         params\u001b[39m=\u001b[39;49mparams,\n\u001b[1;32m    157\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    158\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    159\u001b[0m         request_id\u001b[39m=\u001b[39;49mrequest_id,\n\u001b[1;32m    160\u001b[0m         request_timeout\u001b[39m=\u001b[39;49mrequest_timeout,\n\u001b[1;32m    161\u001b[0m     )\n\u001b[1;32m    163\u001b[0m     \u001b[39mif\u001b[39;00m stream:\n\u001b[1;32m    164\u001b[0m         \u001b[39m# must be an iterator\u001b[39;00m\n\u001b[1;32m    165\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(response, OpenAIResponse)\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/openai/api_requestor.py:298\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[1;32m    278\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    279\u001b[0m     method,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    286\u001b[0m     request_timeout: Optional[Union[\u001b[39mfloat\u001b[39m, Tuple[\u001b[39mfloat\u001b[39m, \u001b[39mfloat\u001b[39m]]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    287\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[39mbool\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[1;32m    288\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequest_raw(\n\u001b[1;32m    289\u001b[0m         method\u001b[39m.\u001b[39mlower(),\n\u001b[1;32m    290\u001b[0m         url,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    296\u001b[0m         request_timeout\u001b[39m=\u001b[39mrequest_timeout,\n\u001b[1;32m    297\u001b[0m     )\n\u001b[0;32m--> 298\u001b[0m     resp, got_stream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response(result, stream)\n\u001b[1;32m    299\u001b[0m     \u001b[39mreturn\u001b[39;00m resp, got_stream, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_key\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/openai/api_requestor.py:700\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    692\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m    693\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interpret_response_line(\n\u001b[1;32m    694\u001b[0m             line, result\u001b[39m.\u001b[39mstatus_code, result\u001b[39m.\u001b[39mheaders, stream\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    695\u001b[0m         )\n\u001b[1;32m    696\u001b[0m         \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m parse_stream(result\u001b[39m.\u001b[39miter_lines())\n\u001b[1;32m    697\u001b[0m     ), \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    698\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m--> 700\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response_line(\n\u001b[1;32m    701\u001b[0m             result\u001b[39m.\u001b[39;49mcontent\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m    702\u001b[0m             result\u001b[39m.\u001b[39;49mstatus_code,\n\u001b[1;32m    703\u001b[0m             result\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    704\u001b[0m             stream\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    705\u001b[0m         ),\n\u001b[1;32m    706\u001b[0m         \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    707\u001b[0m     )\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/openai/api_requestor.py:763\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    761\u001b[0m stream_error \u001b[39m=\u001b[39m stream \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39merror\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m resp\u001b[39m.\u001b[39mdata\n\u001b[1;32m    762\u001b[0m \u001b[39mif\u001b[39;00m stream_error \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39m200\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m rcode \u001b[39m<\u001b[39m \u001b[39m300\u001b[39m:\n\u001b[0;32m--> 763\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle_error_response(\n\u001b[1;32m    764\u001b[0m         rbody, rcode, resp\u001b[39m.\u001b[39mdata, rheaders, stream_error\u001b[39m=\u001b[39mstream_error\n\u001b[1;32m    765\u001b[0m     )\n\u001b[1;32m    766\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
            "\u001b[0;31mInvalidRequestError\u001b[0m: This model's maximum context length is 16385 tokens. However, your messages resulted in 25572 tokens. Please reduce the length of the messages."
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "qac_triples = []\n",
        "\n",
        "for text in tqdm(texts[:2]):\n",
        "  messages = prompt_template.format_messages(\n",
        "      context=text,\n",
        "      format_instructions=format_instructions\n",
        "  )\n",
        "  print(messages, \"/n/n/n/n\")\n",
        "  response = primary_qa_llm(messages)\n",
        "  try:\n",
        "    output_dict = question_output_parser.parse(response.content)\n",
        "  except Exception as e:\n",
        "    continue\n",
        "  output_dict[\"context\"] = text\n",
        "  qac_triples.append(output_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "vNB9Z2DrX2TC"
      },
      "outputs": [],
      "source": [
        "primary_ground_truth_llm = ChatOpenAI(model_name=\"gpt-4\", temperature=0)\n",
        "\n",
        "answer_schema = ResponseSchema(\n",
        "    name=\"answer\",\n",
        "    description=\"an answer to the question\"\n",
        ")\n",
        "\n",
        "answer_response_schemas = [\n",
        "    answer_schema,\n",
        "]\n",
        "\n",
        "answer_output_parser = StructuredOutputParser.from_response_schemas(answer_response_schemas)\n",
        "format_instructions = answer_output_parser.get_format_instructions()\n",
        "\n",
        "qa_template = \"\"\"\\\n",
        "You are a University Professor creating a test for advanced students. For each question and context, create an answer.\n",
        "\n",
        "answer: a question about the context.\n",
        "\n",
        "Format the output as JSON with the following keys:\n",
        "answer\n",
        "\n",
        "question: {question}\n",
        "context: {context}\n",
        "\"\"\"\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_template(template=qa_template)\n",
        "\n",
        "messages = prompt_template.format_messages(\n",
        "    context=qac_triples[0][\"context\"],\n",
        "    question=qac_triples[0][\"question\"],\n",
        "    format_instructions=format_instructions\n",
        ")\n",
        "\n",
        "response = primary_ground_truth_llm(messages)\n",
        "output_dict = answer_output_parser.parse(response.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rk-_lRR6fn5U",
        "outputId": "ed311efd-16c5-4590-82bc-2e139fc7e238"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "answer\n",
            "The advantages of retrieval-augmented text generation compared to generation-based models include: 1) The knowledge is not necessary to be implicitly stored in model parameters, but is explicitly acquired in a plug-and-play manner, leading to great scalability; 2) Instead of generating from scratch, the paradigm generating text from some retrieved human-written reference, which potentially alleviates the difficulty of text generation. It has achieved state-of-the-art performance in many NLP tasks.\n"
          ]
        }
      ],
      "source": [
        "for k, v in output_dict.items():\n",
        "  print(k)\n",
        "  print(v)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "yCdH0e9rrAKd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 5/5 [00:33<00:00,  6.77s/it]\n"
          ]
        }
      ],
      "source": [
        "for triple in tqdm(qac_triples):\n",
        "  messages = prompt_template.format_messages(\n",
        "      context=triple[\"context\"],\n",
        "      question=triple[\"question\"],\n",
        "      format_instructions=format_instructions\n",
        "  )\n",
        "  response = primary_ground_truth_llm(messages)\n",
        "  try:\n",
        "    output_dict = answer_output_parser.parse(response.content)\n",
        "  except Exception as e:\n",
        "    continue\n",
        "  triple[\"answer\"] = output_dict[\"answer\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "rrHXgH9Qs1ep"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting datasets\n",
            "  Obtaining dependency information for datasets from https://files.pythonhosted.org/packages/09/7e/fd4d6441a541dba61d0acb3c1fd5df53214c2e9033854e837a99dd9e0793/datasets-2.14.5-py3-none-any.whl.metadata\n",
            "  Downloading datasets-2.14.5-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets) (1.25.1)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets) (12.0.1)\n",
            "Collecting dill<0.3.8,>=0.3.0 (from datasets)\n",
            "  Obtaining dependency information for dill<0.3.8,>=0.3.0 from https://files.pythonhosted.org/packages/f5/3a/74a29b11cf2cdfcd6ba89c0cecd70b37cd1ba7b77978ce611eb7a146a832/dill-0.3.7-py3-none-any.whl.metadata\n",
            "  Downloading dill-0.3.7-py3-none-any.whl.metadata (9.9 kB)\n",
            "Requirement already satisfied: pandas in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets) (2.1.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets) (4.65.0)\n",
            "Collecting xxhash (from datasets)\n",
            "  Obtaining dependency information for xxhash from https://files.pythonhosted.org/packages/ee/23/020ff3fa540e0d06886b6b866f1e173c554723e04f286ac205c5ddeb479e/xxhash-3.3.0-cp311-cp311-macosx_11_0_arm64.whl.metadata\n",
            "  Downloading xxhash-3.3.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (12 kB)\n",
            "Collecting multiprocess (from datasets)\n",
            "  Obtaining dependency information for multiprocess from https://files.pythonhosted.org/packages/e7/41/96ac938770ba6e7d5ae1d8c9cafebac54b413549042c6260f0d0a6ec6622/multiprocess-0.70.15-py311-none-any.whl.metadata\n",
            "  Downloading multiprocess-0.70.15-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec[http]<2023.9.0,>=2023.1.0 (from datasets)\n",
            "  Obtaining dependency information for fsspec[http]<2023.9.0,>=2023.1.0 from https://files.pythonhosted.org/packages/e3/bd/4c0a4619494188a9db5d77e2100ab7d544a42e76b2447869d8e124e981d8/fsspec-2023.6.0-py3-none-any.whl.metadata\n",
            "  Downloading fsspec-2023.6.0-py3-none-any.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: aiohttp in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets) (3.8.5)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets) (0.16.4)\n",
            "Requirement already satisfied: packaging in /Users/ukizhake/Library/Python/3.11/lib/python/site-packages (from datasets) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp->datasets) (3.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.7.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests>=2.19.0->datasets) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/ukizhake/Library/Python/3.11/lib/python/site-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pandas->datasets) (2023.3)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pandas->datasets) (2023.3)\n",
            "Requirement already satisfied: six>=1.5 in /Users/ukizhake/Library/Python/3.11/lib/python/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Using cached datasets-2.14.5-py3-none-any.whl (519 kB)\n",
            "Using cached dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "Using cached multiprocess-0.70.15-py311-none-any.whl (135 kB)\n",
            "Using cached xxhash-3.3.0-cp311-cp311-macosx_11_0_arm64.whl (30 kB)\n",
            "Using cached fsspec-2023.6.0-py3-none-any.whl (163 kB)\n",
            "Installing collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2023.9.2\n",
            "    Uninstalling fsspec-2023.9.2:\n",
            "      Successfully uninstalled fsspec-2023.9.2\n",
            "Successfully installed datasets-2.14.5 dill-0.3.7 fsspec-2023.6.0 multiprocess-0.70.15 xxhash-3.3.0\n"
          ]
        }
      ],
      "source": [
        "!pip3 install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "uAvGGTyXsoHQ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "\n",
        "ground_truth_qac_set = pd.DataFrame(qac_triples)\n",
        "ground_truth_qac_set[\"context\"] = ground_truth_qac_set[\"context\"].map(lambda x: str(x.page_content))\n",
        "ground_truth_qac_set = ground_truth_qac_set.rename(columns={\"answer\" : \"ground_truth\"})\n",
        "\n",
        "\n",
        "eval_dataset = Dataset.from_pandas(ground_truth_qac_set)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "9459643d14994e269ec3897e5b071029",
            "285688f9e00548388c923b0bc2bc38fd",
            "8df3b60cdb35424da81299c607e17d0f",
            "7f925c5a99194d4daec387f3847337b8",
            "349dbc81cb3843caba834cd940e4d41c",
            "4316578e814246358ad08c55d80e44df",
            "9ddeea42439a49fd9fe2c310e8facb07",
            "5d6474260ff648adbadf28e8e0839fa2",
            "db3e3d58eeda4567964682c366157086",
            "ff4ca4f11c3843809b0fb2c194b58327",
            "8d4ac912eaf94f6c8e013977133a4906"
          ]
        },
        "id": "Nhp5X4M8zqrm",
        "outputId": "1190ec42-5c1d-4216-b734-e7c8dc7ee1cf"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f6b9230b627a42db9f86d1d139c89e86",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Creating CSV from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "23017"
            ]
          },
          "execution_count": 73,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "eval_dataset.to_csv(\"groundtruth_eval_dataset.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Al5cagr-rvL"
      },
      "source": [
        "### Evaluating RAG Pipelines\n",
        "\n",
        "If you skipped ahead and need to load the `.csv` directly - uncomment the code below.\n",
        "\n",
        "If you're using Colab to do this notebook - please ensure you add it to your session files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "QJhes58R66-P"
      },
      "outputs": [],
      "source": [
        "# from datasets import Dataset\n",
        "# eval_dataset = Dataset.from_csv(\"groundtruth_eval_dataset.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5fAD8c_kthWc",
        "outputId": "35e579c6-b184-456f-84e9-83a39e0408ac"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['question', 'context', 'ground_truth'],\n",
              "    num_rows: 5\n",
              "})"
            ]
          },
          "execution_count": 75,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "eval_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IqFYbjLK-6X7"
      },
      "source": [
        "### Evaluation Using RAGAS\n",
        "\n",
        "Now we can evaluate using RAGAS!\n",
        "\n",
        "The set-up is fairly straightforward - we simply need to create a dataset with our generated answers and our contexts, and then evaluate using the framework.\n",
        "\n",
        "More details on the specific metrics can be found [here](https://github.com/explodinggradients/ragas/blob/main/docs/metrics.md)!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting ragas\n",
            "  Obtaining dependency information for ragas from https://files.pythonhosted.org/packages/24/99/3d07f2377b9776363b7a414665427061ec6db87449743c213fe1d32253ca/ragas-0.0.16-py3-none-any.whl.metadata\n",
            "  Downloading ragas-0.0.16-py3-none-any.whl.metadata (7.6 kB)\n",
            "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from ragas) (1.25.1)\n",
            "Collecting transformers (from ragas)\n",
            "  Obtaining dependency information for transformers from https://files.pythonhosted.org/packages/98/46/f6a79f944d5c7763a9bc13b2aa6ac72daf43a6551f5fb03bccf0a9c2fec1/transformers-4.33.3-py3-none-any.whl.metadata\n",
            "  Downloading transformers-4.33.3-py3-none-any.whl.metadata (119 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.9/119.9 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentence-transformers (from ragas)\n",
            "  Using cached sentence_transformers-2.2.2-py3-none-any.whl\n",
            "Requirement already satisfied: datasets in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from ragas) (2.14.5)\n",
            "Requirement already satisfied: tiktoken in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from ragas) (0.4.0)\n",
            "Collecting langchain>=0.0.288 (from ragas)\n",
            "  Obtaining dependency information for langchain>=0.0.288 from https://files.pythonhosted.org/packages/a1/4c/7f35944782b091ea4827ed25b474785960a03d639c3c7174ab7c416abeae/langchain-0.0.304-py3-none-any.whl.metadata\n",
            "  Downloading langchain-0.0.304-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: openai in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from ragas) (0.27.8)\n",
            "Requirement already satisfied: pydantic<2.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from ragas) (1.10.12)\n",
            "Collecting pysbd>=0.3.4 (from ragas)\n",
            "  Using cached pysbd-0.3.4-py3-none-any.whl (71 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from langchain>=0.0.288->ragas) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from langchain>=0.0.288->ragas) (2.0.19)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from langchain>=0.0.288->ragas) (3.8.5)\n",
            "Requirement already satisfied: anyio<4.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from langchain>=0.0.288->ragas) (3.7.1)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from langchain>=0.0.288->ragas) (0.5.13)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain>=0.0.288->ragas)\n",
            "  Obtaining dependency information for jsonpatch<2.0,>=1.33 from https://files.pythonhosted.org/packages/73/07/02e16ed01e04a374e644b575638ec7987ae846d25ad97bcc9945a3ee4b0e/jsonpatch-1.33-py2.py3-none-any.whl.metadata\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting langsmith<0.1.0,>=0.0.38 (from langchain>=0.0.288->ragas)\n",
            "  Obtaining dependency information for langsmith<0.1.0,>=0.0.38 from https://files.pythonhosted.org/packages/70/31/4bd6640c0e2033849630fefe885430236948c91e3b501fae32705d5118dc/langsmith-0.0.41-py3-none-any.whl.metadata\n",
            "  Downloading langsmith-0.0.41-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from langchain>=0.0.288->ragas) (2.8.4)\n",
            "Requirement already satisfied: requests<3,>=2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from langchain>=0.0.288->ragas) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from langchain>=0.0.288->ragas) (8.2.2)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pydantic<2.0->ragas) (4.7.1)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets->ragas) (12.0.1)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets->ragas) (0.3.7)\n",
            "Requirement already satisfied: pandas in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets->ragas) (2.1.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets->ragas) (4.65.0)\n",
            "Requirement already satisfied: xxhash in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets->ragas) (3.3.0)\n",
            "Requirement already satisfied: multiprocess in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets->ragas) (0.70.15)\n",
            "Requirement already satisfied: fsspec[http]<2023.9.0,>=2023.1.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets->ragas) (2023.6.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets->ragas) (0.16.4)\n",
            "Requirement already satisfied: packaging in /Users/ukizhake/Library/Python/3.11/lib/python/site-packages (from datasets->ragas) (23.1)\n",
            "Collecting torch>=1.6.0 (from sentence-transformers->ragas)\n",
            "  Using cached torch-2.0.1-cp311-none-macosx_11_0_arm64.whl (55.8 MB)\n",
            "Collecting torchvision (from sentence-transformers->ragas)\n",
            "  Using cached torchvision-0.15.2-cp311-cp311-macosx_11_0_arm64.whl (1.4 MB)\n",
            "Requirement already satisfied: scikit-learn in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from sentence-transformers->ragas) (1.3.0)\n",
            "Requirement already satisfied: scipy in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from sentence-transformers->ragas) (1.11.1)\n",
            "Collecting nltk (from sentence-transformers->ragas)\n",
            "  Using cached nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
            "Collecting sentencepiece (from sentence-transformers->ragas)\n",
            "  Using cached sentencepiece-0.1.99-cp311-cp311-macosx_11_0_arm64.whl (1.2 MB)\n",
            "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers->ragas) (3.12.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers->ragas) (2023.6.3)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers->ragas)\n",
            "  Using cached tokenizers-0.13.3-cp311-cp311-macosx_12_0_arm64.whl (3.9 MB)\n",
            "Collecting safetensors>=0.3.1 (from transformers->ragas)\n",
            "  Obtaining dependency information for safetensors>=0.3.1 from https://files.pythonhosted.org/packages/34/0e/12d55d5dd648b8f7ea7216c5b7cef9703b4dbd3b2a042872c711d5e98551/safetensors-0.3.3-cp311-cp311-macosx_13_0_arm64.whl.metadata\n",
            "  Downloading safetensors-0.3.3-cp311-cp311-macosx_13_0_arm64.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.288->ragas) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.288->ragas) (3.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.288->ragas) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.288->ragas) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.288->ragas) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.288->ragas) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.288->ragas) (1.3.1)\n",
            "Requirement already satisfied: idna>=2.8 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from anyio<4.0->langchain>=0.0.288->ragas) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from anyio<4.0->langchain>=0.0.288->ragas) (1.3.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain>=0.0.288->ragas) (3.20.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain>=0.0.288->ragas) (0.9.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain>=0.0.288->ragas) (2.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests<3,>=2->langchain>=0.0.288->ragas) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests<3,>=2->langchain>=0.0.288->ragas) (2023.7.22)\n",
            "Requirement already satisfied: sympy in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch>=1.6.0->sentence-transformers->ragas) (1.12)\n",
            "Requirement already satisfied: networkx in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch>=1.6.0->sentence-transformers->ragas) (3.1)\n",
            "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch>=1.6.0->sentence-transformers->ragas) (3.1.2)\n",
            "Requirement already satisfied: click in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from nltk->sentence-transformers->ragas) (8.1.5)\n",
            "Requirement already satisfied: joblib in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from nltk->sentence-transformers->ragas) (1.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/ukizhake/Library/Python/3.11/lib/python/site-packages (from pandas->datasets->ragas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pandas->datasets->ragas) (2023.3)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pandas->datasets->ragas) (2023.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from scikit-learn->sentence-transformers->ragas) (3.2.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torchvision->sentence-transformers->ragas) (9.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /Users/ukizhake/Library/Python/3.11/lib/python/site-packages (from python-dateutil>=2.8.2->pandas->datasets->ragas) (1.16.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain>=0.0.288->ragas) (1.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from jinja2->torch>=1.6.0->sentence-transformers->ragas) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from sympy->torch>=1.6.0->sentence-transformers->ragas) (1.3.0)\n",
            "Using cached ragas-0.0.16-py3-none-any.whl (38 kB)\n",
            "Using cached langchain-0.0.304-py3-none-any.whl (1.7 MB)\n",
            "Downloading transformers-4.33.3-py3-none-any.whl (7.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hUsing cached jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Downloading langsmith-0.0.41-py3-none-any.whl (39 kB)\n",
            "Using cached safetensors-0.3.3-cp311-cp311-macosx_13_0_arm64.whl (406 kB)\n",
            "Installing collected packages: tokenizers, sentencepiece, safetensors, pysbd, nltk, jsonpatch, torch, langsmith, transformers, torchvision, langchain, sentence-transformers, ragas\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.14.0\n",
            "    Uninstalling tokenizers-0.14.0:\n",
            "      Successfully uninstalled tokenizers-0.14.0\n",
            "  Attempting uninstall: langsmith\n",
            "    Found existing installation: langsmith 0.0.14\n",
            "    Uninstalling langsmith-0.0.14:\n",
            "      Successfully uninstalled langsmith-0.0.14\n",
            "  Attempting uninstall: langchain\n",
            "    Found existing installation: langchain 0.0.243\n",
            "    Uninstalling langchain-0.0.243:\n",
            "      Successfully uninstalled langchain-0.0.243\n",
            "Successfully installed jsonpatch-1.33 langchain-0.0.304 langsmith-0.0.41 nltk-3.8.1 pysbd-0.3.4 ragas-0.0.16 safetensors-0.3.3 sentence-transformers-2.2.2 sentencepiece-0.1.99 tokenizers-0.13.3 torch-2.0.1 torchvision-0.15.2 transformers-4.33.3\n"
          ]
        }
      ],
      "source": [
        "!pip3 install ragas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "1eBoHaf5t4w8"
      },
      "outputs": [],
      "source": [
        "from ragas.metrics import (\n",
        "    answer_relevancy,\n",
        "    faithfulness,\n",
        "    context_recall,\n",
        "    context_precision,\n",
        ")\n",
        "from ragas.metrics.critique import harmfulness\n",
        "from ragas import evaluate\n",
        "\n",
        "def create_ragas_dataset(rag_pipeline, eval_dataset):\n",
        "  rag_dataset = []\n",
        "  for row in tqdm(eval_dataset):\n",
        "    answer = rag_pipeline({\"query\" : row[\"question\"]})\n",
        "    rag_dataset.append(\n",
        "        {\"question\" : row[\"question\"],\n",
        "         \"answer\" : answer[\"result\"],\n",
        "         \"contexts\" : [context.page_content for context in answer[\"source_documents\"]],\n",
        "         \"ground_truths\" : [row[\"ground_truth\"]]\n",
        "         }\n",
        "    )\n",
        "  rag_df = pd.DataFrame(rag_dataset)\n",
        "  rag_eval_dataset = Dataset.from_pandas(rag_df)\n",
        "  return rag_eval_dataset\n",
        "\n",
        "def evaluate_ragas_dataset(ragas_dataset):\n",
        "  result = evaluate(\n",
        "    ragas_dataset,\n",
        "    metrics=[\n",
        "        context_precision,\n",
        "        faithfulness,\n",
        "        answer_relevancy,\n",
        "        context_recall,\n",
        "    ],\n",
        "  )\n",
        "  return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4c4Jd8G_lXY"
      },
      "source": [
        "Lets create our dataset first:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R7oXgcjkuopO",
        "outputId": "62d20b9f-fb45-4c31-b818-523e3a23561d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 5/5 [00:12<00:00,  2.50s/it]\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "\n",
        "basic_qa_ragas_dataset = create_ragas_dataset(\n",
        "    qa_chain,### YOUR CODE HERE\n",
        "    eval_dataset### YOUR CODE HERE\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Obbgw3im_n01"
      },
      "source": [
        "Save it for later:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "79e167d43da549c292c2573c782581b2",
            "700f1814c1a14134b2d12a54d018416d",
            "eeb1186495bb4580af9e99ba60c3ddd7",
            "9362cbc2937c40eb821d05afd7cdef77",
            "4fc6ddfce74a4957bdb03822d9130cb9",
            "ad146de916d1474ca3bf252d1c68ca6f",
            "6b55cf5bf0d14e7ba1384b1faad82f62",
            "36183c3078ac4be8a0fd48de8c00ff09",
            "21238add60ea411e937a445c154efaa7",
            "3ce331032ea44ea2add34ae1c04002c5",
            "18dbdffae47e407da2824cccb96bb27b"
          ]
        },
        "id": "6FG8x4i3yZ2B",
        "outputId": "0d4f62b3-5f4f-4121-8a81-cff3f1c3f37d"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1be8d879f1c8487b87a7b78c6ac6c3fa",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Creating CSV from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "15152"
            ]
          },
          "execution_count": 85,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "basic_qa_ragas_dataset.to_csv(\n",
        "    \"basic_qa_ragas_dataset.csv\"### YOUR CODE HERE\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A5I_d_RT_pFr"
      },
      "source": [
        "And finally - evaluate how it did!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ywp3Rwavy9pc",
        "outputId": "4934d132-a653-42ef-b6f9-cc2e8148baf4"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1591ccd4fcdf40869ce1cbde8b8ce01f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)lve/main/config.json:   0%|          | 0.00/647 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "673c40bcd7c24886b845bd52c9ba5a5c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading pytorch_model.bin:   0%|          | 0.00/57.4M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "060f67388b384d75ac37b9a538acaa8e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)okenizer_config.json:   0%|          | 0.00/517 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "eb5ebda192bf4d40882d9cfaf3b12e35",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "573221d51ab54ba69a58586b6ed5f1c0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "evaluating with [context_relevancy]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/1 [00:00<?, ?it/s]WARNING:root:The 'context_relevancy' metric is going to be deprecated soon! Please use the 'context_precision' metric instead. It is a drop-in replacement just a simple search and replace should work.\n",
            "100%|██████████| 1/1 [00:09<00:00,  9.26s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "evaluating with [faithfulness]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:37<00:00, 37.52s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "evaluating with [answer_relevancy]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:07<00:00,  7.12s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "evaluating with [context_recall]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:19<00:00, 19.62s/it]\n"
          ]
        }
      ],
      "source": [
        "basic_qa_result = evaluate_ragas_dataset(\n",
        "    basic_qa_ragas_dataset### YOUR CODE HERE\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m4oYnKTn15gY",
        "outputId": "35fe5193-6ff0-47a3-b467-a82bcedbd487"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'ragas_score': 0.0956, 'context_relevancy': 0.0272, 'faithfulness': 0.9500, 'answer_relevancy': 0.9832, 'context_recall': 0.3333}"
            ]
          },
          "execution_count": 89,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "basic_qa_result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SwhBxlxYAdno"
      },
      "source": [
        "### Testing Other Retrievers\n",
        "\n",
        "Now we can test our how changing our Retriever impacts our RAGAS evaluation!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "qnfy4VNkzZi2"
      },
      "outputs": [],
      "source": [
        "def create_qa_chain(retriever):\n",
        "  primary_qa_llm = ChatOpenAI(model_name=\"gpt-3.5-turbo-16k\", temperature=0)\n",
        "\n",
        "  created_qa_chain = RetrievalQA.from_chain_type(\n",
        "      primary_qa_llm,\n",
        "      retriever=retriever,\n",
        "      return_source_documents=True\n",
        "  )\n",
        "\n",
        "  return created_qa_chain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOPp4Xq7AvEx"
      },
      "source": [
        "#### Parent Document Retriever\n",
        "\n",
        "One of the easier ways we can imagine improving a retriever is to embed our documents into small chunks, and then retrieve a significant amount of additional context that \"surrounds\" the found context.\n",
        "\n",
        "You can read more about this method [here](https://python.langchain.com/docs/modules/data_connection/retrievers/parent_document_retriever)!\n",
        "\n",
        "- [`ParentDocumentRetriever`](https://api.python.langchain.com/en/latest/retrievers/langchain.retrievers.parent_document_retriever.ParentDocumentRetriever.html?highlight=parentdocumentretriever#langchain-retrievers-parent-document-retriever-parentdocumentretriever)\n",
        "- [`InMemoryStore`](https://api.python.langchain.com/en/latest/retrievers/langchain.retrievers.parent_document_retriever.ParentDocumentRetriever.html?highlight=parentdocumentretriever#langchain-retrievers-parent-document-retriever-parentdocumentretriever)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: langchain in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (0.0.304)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from langchain) (2.0.19)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from langchain) (3.8.5)\n",
            "Requirement already satisfied: anyio<4.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from langchain) (3.7.1)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from langchain) (0.5.13)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from langchain) (1.33)\n",
            "Requirement already satisfied: langsmith<0.1.0,>=0.0.38 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from langchain) (0.0.41)\n",
            "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from langchain) (2.8.4)\n",
            "Requirement already satisfied: numpy<2,>=1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from langchain) (1.25.1)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from langchain) (1.10.12)\n",
            "Requirement already satisfied: requests<3,>=2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from langchain) (8.2.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (3.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: idna>=2.8 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from anyio<4.0->langchain) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from anyio<4.0->langchain) (1.3.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.20.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pydantic<3,>=1->langchain) (4.7.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests<3,>=2->langchain) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests<3,>=2->langchain) (2023.7.22)\n",
            "Requirement already satisfied: packaging>=17.0 in /Users/ukizhake/Library/Python/3.11/lib/python/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.7,>=0.5.7->langchain) (23.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n"
          ]
        }
      ],
      "source": [
        "!pip3 install langchain\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: langchain in /Users/ukizhake/miniconda3/lib/python3.11/site-packages (0.0.304)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /Users/ukizhake/miniconda3/lib/python3.11/site-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /Users/ukizhake/miniconda3/lib/python3.11/site-packages (from langchain) (2.0.21)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /Users/ukizhake/miniconda3/lib/python3.11/site-packages (from langchain) (3.8.5)\n",
            "Requirement already satisfied: anyio<4.0 in /Users/ukizhake/miniconda3/lib/python3.11/site-packages (from langchain) (3.7.1)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /Users/ukizhake/miniconda3/lib/python3.11/site-packages (from langchain) (0.5.14)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/ukizhake/miniconda3/lib/python3.11/site-packages (from langchain) (1.33)\n",
            "Requirement already satisfied: langsmith<0.1.0,>=0.0.38 in /Users/ukizhake/miniconda3/lib/python3.11/site-packages (from langchain) (0.0.38)\n",
            "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /Users/ukizhake/miniconda3/lib/python3.11/site-packages (from langchain) (2.8.6)\n",
            "Requirement already satisfied: numpy<2,>=1 in /Users/ukizhake/miniconda3/lib/python3.11/site-packages (from langchain) (1.26.0)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /Users/ukizhake/miniconda3/lib/python3.11/site-packages (from langchain) (1.10.12)\n",
            "Requirement already satisfied: requests<3,>=2 in /Users/ukizhake/miniconda3/lib/python3.11/site-packages (from langchain) (2.29.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /Users/ukizhake/miniconda3/lib/python3.11/site-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /Users/ukizhake/miniconda3/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /Users/ukizhake/miniconda3/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.0.4)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/ukizhake/miniconda3/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/ukizhake/miniconda3/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/ukizhake/miniconda3/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /Users/ukizhake/miniconda3/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /Users/ukizhake/miniconda3/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: idna>=2.8 in /Users/ukizhake/miniconda3/lib/python3.11/site-packages (from anyio<4.0->langchain) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /Users/ukizhake/miniconda3/lib/python3.11/site-packages (from anyio<4.0->langchain) (1.3.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Users/ukizhake/miniconda3/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.20.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /Users/ukizhake/miniconda3/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /Users/ukizhake/miniconda3/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain) (2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /Users/ukizhake/miniconda3/lib/python3.11/site-packages (from pydantic<3,>=1->langchain) (4.8.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/ukizhake/miniconda3/lib/python3.11/site-packages (from requests<3,>=2->langchain) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/ukizhake/miniconda3/lib/python3.11/site-packages (from requests<3,>=2->langchain) (2023.5.7)\n",
            "Requirement already satisfied: packaging>=17.0 in /Users/ukizhake/miniconda3/lib/python3.11/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.7,>=0.5.7->langchain) (23.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/ukizhake/miniconda3/lib/python3.11/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement langchain.schema (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for langchain.schema\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install langchain\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting langchain==0.0.304\n",
            "  Using cached langchain-0.0.304-py3-none-any.whl (1.7 MB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /Users/ukizhake/miniconda3/lib/python3.11/site-packages (from langchain==0.0.304) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /Users/ukizhake/miniconda3/lib/python3.11/site-packages (from langchain==0.0.304) (2.0.21)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /Users/ukizhake/miniconda3/lib/python3.11/site-packages (from langchain==0.0.304) (3.8.5)\n",
            "Requirement already satisfied: anyio<4.0 in /Users/ukizhake/miniconda3/lib/python3.11/site-packages (from langchain==0.0.304) (3.7.1)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /Users/ukizhake/miniconda3/lib/python3.11/site-packages (from langchain==0.0.304) (0.5.14)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/ukizhake/miniconda3/lib/python3.11/site-packages (from langchain==0.0.304) (1.33)\n",
            "Requirement already satisfied: langsmith<0.1.0,>=0.0.38 in /Users/ukizhake/miniconda3/lib/python3.11/site-packages (from langchain==0.0.304) (0.0.38)\n",
            "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /Users/ukizhake/miniconda3/lib/python3.11/site-packages (from langchain==0.0.304) (2.8.6)\n",
            "Requirement already satisfied: numpy<2,>=1 in /Users/ukizhake/miniconda3/lib/python3.11/site-packages (from langchain==0.0.304) (1.26.0)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /Users/ukizhake/miniconda3/lib/python3.11/site-packages (from langchain==0.0.304) (1.10.12)\n",
            "Requirement already satisfied: requests<3,>=2 in /Users/ukizhake/miniconda3/lib/python3.11/site-packages (from langchain==0.0.304) (2.29.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /Users/ukizhake/miniconda3/lib/python3.11/site-packages (from langchain==0.0.304) (8.2.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /Users/ukizhake/miniconda3/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.304) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /Users/ukizhake/miniconda3/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.304) (2.0.4)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/ukizhake/miniconda3/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.304) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/ukizhake/miniconda3/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.304) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/ukizhake/miniconda3/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.304) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /Users/ukizhake/miniconda3/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.304) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /Users/ukizhake/miniconda3/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.304) (1.3.1)\n",
            "Requirement already satisfied: idna>=2.8 in /Users/ukizhake/miniconda3/lib/python3.11/site-packages (from anyio<4.0->langchain==0.0.304) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /Users/ukizhake/miniconda3/lib/python3.11/site-packages (from anyio<4.0->langchain==0.0.304) (1.3.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Users/ukizhake/miniconda3/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.0.304) (3.20.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /Users/ukizhake/miniconda3/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.0.304) (0.9.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /Users/ukizhake/miniconda3/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain==0.0.304) (2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /Users/ukizhake/miniconda3/lib/python3.11/site-packages (from pydantic<3,>=1->langchain==0.0.304) (4.8.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/ukizhake/miniconda3/lib/python3.11/site-packages (from requests<3,>=2->langchain==0.0.304) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/ukizhake/miniconda3/lib/python3.11/site-packages (from requests<3,>=2->langchain==0.0.304) (2023.5.7)\n",
            "Requirement already satisfied: packaging>=17.0 in /Users/ukizhake/miniconda3/lib/python3.11/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.7,>=0.5.7->langchain==0.0.304) (23.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/ukizhake/miniconda3/lib/python3.11/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain==0.0.304) (1.0.0)\n",
            "Installing collected packages: langchain\n",
            "  Attempting uninstall: langchain\n",
            "    Found existing installation: langchain 0.0.20\n",
            "    Uninstalling langchain-0.0.20:\n",
            "      Successfully uninstalled langchain-0.0.20\n",
            "Successfully installed langchain-0.0.304\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain==0.0.304"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: langchain in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (0.0.20)\n",
            "Requirement already satisfied: pydantic in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from langchain) (1.10.12)\n",
            "Requirement already satisfied: sqlalchemy in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from langchain) (2.0.19)\n",
            "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from langchain) (1.25.1)\n",
            "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: pyyaml in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pydantic->langchain) (4.7.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->langchain) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->langchain) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->langchain) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->langchain) (2023.7.22)\n"
          ]
        }
      ],
      "source": [
        "!pip3 install langchain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: langchain==0.0.304 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (0.0.304)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from langchain==0.0.304) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from langchain==0.0.304) (2.0.19)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from langchain==0.0.304) (3.8.5)\n",
            "Requirement already satisfied: anyio<4.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from langchain==0.0.304) (3.7.1)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from langchain==0.0.304) (0.5.13)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from langchain==0.0.304) (1.33)\n",
            "Requirement already satisfied: langsmith<0.1.0,>=0.0.38 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from langchain==0.0.304) (0.0.41)\n",
            "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from langchain==0.0.304) (2.8.4)\n",
            "Requirement already satisfied: numpy<2,>=1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from langchain==0.0.304) (1.25.1)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from langchain==0.0.304) (1.10.12)\n",
            "Requirement already satisfied: requests<3,>=2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from langchain==0.0.304) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from langchain==0.0.304) (8.2.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.304) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.304) (3.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.304) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.304) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.304) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.304) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.304) (1.3.1)\n",
            "Requirement already satisfied: idna>=2.8 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from anyio<4.0->langchain==0.0.304) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from anyio<4.0->langchain==0.0.304) (1.3.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.0.304) (3.20.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.0.304) (0.9.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain==0.0.304) (2.4)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pydantic<3,>=1->langchain==0.0.304) (4.7.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests<3,>=2->langchain==0.0.304) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests<3,>=2->langchain==0.0.304) (2023.7.22)\n",
            "Requirement already satisfied: packaging>=17.0 in /Users/ukizhake/Library/Python/3.11/lib/python/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.7,>=0.5.7->langchain==0.0.304) (23.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain==0.0.304) (1.0.0)\n",
            "Name: langchain\n",
            "Version: 0.0.304\n",
            "Summary: Building applications with LLMs through composability\n",
            "Home-page: https://github.com/langchain-ai/langchain\n",
            "Author: \n",
            "Author-email: \n",
            "License: MIT\n",
            "Location: /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages\n",
            "Requires: aiohttp, anyio, dataclasses-json, jsonpatch, langsmith, numexpr, numpy, pydantic, PyYAML, requests, SQLAlchemy, tenacity\n",
            "Required-by: ragas\n",
            "Name: langchain\n",
            "Version: 0.0.304\n",
            "Summary: Building applications with LLMs through composability\n",
            "Home-page: https://github.com/langchain-ai/langchain\n",
            "Author: \n",
            "Author-email: \n",
            "License: MIT\n",
            "Location: /Users/ukizhake/miniconda3/lib/python3.11/site-packages\n",
            "Requires: aiohttp, anyio, dataclasses-json, jsonpatch, langsmith, numexpr, numpy, pydantic, PyYAML, requests, SQLAlchemy, tenacity\n",
            "Required-by: ragas\n"
          ]
        }
      ],
      "source": [
        "!pip3 install langchain==0.0.304\n",
        "!pip3 show langchain\n",
        "!pip show langchain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: chromadb in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (0.4.13)\n",
            "Requirement already satisfied: requests>=2.28 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from chromadb) (2.31.0)\n",
            "Requirement already satisfied: pydantic>=1.9 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from chromadb) (1.10.12)\n",
            "Requirement already satisfied: chroma-hnswlib==0.7.3 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from chromadb) (0.7.3)\n",
            "Requirement already satisfied: fastapi>=0.95.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from chromadb) (0.99.1)\n",
            "Requirement already satisfied: uvicorn[standard]>=0.18.3 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from chromadb) (0.23.2)\n",
            "Requirement already satisfied: posthog>=2.4.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from chromadb) (3.0.2)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from chromadb) (4.7.1)\n",
            "Requirement already satisfied: pulsar-client>=3.1.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from chromadb) (3.3.0)\n",
            "Requirement already satisfied: onnxruntime>=1.14.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from chromadb) (1.16.0)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from chromadb) (0.13.3)\n",
            "Requirement already satisfied: pypika>=0.48.9 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from chromadb) (0.48.9)\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from chromadb) (4.65.0)\n",
            "Requirement already satisfied: overrides>=7.3.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from chromadb) (7.4.0)\n",
            "Requirement already satisfied: importlib-resources in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from chromadb) (6.1.0)\n",
            "Requirement already satisfied: bcrypt>=4.0.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from chromadb) (4.0.1)\n",
            "Requirement already satisfied: typer>=0.9.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from chromadb) (0.9.0)\n",
            "Requirement already satisfied: numpy>=1.22.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from chromadb) (1.25.1)\n",
            "Requirement already satisfied: starlette<0.28.0,>=0.27.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from fastapi>=0.95.2->chromadb) (0.27.0)\n",
            "Requirement already satisfied: coloredlogs in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from onnxruntime>=1.14.1->chromadb) (23.5.26)\n",
            "Requirement already satisfied: packaging in /Users/ukizhake/Library/Python/3.11/lib/python/site-packages (from onnxruntime>=1.14.1->chromadb) (23.1)\n",
            "Requirement already satisfied: protobuf in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from onnxruntime>=1.14.1->chromadb) (4.23.4)\n",
            "Requirement already satisfied: sympy in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from onnxruntime>=1.14.1->chromadb) (1.12)\n",
            "Requirement already satisfied: six>=1.5 in /Users/ukizhake/Library/Python/3.11/lib/python/site-packages (from posthog>=2.4.0->chromadb) (1.16.0)\n",
            "Requirement already satisfied: monotonic>=1.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from posthog>=2.4.0->chromadb) (1.6)\n",
            "Requirement already satisfied: backoff>=1.10.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from posthog>=2.4.0->chromadb) (2.2.1)\n",
            "Requirement already satisfied: python-dateutil>2.1 in /Users/ukizhake/Library/Python/3.11/lib/python/site-packages (from posthog>=2.4.0->chromadb) (2.8.2)\n",
            "Requirement already satisfied: certifi in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pulsar-client>=3.1.0->chromadb) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests>=2.28->chromadb) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests>=2.28->chromadb) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests>=2.28->chromadb) (2.0.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from typer>=0.9.0->chromadb) (8.1.5)\n",
            "Requirement already satisfied: h11>=0.8 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.14.0)\n",
            "Requirement already satisfied: httptools>=0.5.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.0)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (6.0.1)\n",
            "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.17.0)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.20.0)\n",
            "Requirement already satisfied: websockets>=10.4 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (11.0.3)\n",
            "Requirement already satisfied: anyio<5,>=3.4.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from starlette<0.28.0,>=0.27.0->fastapi>=0.95.2->chromadb) (3.7.1)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from anyio<5,>=3.4.0->starlette<0.28.0,>=0.27.0->fastapi>=0.95.2->chromadb) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip3 install chromadb\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement ParentDocumentRetriever (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for ParentDocumentRetriever\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip3 install ParentDocumentRetriever"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {
        "id": "67I6QJAJ0Un7"
      },
      "outputs": [
        {
          "ename": "ImportError",
          "evalue": "cannot import name 'ParentDocumentRetriever' from 'langchain.retrievers' (unknown location)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[1;32m/Users/ukizhake/Documents/LLM-Ops-Cohort-2-main-week2-thurs/Week 2/Thursday - Evaluating Advanced Retrieval Methods in LangChain with RAGAS/Evaluating_Advanced_Retrieval_Methods_in_LangChain_with_RAGAS (Assignment Version).ipynb Cell 56\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/ukizhake/Documents/LLM-Ops-Cohort-2-main-week2-thurs/Week%202/Thursday%20-%20Evaluating%20Advanced%20Retrieval%20Methods%20in%20LangChain%20with%20RAGAS/Evaluating_Advanced_Retrieval_Methods_in_LangChain_with_RAGAS%20%28Assignment%20Version%29.ipynb#X61sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mretrievers\u001b[39;00m \u001b[39mimport\u001b[39;00m ParentDocumentRetriever\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ukizhake/Documents/LLM-Ops-Cohort-2-main-week2-thurs/Week%202/Thursday%20-%20Evaluating%20Advanced%20Retrieval%20Methods%20in%20LangChain%20with%20RAGAS/Evaluating_Advanced_Retrieval_Methods_in_LangChain_with_RAGAS%20%28Assignment%20Version%29.ipynb#X61sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mstorage\u001b[39;00m \u001b[39mimport\u001b[39;00m InMemoryStore\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ukizhake/Documents/LLM-Ops-Cohort-2-main-week2-thurs/Week%202/Thursday%20-%20Evaluating%20Advanced%20Retrieval%20Methods%20in%20LangChain%20with%20RAGAS/Evaluating_Advanced_Retrieval_Methods_in_LangChain_with_RAGAS%20%28Assignment%20Version%29.ipynb#X61sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdocument_loaders\u001b[39;00m \u001b[39mimport\u001b[39;00m TextLoader\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'ParentDocumentRetriever' from 'langchain.retrievers' (unknown location)"
          ]
        }
      ],
      "source": [
        "from langchain.retrievers import ParentDocumentRetriever\n",
        "from langchain.storage import InMemoryStore\n",
        "from langchain.document_loaders import TextLoader\n",
        "\n",
        "\n",
        "parent_splitter = RecursiveCharacterTextSplitter(chunk_size=2000)### YOUR CODE HERE\n",
        "child_splitter = RecursiveCharacterTextSplitter(chunk_size=400)### YOUR CODE HERE\n",
        "\n",
        "vectorstore = Chroma(\n",
        "    collection_name=\"split_parents\", \n",
        "    embedding_function=OpenAIEmbeddings()### YOUR CODE HERE\n",
        ")\n",
        "\n",
        "store = InMemoryStore()### YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting langchain==0.0.20\n",
            "  Downloading langchain-0.0.20-py3-none-any.whl (101 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.9/101.9 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from langchain==0.0.20) (1.10.12)\n",
            "Requirement already satisfied: sqlalchemy in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from langchain==0.0.20) (2.0.19)\n",
            "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from langchain==0.0.20) (1.25.1)\n",
            "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from langchain==0.0.20) (2.31.0)\n",
            "Requirement already satisfied: pyyaml in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from langchain==0.0.20) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pydantic->langchain==0.0.20) (4.7.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->langchain==0.0.20) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->langchain==0.0.20) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->langchain==0.0.20) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->langchain==0.0.20) (2023.7.22)\n",
            "Installing collected packages: langchain\n",
            "  Attempting uninstall: langchain\n",
            "    Found existing installation: langchain 0.0.304\n",
            "    Uninstalling langchain-0.0.304:\n",
            "      Successfully uninstalled langchain-0.0.304\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ragas 0.0.16 requires langchain>=0.0.288, but you have langchain 0.0.20 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed langchain-0.0.20\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {
        "id": "zfk5RYUt00Pw"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'ParentDocumentRetriever' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m/Users/ukizhake/Documents/LLM-Ops-Cohort-2-main-week2-thurs/Week 2/Thursday - Evaluating Advanced Retrieval Methods in LangChain with RAGAS/Evaluating_Advanced_Retrieval_Methods_in_LangChain_with_RAGAS (Assignment Version).ipynb Cell 55\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/ukizhake/Documents/LLM-Ops-Cohort-2-main-week2-thurs/Week%202/Thursday%20-%20Evaluating%20Advanced%20Retrieval%20Methods%20in%20LangChain%20with%20RAGAS/Evaluating_Advanced_Retrieval_Methods_in_LangChain_with_RAGAS%20%28Assignment%20Version%29.ipynb#X62sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m parent_document_retriever \u001b[39m=\u001b[39m ParentDocumentRetriever(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ukizhake/Documents/LLM-Ops-Cohort-2-main-week2-thurs/Week%202/Thursday%20-%20Evaluating%20Advanced%20Retrieval%20Methods%20in%20LangChain%20with%20RAGAS/Evaluating_Advanced_Retrieval_Methods_in_LangChain_with_RAGAS%20%28Assignment%20Version%29.ipynb#X62sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     vectorstore\u001b[39m=\u001b[39mvectorstore,\u001b[39m### YOUR CODE HERE\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ukizhake/Documents/LLM-Ops-Cohort-2-main-week2-thurs/Week%202/Thursday%20-%20Evaluating%20Advanced%20Retrieval%20Methods%20in%20LangChain%20with%20RAGAS/Evaluating_Advanced_Retrieval_Methods_in_LangChain_with_RAGAS%20%28Assignment%20Version%29.ipynb#X62sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     docstore\u001b[39m=\u001b[39mbase_docs,\u001b[39m### YOUR CODE HERE\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ukizhake/Documents/LLM-Ops-Cohort-2-main-week2-thurs/Week%202/Thursday%20-%20Evaluating%20Advanced%20Retrieval%20Methods%20in%20LangChain%20with%20RAGAS/Evaluating_Advanced_Retrieval_Methods_in_LangChain_with_RAGAS%20%28Assignment%20Version%29.ipynb#X62sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     child_splitter\u001b[39m=\u001b[39mchild_splitter,\u001b[39m### YOUR CODE HERE\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ukizhake/Documents/LLM-Ops-Cohort-2-main-week2-thurs/Week%202/Thursday%20-%20Evaluating%20Advanced%20Retrieval%20Methods%20in%20LangChain%20with%20RAGAS/Evaluating_Advanced_Retrieval_Methods_in_LangChain_with_RAGAS%20%28Assignment%20Version%29.ipynb#X62sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     parent_splitter\u001b[39m=\u001b[39mparent_splitter\u001b[39m### YOUR CODE HERE\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ukizhake/Documents/LLM-Ops-Cohort-2-main-week2-thurs/Week%202/Thursday%20-%20Evaluating%20Advanced%20Retrieval%20Methods%20in%20LangChain%20with%20RAGAS/Evaluating_Advanced_Retrieval_Methods_in_LangChain_with_RAGAS%20%28Assignment%20Version%29.ipynb#X62sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m )\n",
            "\u001b[0;31mNameError\u001b[0m: name 'ParentDocumentRetriever' is not defined"
          ]
        }
      ],
      "source": [
        "parent_document_retriever = ParentDocumentRetriever(\n",
        "    vectorstore=vectorstore,### YOUR CODE HERE\n",
        "    docstore=base_docs,### YOUR CODE HERE\n",
        "    child_splitter=child_splitter,### YOUR CODE HERE\n",
        "    parent_splitter=parent_splitter### YOUR CODE HERE\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "68c1t4o104AK"
      },
      "outputs": [],
      "source": [
        "parent_document_retriever.add_documents(base_docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTH0MDolBndm"
      },
      "source": [
        "Let's create, test, and then evaluate our new chain!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KMjLfqOC09Iw"
      },
      "outputs": [],
      "source": [
        "parent_document_retriever_qa_chain = create_qa_chain(\n",
        "    ### YOUR CODE HERE\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "Rv8bAHPN1H4P",
        "outputId": "be9bbf18-b0e6-4a29-9f08-34cd4aa5fb6a"
      },
      "outputs": [],
      "source": [
        "parent_document_retriever_qa_chain({\"query\" : \"What is RAG?\"})[\"result\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OQJRIQmU1WTw",
        "outputId": "6d51a5d2-2c1c-4604-8030-43f432c4aa84"
      },
      "outputs": [],
      "source": [
        "pdr_qa_ragas_dataset = create_ragas_dataset(\n",
        "    ### YOUR CODE HERE\n",
        "    ### YOUR CODE HERE\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "467c2b45d8dd4a67a229d66ff2fc9bc7",
            "293bbbe3717c4c0899792535eaff8acf",
            "c523276907e648e2ba79ac03be49576c",
            "c4f448cab4ec493d86f279f7207a5ce8",
            "a78ccb267c854f3cb093a132ff8256d9",
            "df9133aad3b047aaa69b7d1da7eb19a9",
            "bc674f2c0ec84cecb99bed69ff80c0bf",
            "84b05599d12849dba867dd219d36d212",
            "a93c4c0ec57646f1b84355a16f8ba947",
            "1538f80d733844e0a5f6c71ae2bbf948",
            "a4d900bf76d14cbd8da6df656ec21fa0"
          ]
        },
        "id": "d9vfKnCL1jtB",
        "outputId": "5f67fdf4-20fb-4d7a-cde2-07caf6f6cce1"
      },
      "outputs": [],
      "source": [
        "pdr_qa_ragas_dataset.to_csv(\n",
        "    ### YOUR CODE HERE\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qfB1H9S_1mW3",
        "outputId": "453628e0-10f4-4aa2-bc73-86552d400806"
      },
      "outputs": [],
      "source": [
        "pdr_qa_result = evaluate_ragas_dataset(\n",
        "    ### YOUR CODE HERE\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5nFyYCdL2Nco",
        "outputId": "707a8e3e-c808-4f55-dd94-2279a168da5b"
      },
      "outputs": [],
      "source": [
        "pdr_qa_result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JaNk6o7_BqX8"
      },
      "source": [
        "#### Ensemble Retrieval\n",
        "\n",
        "There are a number of excellent options to retrieve documents - we'll be looking at an additional example today, which is called the EnsembleRetriever.\n",
        "\n",
        "The method this is using is outlined in [this paper](https://plg.uwaterloo.ca/~gvcormac/cormacksigir09-rrf.pdf).\n",
        "\n",
        "The brief explanation is:\n",
        "\n",
        "- We collect results from two different retrieval methods over the same corpus\n",
        "- We apply a reranking algorithm to rerank our source documents to be the most relevant without losing specific or potentially low-ranked information rich documents\n",
        "- We feed the top-k results into the LLM with our query as context.\n",
        "\n",
        "> HINT: Your weight list should be of type List[float] and the sum(List[float]) should be 1.\n",
        "\n",
        "We'll be leveraging the following tools:\n",
        "\n",
        "- [`BM25Retriever`](https://api.python.langchain.com/en/latest/retrievers/langchain.retrievers.bm25.BM25Retriever.html)\n",
        "- [`EnsembleRetriever`](https://api.python.langchain.com/en/latest/retrievers/langchain.retrievers.ensemble.EnsembleRetriever.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### High Level Diagram\n",
        "\n",
        "Leverages the [RRF](https://plg.uwaterloo.ca/~gvcormac/cormacksigir09-rrf.pdf) reranking algorithm to combine sparse and dense search results for increased effectiveness for relevant document retrieval.\n",
        "\n",
        "![image](https://i.imgur.com/mn4jXAz.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zz7dl1GD5-L-"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U rank_bm25"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vs8wxT9b5pRA"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
        "\n",
        "text_splitter = ### YOUR CODE HERE\n",
        "docs = text_splitter.split_documents(\n",
        "    ### YOUR CODE HERE\n",
        ")\n",
        "\n",
        "bm25_retriever = BM25Retriever.from_documents(\n",
        "    ### YOUR CODE HERE\n",
        ")\n",
        "bm25_retriever.k = ### YOUR CODE HERE\n",
        "\n",
        "embedding = ### YOUR CODE HERE\n",
        "vectorstore = Chroma.from_documents(\n",
        "    ### YOUR CODE HERE\n",
        "    ### YOUR CODE HERE\n",
        ")\n",
        "chroma_retriever = vectorstore.as_retriever(\n",
        "    ### YOUR CODE HERE\n",
        ")\n",
        "\n",
        "ensemble_retriever = EnsembleRetriever(\n",
        "    ### YOUR CODE HERE\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cv69YDpF6PrJ"
      },
      "outputs": [],
      "source": [
        "ensemble_retriever_qa_chain = create_qa_chain(ensemble_retriever)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "6lSszzrf6UmP",
        "outputId": "a72a74bf-be4d-492e-ebeb-bb6ee619cf53"
      },
      "outputs": [],
      "source": [
        "ensemble_retriever_qa_chain({\"query\" : \"What is RAG?\"})[\"result\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "abUgTGDT6UrV",
        "outputId": "41eb67ab-06c4-495a-c401-7b0fc909e705"
      },
      "outputs": [],
      "source": [
        "ensemble_qa_ragas_dataset = create_ragas_dataset(ensemble_retriever_qa_chain, eval_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "f207234b51b647fd8219b5ffc9c39bd6",
            "971e27539ecb42718f3c31bf14d855fc",
            "6647629812ba4652b6b6c8f3400cb79f",
            "3e638938eb934c8db1f970675eb4a0d7",
            "6d228f73d99247048af148f154c7fed3",
            "5476ebd8683747e387ef875cfeed2ee5",
            "59f0fe93f0e5478098ababace0670e80",
            "a5d5c3a0cbd14e97aed63260d032c41f",
            "394d86f3b82e414193e96d49da77a0d5",
            "8ddc0f7dfdb24fc6932dfc589cf056d9",
            "91b8e7e80ffb4897983b607f3165033c"
          ]
        },
        "id": "bGHipYsf7phk",
        "outputId": "c890d308-6b66-4f01-9919-4e3970283321"
      },
      "outputs": [],
      "source": [
        "ensemble_qa_ragas_dataset.to_csv(\"ensemble_qa_ragas_dataset.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ozo0jkvx7r1d",
        "outputId": "6356da39-9983-4f12-f8ef-9fce34aa9573"
      },
      "outputs": [],
      "source": [
        "ensemble_qa_result = evaluate_ragas_dataset(ensemble_qa_ragas_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hvabdcbh793a",
        "outputId": "40263758-cb97-42c0-864a-ab9d37e8bd92"
      },
      "outputs": [],
      "source": [
        "ensemble_qa_result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O4vXVWqiCcSI"
      },
      "source": [
        "### Conclusion\n",
        "\n",
        "Observe your results in a table!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g0Tu_GM_ESC5"
      },
      "outputs": [],
      "source": [
        "### YOUR CODE HERE"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1538f80d733844e0a5f6c71ae2bbf948": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "18dbdffae47e407da2824cccb96bb27b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "21238add60ea411e937a445c154efaa7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "285688f9e00548388c923b0bc2bc38fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4316578e814246358ad08c55d80e44df",
            "placeholder": "​",
            "style": "IPY_MODEL_9ddeea42439a49fd9fe2c310e8facb07",
            "value": "Creating CSV from Arrow format: 100%"
          }
        },
        "293bbbe3717c4c0899792535eaff8acf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_df9133aad3b047aaa69b7d1da7eb19a9",
            "placeholder": "​",
            "style": "IPY_MODEL_bc674f2c0ec84cecb99bed69ff80c0bf",
            "value": "Creating CSV from Arrow format: 100%"
          }
        },
        "349dbc81cb3843caba834cd940e4d41c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "36183c3078ac4be8a0fd48de8c00ff09": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "394d86f3b82e414193e96d49da77a0d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3ce331032ea44ea2add34ae1c04002c5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3e638938eb934c8db1f970675eb4a0d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8ddc0f7dfdb24fc6932dfc589cf056d9",
            "placeholder": "​",
            "style": "IPY_MODEL_91b8e7e80ffb4897983b607f3165033c",
            "value": " 1/1 [00:00&lt;00:00, 22.85ba/s]"
          }
        },
        "4316578e814246358ad08c55d80e44df": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "467c2b45d8dd4a67a229d66ff2fc9bc7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_293bbbe3717c4c0899792535eaff8acf",
              "IPY_MODEL_c523276907e648e2ba79ac03be49576c",
              "IPY_MODEL_c4f448cab4ec493d86f279f7207a5ce8"
            ],
            "layout": "IPY_MODEL_a78ccb267c854f3cb093a132ff8256d9"
          }
        },
        "4fc6ddfce74a4957bdb03822d9130cb9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5476ebd8683747e387ef875cfeed2ee5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "59f0fe93f0e5478098ababace0670e80": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5d6474260ff648adbadf28e8e0839fa2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6647629812ba4652b6b6c8f3400cb79f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a5d5c3a0cbd14e97aed63260d032c41f",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_394d86f3b82e414193e96d49da77a0d5",
            "value": 1
          }
        },
        "6b55cf5bf0d14e7ba1384b1faad82f62": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6d228f73d99247048af148f154c7fed3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "700f1814c1a14134b2d12a54d018416d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ad146de916d1474ca3bf252d1c68ca6f",
            "placeholder": "​",
            "style": "IPY_MODEL_6b55cf5bf0d14e7ba1384b1faad82f62",
            "value": "Creating CSV from Arrow format: 100%"
          }
        },
        "79e167d43da549c292c2573c782581b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_700f1814c1a14134b2d12a54d018416d",
              "IPY_MODEL_eeb1186495bb4580af9e99ba60c3ddd7",
              "IPY_MODEL_9362cbc2937c40eb821d05afd7cdef77"
            ],
            "layout": "IPY_MODEL_4fc6ddfce74a4957bdb03822d9130cb9"
          }
        },
        "7f925c5a99194d4daec387f3847337b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ff4ca4f11c3843809b0fb2c194b58327",
            "placeholder": "​",
            "style": "IPY_MODEL_8d4ac912eaf94f6c8e013977133a4906",
            "value": " 1/1 [00:00&lt;00:00, 19.62ba/s]"
          }
        },
        "84b05599d12849dba867dd219d36d212": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8d4ac912eaf94f6c8e013977133a4906": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8ddc0f7dfdb24fc6932dfc589cf056d9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8df3b60cdb35424da81299c607e17d0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5d6474260ff648adbadf28e8e0839fa2",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_db3e3d58eeda4567964682c366157086",
            "value": 1
          }
        },
        "91b8e7e80ffb4897983b607f3165033c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9362cbc2937c40eb821d05afd7cdef77": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3ce331032ea44ea2add34ae1c04002c5",
            "placeholder": "​",
            "style": "IPY_MODEL_18dbdffae47e407da2824cccb96bb27b",
            "value": " 1/1 [00:00&lt;00:00, 20.82ba/s]"
          }
        },
        "9459643d14994e269ec3897e5b071029": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_285688f9e00548388c923b0bc2bc38fd",
              "IPY_MODEL_8df3b60cdb35424da81299c607e17d0f",
              "IPY_MODEL_7f925c5a99194d4daec387f3847337b8"
            ],
            "layout": "IPY_MODEL_349dbc81cb3843caba834cd940e4d41c"
          }
        },
        "971e27539ecb42718f3c31bf14d855fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5476ebd8683747e387ef875cfeed2ee5",
            "placeholder": "​",
            "style": "IPY_MODEL_59f0fe93f0e5478098ababace0670e80",
            "value": "Creating CSV from Arrow format: 100%"
          }
        },
        "9ddeea42439a49fd9fe2c310e8facb07": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a4d900bf76d14cbd8da6df656ec21fa0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a5d5c3a0cbd14e97aed63260d032c41f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a78ccb267c854f3cb093a132ff8256d9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a93c4c0ec57646f1b84355a16f8ba947": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ad146de916d1474ca3bf252d1c68ca6f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bc674f2c0ec84cecb99bed69ff80c0bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c4f448cab4ec493d86f279f7207a5ce8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1538f80d733844e0a5f6c71ae2bbf948",
            "placeholder": "​",
            "style": "IPY_MODEL_a4d900bf76d14cbd8da6df656ec21fa0",
            "value": " 1/1 [00:00&lt;00:00, 27.01ba/s]"
          }
        },
        "c523276907e648e2ba79ac03be49576c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_84b05599d12849dba867dd219d36d212",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a93c4c0ec57646f1b84355a16f8ba947",
            "value": 1
          }
        },
        "db3e3d58eeda4567964682c366157086": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "df9133aad3b047aaa69b7d1da7eb19a9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eeb1186495bb4580af9e99ba60c3ddd7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_36183c3078ac4be8a0fd48de8c00ff09",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_21238add60ea411e937a445c154efaa7",
            "value": 1
          }
        },
        "f207234b51b647fd8219b5ffc9c39bd6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_971e27539ecb42718f3c31bf14d855fc",
              "IPY_MODEL_6647629812ba4652b6b6c8f3400cb79f",
              "IPY_MODEL_3e638938eb934c8db1f970675eb4a0d7"
            ],
            "layout": "IPY_MODEL_6d228f73d99247048af148f154c7fed3"
          }
        },
        "ff4ca4f11c3843809b0fb2c194b58327": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

question,context,ground_truth,metadata
What are the advantages of retrieval-augmented text generation compared to conventional generation models?,"A Survey on Retrieval-Augmented Text Generation
Huayang Li♥,∗
Yixuan Su♠,∗
Deng Cai♦,∗
Yan Wang♣,∗
Lemao Liu♣,∗
♥Nara Institute of Science and Technology
♠University of Cambridge
♦The Chinese University of Hong Kong
♣Tencent AI Lab
li.huayang.lh6@is.naist.jp, ys484@cam.ac.uk
thisisjcykcd@gmail.com, brandenwang@tencent.com
lemaoliu@gmail.com
Abstract
Recently, retrieval-augmented text generation
attracted increasing attention of the compu-
tational linguistics community.
Compared
with conventional generation models, retrieval-
augmented text generation has remarkable ad-
vantages and particularly has achieved state-of-
the-art performance in many NLP tasks. This
paper aims to conduct a survey about retrieval-
augmented text generation. It ﬁrstly highlights
the generic paradigm of retrieval-augmented
generation, and then it reviews notable ap-
proaches according to different tasks including
dialogue response generation, machine trans-
lation, and other generation tasks. Finally, it","Retrieval-augmented text generation has remarkable advantages over conventional generation models, including achieving state-of-the-art performance in many NLP tasks. It is particularly effective in tasks such as dialogue response generation, machine translation, and other generation tasks.",
What are the advantages of retrieval-augmented text generation compared to generation-based models?,"dialogue response generation, machine trans-
lation, and other generation tasks. Finally, it
points out some promising directions on top of
recent methods to facilitate future research.
1
Introduction
Retrieval-augmented text generation, as a new
text generation paradigm that fuses emerging deep
learning technology and traditional retrieval tech-
nology, has achieved state-of-the-art (SOTA) per-
formance in many NLP tasks and attracted the at-
tention of the computational linguistics community
(Weston et al., 2018; Dinan et al., 2018; Cai et al.,
2021). Compared with generation-based counter-
part, this new paradigm has some remarkable ad-
vantages: 1) The knowledge is not necessary to be
implicitly stored in model parameters, but is explic-
itly acquired in a plug-and-play manner, leading
to great scalibility; 2) Instead of generating from
scratch, the paradigm generating text from some re-
trieved human-written reference, which potentially","The advantages of retrieval-augmented text generation compared to generation-based models include: 1) The knowledge is not necessary to be implicitly stored in model parameters, but is explicitly acquired in a plug-and-play manner, leading to great scalability; 2) Instead of generating from scratch, the paradigm generates text from some retrieved human-written reference, which potentially enhances the quality of the generated text.",
What are the three key components of retrieval-augmented generation?,"trieved human-written reference, which potentially
alleviates the difﬁculty of text generation.
This paper aims to review many representative
approaches for retrieval-augmented text generation
tasks including dialogue response generation (We-
ston et al., 2018), machine translation (Gu et al.,
2018) and others (Hashimoto et al., 2018). We
∗All authors contributed equally.
ﬁrstly present the generic paradigm of retrieval-
augmented generation as well as three key com-
ponents under this paradigm, which are retrieval
sources, retrieval metrics and generation models.
Then, we introduce notable methods about
retrieval-augmented generation, which are orga-
nized with respect to different tasks. Speciﬁcally,
on the dialogue response generation task, exem-
plar/template retrieval as an intermediate step has
been shown beneﬁcial to informative response gen-
eration (Weston et al., 2018; Wu et al., 2019; Cai
et al., 2019a,b). In addition, there has been growing","The three key components of retrieval-augmented generation are retrieval sources, retrieval metrics, and generation models.",
What are some notable approaches to retrieval-augmented text generation in machine translation?,"et al., 2019a,b). In addition, there has been growing
interest in knowledge-grounded generation explor-
ing different forms of knowledge such as knowl-
edge bases and external documents (Dinan et al.,
2018; Zhou et al., 2018; Lian et al., 2019; Li et al.,
2019; Qin et al., 2019; Wu et al., 2021; Zhang et al.,
2021). On the machine translation task, we summa-
rize the early work on how the retrieved sentences
(called translation memory) are used to improve
statistical machine translation (SMT) (Koehn et al.,
2003) models (Simard and Isabelle, 2009; Koehn
and Senellart, 2010) and in particular, we inten-
sively highlight several popular methods to inte-
grating translation memory to NMT models (Gu
et al., 2018; Zhang et al., 2018; Xu et al., 2020;
He et al., 2021). We also review the applications
of retrieval-augmented generation in other genera-
tion tasks such as abstractive summarization (Peng
et al., 2019), code generation (Hashimoto et al.,","Some notable approaches to retrieval-augmented text generation in machine translation include the use of retrieved sentences, also known as translation memory, to improve statistical machine translation (SMT) models as demonstrated by Koehn et al., 2003. This method was further developed by Simard and Isabelle, 2009; Koehn and Senellart, 2010. Additionally, several popular methods of integrating translation memory to Neural Machine Translation (NMT) models have been highlighted, such as those by Gu et al., 2018; Zhang et al., 2018; Xu et al., 2020; He et al., 2021. Retrieval-augmented generation has also been applied to other tasks like abstractive summarization and code generation.",
What are the three major components of the retrieval-augmented generation paradigm?,"et al., 2019), code generation (Hashimoto et al.,
2018), paraphrase (Kazemnejad et al., 2020; Su
et al., 2021b), and knowledge-intensive generation
(Lewis et al., 2020b). Finally, we also point out
some promising directions on retrieval-augmented
generation to push forward the future research.
2
Retrieval-Augmented Paradigm
In this section, we ﬁrst give a general formulation
of retrieval-augmented text generation. Then, we
discuss three major components of the retrieval-
augmented generation paradigm, including the re-
arXiv:2202.01110v2  [cs.CL]  13 Feb 2022
Input
Sources 
(Sec. 2.2):
Training 
Corpus
External Data
Unsupervised 
Data
Metrics
(Sec. 2.3):
Sparse-vector 
Retrieval
Dense-vector 
Retrieval
Task-specific 
Retrieval
Retrieval Memory
Generation Model
Sec. 4: Machine 
Translation
Sec. 5: Other 
Tasks
Data 
Augmentation
Attention 
Mechanism
Skeleton & 
Templates
Information Retrieval
Tasks:
Sec. 3: Dialogue 
Generation
Models 
(Sec 2.4):
Output","The three major components of the retrieval-augmented generation paradigm are Input Sources, Metrics, and Models.",
What is the formulation for most text generation tasks?,"Templates
Information Retrieval
Tasks:
Sec. 3: Dialogue 
Generation
Models 
(Sec 2.4):
Output
Figure 1: The overview of this survey.
trieval source, retrieval metric and integration meth-
ods.
2.1
Formulation
Most text generation tasks can be formulated as a
mapping from input sequence x to output sequence
y : y = f(x). For instance, x and y could be the
dialogue history and the corresponding response
for dialogue response generation, the text in the
source language and the translation in the target
language for machine translation, and so on.
Recently, some researchers suggest to endow
models the capability to access external memory
via some information retrieval techniques, so that
they can acquire more information in the generation
process (Gu et al., 2018; Weston et al., 2018; Cai
et al., 2019b). The retrieval-augmented generation
can be further formulated as:
y = f(x, z)
(1)
where z = {⟨xr, yr⟩} is a set of relevant instances
retrieved from the original training set or external","Most text generation tasks can be formulated as a mapping from input sequence x to output sequence y : y = f(x). For instance, x and y could be the dialogue history and the corresponding response for dialogue response generation, the text in the source language and the translation in the target language for machine translation, and so on. Recently, some researchers suggest to endow models the capability to access external memory via some information retrieval techniques, so that they can acquire more information in the generation process. The retrieval-augmented generation can be further formulated as: y = f(x, z) where z = {⟨xr, yr⟩} is a set of relevant instances retrieved from the original training set or external.","{'Authors': 'Huayang Li, Yixuan Su, Deng Cai, Yan Wang, Lemao Liu', 'Published': '2022-02-13', 'Summary': 'Recently, retrieval-augmented text generation attracted increasing attention\nof the computational linguistics community. Compared with conventional\ngeneration models, retrieval-augmented text generation has remarkable\nadvantages and particularly has achieved state-of-the-art performance in many\nNLP tasks. This paper aims to conduct a survey about retrieval-augmented text\ngeneration. It firstly highlights the generic paradigm of retrieval-augmented\ngeneration, and then it reviews notable approaches according to different tasks\nincluding dialogue response generation, machine translation, and other\ngeneration tasks. Finally, it points out some important directions on top of\nrecent methods to facilitate future research.', 'Title': 'A Survey on Retrieval-Augmented Text Generation'}"
What are the three kinds of sources from which retrieval memory can be retrieved?,"retrieved from the original training set or external
datasets. The main idea of this paradigm is that yr
may beneﬁt the response generation, if xr (or yr)
is similar (or relevant) to the input x. It is worth
noting that xr = ∅ when unsupervised retrieval
sources are used. In general, the retrieval mem-
ory can be retrieved from three kinds of sources:
the training corpus, external datasets in the same
format with the training corpus, and large-scale
unsupervised corpus (§2.2). Metrics that evaluate
the relevance between text are varied as well, in
§2.3 we divided them into three categories: sparse-
vector retrieval, dense-vector retrieval, and training-
based retrieval. Finally, how to integrate the re-
trieval memory to the generation model is also sig-
niﬁcant, we also introduce some popular integra-
tion approaches in §2.4.
2.2
Retrieval Sources
Training Corpus
Most previous studies search
the external memory from its training corpus (Song","The three kinds of sources from which retrieval memory can be retrieved are the training corpus, external datasets in the same format with the training corpus, and large-scale unsupervised corpus.",
What is the main motivation behind storing knowledge in an explicit and accessible form during inference?,"Training Corpus
Most previous studies search
the external memory from its training corpus (Song
et al., 2016; Gu et al., 2018; Weston et al., 2018).
In the inference time, retrieved examples with high
relevant scores could be regarded as extra refer-
ences and reduce model’s uncertainty in generation.
The main motivation of those works is to to store
knowledge not only in the model parameters but
also in an explicit and accessible form, making the
model be able to re-access it during inference.
External Data
Some researchers also propose to
retrieval relevant samples from external datasets
(Su et al., 2021c; Xiao et al., 2021). In these stud-
ies, the retrieval pool is different with the training
corpus, which can further provide additional infor-
mation that are not contained in the training corpus.
This is especially beneﬁcial for applications such
as domain adaptation and knowledge update. For
example, Khandelwal et al. (2020a); Zheng et al.",The main motivation behind storing knowledge in an explicit and accessible form during inference is to not only store knowledge in the model parameters but also make the model be able to re-access it during inference. This can reduce the model's uncertainty in generation by providing extra references with high relevant scores retrieved from the training corpus.,
What is the main idea behind the cross-lingual retriever proposed by Cai et al. (2021) for machine translation?,"as domain adaptation and knowledge update. For
example, Khandelwal et al. (2020a); Zheng et al.
(2021a) employ the in-domain dataset as the exter-
nal memory to achieve fast domain adaptation for
machine translation.
Unsupervised Data
One limitation for previous
two sources is that the datasets have to be super-
vised datasets consisting of aligned input-output
pairs. For machine translation, Cai et al. (2021) pro-
pose a cross-lingual retriever to directly retrieve tar-
get sentence from unsupervised corpus (i.e., mono-
lingual corpus in the target language). The main
idea is aligning source-side sentences and the corre-
sponding target-side translations in a dense vector
space, i.e., aligning x and yr when xr is absent.
As a result, the retriever directly connects the dots
between the source-side input and target-side trans-
lations, enabling monolingual data in the target
language to be used alone as memories.
2.3
Retrieval Metrics
Sparse-vector Retrieval
Given an input se-","The main idea behind the cross-lingual retriever proposed by Cai et al. (2021) for machine translation is to directly retrieve the target sentence from an unsupervised corpus, which is a monolingual corpus in the target language. This is achieved by aligning source-side sentences and the corresponding target-side translations in a dense vector space, even when the source-side reference is absent. This method allows the retriever to directly connect the source-side input and target-side translations, enabling the use of monolingual data in the target language alone as memories.","{'Authors': 'Huayang Li, Yixuan Su, Deng Cai, Yan Wang, Lemao Liu', 'Published': '2022-02-13', 'Summary': 'Recently, retrieval-augmented text generation attracted increasing attention\nof the computational linguistics community. Compared with conventional\ngeneration models, retrieval-augmented text generation has remarkable\nadvantages and particularly has achieved state-of-the-art performance in many\nNLP tasks. This paper aims to conduct a survey about retrieval-augmented text\ngeneration. It firstly highlights the generic paradigm of retrieval-augmented\ngeneration, and then it reviews notable approaches according to different tasks\nincluding dialogue response generation, machine translation, and other\ngeneration tasks. Finally, it points out some important directions on top of\nrecent methods to facilitate future research.', 'Title': 'A Survey on Retrieval-Augmented Text Generation'}"
What are some widely used sparse-vector retrieval methods?,"2.3
Retrieval Metrics
Sparse-vector Retrieval
Given an input se-
quence x and a retrieval corpus, retrieval model
aims to retrieve a set of relevant examples z =
{⟨xr, yr⟩} from the corpus. When a supervised
corpus is used, {⟨xr, yr⟩} is retrieved by measur-
ing the similarity between x and xr. For simi-
larity measurement, sparse-vector retrieval meth-
ods such as TF-IDF and BM25 (Robertson and
Zaragoza, 2009) are widely used. They match key-
words efﬁciently with an inverted index.
Dense-vector Retrieval
However, these meth-
ods prefer examples with similar surfaces, and may
fail to retrieve examples that are only semantically
relevant. To alleviate above problem, some stud-
ies (Cao and Xiong, 2018) attempt to retrieve in
dense-vector space instead of the lexical overlap.
Recent work (Lee et al., 2019) makes use of pre-
trained language models, which encodes the text to
low-dimensional dense vectors via BERT-based en-
coders. The retrieval score are computed via inner",TF-IDF and BM25 are some widely used sparse-vector retrieval methods.,

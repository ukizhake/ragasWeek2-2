question,answer,contexts,ground_truths
What are the advantages of retrieval-augmented text generation compared to generation-based models?,"The advantages of retrieval-augmented text generation compared to generation-based models are:

1) Scalability: Retrieval-augmented text generation allows for explicit acquisition of knowledge in a plug-and-play manner, rather than implicitly storing it in model parameters. This leads to greater scalability.

2) Ease of text generation: Instead of generating text from scratch, retrieval-augmented text generation generates text from retrieved human-written references. This potentially alleviates the difficulty of text generation.

Overall, retrieval-augmented text generation has shown remarkable advantages and has achieved state-of-the-art performance in many NLP tasks.","['A Survey on Retrieval-Augmented Text Generation\nHuayang Li♥,∗\nYixuan Su♠,∗\nDeng Cai♦,∗\nYan Wang♣,∗\nLemao Liu♣,∗\n♥Nara Institute of Science and Technology\n♠University of Cambridge\n♦The Chinese University of Hong Kong\n♣Tencent AI Lab\nli.huayang.lh6@is.naist.jp, ys484@cam.ac.uk\nthisisjcykcd@gmail.com, brandenwang@tencent.com\nlemaoliu@gmail.com\nAbstract\nRecently, retrieval-augmented text generation\nattracted increasing attention of the compu-\ntational linguistics community.\nCompared\nwith conventional generation models, retrieval-\naugmented text generation has remarkable ad-\nvantages and particularly has achieved state-of-\nthe-art performance in many NLP tasks. This\npaper aims to conduct a survey about retrieval-\naugmented text generation. It ﬁrstly highlights\nthe generic paradigm of retrieval-augmented\ngeneration, and then it reviews notable ap-\nproaches according to different tasks including\ndialogue response generation, machine trans-\nlation, and other generation tasks. Finally, it\npoints out some promising directions on top of\nrecent methods to facilitate future research.\n1\nIntroduction\nRetrieval-augmented text generation, as a new\ntext generation paradigm that fuses emerging deep\nlearning technology and traditional retrieval tech-\nnology, has achieved state-of-the-art (SOTA) per-\nformance in many NLP tasks and attracted the at-\ntention of the computational linguistics community\n(Weston et al., 2018; Dinan et al., 2018; Cai et al.,\n2021). Compared with generation-based counter-\npart, this new paradigm has some remarkable ad-\nvantages: 1) The knowledge is not necessary to be\nimplicitly stored in model parameters, but is explic-\nitly acquired in a plug-and-play manner, leading\nto great scalibility; 2) Instead of generating from\nscratch, the paradigm generating text from some re-\ntrieved human-written reference, which potentially\nalleviates the difﬁculty of text generation.\nThis paper aims to review many representative\napproaches for retrieval-augmented text generation'
 'sirable in the personalized dialogue generation, par-\nallel data that contains speciﬁc terminologies is\nmore helpful in machine translation, and so on. On\nthe other hand, using a universal metric for retrieval\nmay lead to the lack of diversity of the retrieval re-\nsults. Collecting a diverse set of retrieval results\ncan improve the coverage of useful information.\nThus, considering multiple different metrics for re-\ntrieval may lead to generation with higher quality\nin the future.\n7\nConclusion\nIn this paper, we surveyed recent approaches for\nretrieval-augmented text generation. We reviewed\nand summarized the development of different com-\nponents of retrieval-augmented text generation in-\ncluding retrieval metrics, retrieval sources, and in-\ntegration paradigms. We gave in-depth discussions\nwhen retrieval-augmented text generation comes to\ndifferent applications including dialogue response\ngeneration, machine translation, and other genera-\ntion tasks. We also pointed out some future direc-\ntions for retrieval-augmented text generation.\nReferences\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\ngio. 2014.\nNeural machine translation by jointly\nlearning to align and translate.\narXiv preprint\narXiv:1409.0473.\nAnkur Bapna and Orhan Firat. 2019. Non-parametric\nadaptation for neural machine translation. In Pro-\nceedings of the 2019 Conference of the North Amer-\nican Chapter of the Association for Computational\nLinguistics: Human Language Technologies, Vol-\nume 1 (Long and Short Papers), pages 1921–1931.\nErgun Biçici and Marc Dymetman. 2008.\nDynamic\ntranslation memory: Using statistical machine trans-\nlation to improve translation memory fuzzy matches.\nIn International Conference on Intelligent Text Pro-\ncessing and Computational Linguistics, pages 454–\n465. Springer.\nSebastian Borgeaud, Arthur Mensch, Jordan Hoff-\nmann, Trevor Cai, Eliza Rutherford, Katie Millican,\nGeorge van den Driessche, Jean-Baptiste Lespiau,\nBogdan Damoc, Aidan Clark, Diego de Las Casas,'
 'ties between the source data and candidate texts,\nand extract a set of more ﬁne-grained prototypes\nfrom the candidates. Lastly, a generation model\ntakes the prototypes as input to produce the text\nthat describes the given structured data.\nWhile retrieval-augmented generation has been\nwidely explored in the NLP community, we sug-\ngest that future research could extend this approach\nto tasks that involve data from multiple modali-\nties. For instance, with recent advancements in\nimage-text retrieval (Jia et al., 2021; Radford et al.,\n2021), the structural gap between images and texts\nis largely bridged. Some early studies (Zhang et al.,\n2020) have shown that information retrieved from\nimages could improve the performance of neural\nmachine translation model. Naturally, such meth-\nods could be extended to other multi-modal tasks,\nsuch as image captioning (Karpathy and Li, 2015).\nA similar idea could also be applied to tasks be-\nyond images, such as speech-to-text transcription\n(Gales and Young, 2007).\n6\nFuture Directions\nDespite the current success of retrieval augmented\ntext generation, there is still a long way to go as\ndiscussed in previous sections. We highlight some\ndirections to facilitate the future research as fol-\nlows:\nRetrieval Sensitivity\nThe performance of re-\ntrieval augmented text generation is very sensitive\nto the retrieval quality, i.e., the similarity between\nthe query and the retrieved examples. Currently, re-\ntrieval augmented text generation models perform\nwell when the retrieved examples are very simi-\nlar to the query. However, they are even worse\nthan the generation models without retrieval when\nthe retrieval examples are less similar. Therefore,\nit would be important to exploit new methods to\naddress such an issue on similarity.\nRetrieval Efﬁciency\nGenerally, if one enlarges\nthe retrieval memory to some extent, it would be\npossible to retrieve an example which is very simi-\nlar to the query.Unfortunately, the downside is that']","['Retrieval-augmented text generation has several advantages over conventional generation models. Firstly, the knowledge is not necessarily stored implicitly in model parameters, but is explicitly acquired in a plug-and-play manner, which leads to great scalability. Secondly, instead of generating from scratch, this paradigm generates text from some retrieved human-written reference, which potentially alleviates the difficulty of text generation. This approach has achieved state-of-the-art performance in many NLP tasks.']"
What are the three major components of the retrieval-augmented generation paradigm?,"The three major components of the retrieval-augmented generation paradigm are:

1. Retrieval Sources: These are the sources of information that are used to retrieve relevant instances for the generation task. Examples of retrieval sources include training corpora, external data, and unsupervised data.

2. Retrieval Metrics: These metrics are used to measure the relevance or similarity between the retrieved instances and the input sequence. There are different types of retrieval metrics, such as sparse-vector retrieval, dense-vector retrieval, and task-specific retrieval metrics.

3. Generation Models: These models are responsible for generating the output sequence based on the input sequence and the retrieved instances. They can incorporate the retrieved information using various techniques, such as attention mechanisms, skeleton and templates, and information retrieval methods.

These three components work together to enhance the text generation process by leveraging relevant information from retrieval sources.","['trieved human-written reference, which potentially\nalleviates the difﬁculty of text generation.\nThis paper aims to review many representative\napproaches for retrieval-augmented text generation\ntasks including dialogue response generation (We-\nston et al., 2018), machine translation (Gu et al.,\n2018) and others (Hashimoto et al., 2018). We\n∗All authors contributed equally.\nﬁrstly present the generic paradigm of retrieval-\naugmented generation as well as three key com-\nponents under this paradigm, which are retrieval\nsources, retrieval metrics and generation models.\nThen, we introduce notable methods about\nretrieval-augmented generation, which are orga-\nnized with respect to different tasks. Speciﬁcally,\non the dialogue response generation task, exem-\nplar/template retrieval as an intermediate step has\nbeen shown beneﬁcial to informative response gen-\neration (Weston et al., 2018; Wu et al., 2019; Cai\net al., 2019a,b). In addition, there has been growing\ninterest in knowledge-grounded generation explor-\ning different forms of knowledge such as knowl-\nedge bases and external documents (Dinan et al.,\n2018; Zhou et al., 2018; Lian et al., 2019; Li et al.,\n2019; Qin et al., 2019; Wu et al., 2021; Zhang et al.,\n2021). On the machine translation task, we summa-\nrize the early work on how the retrieved sentences\n(called translation memory) are used to improve\nstatistical machine translation (SMT) (Koehn et al.,\n2003) models (Simard and Isabelle, 2009; Koehn\nand Senellart, 2010) and in particular, we inten-\nsively highlight several popular methods to inte-\ngrating translation memory to NMT models (Gu\net al., 2018; Zhang et al., 2018; Xu et al., 2020;\nHe et al., 2021). We also review the applications\nof retrieval-augmented generation in other genera-\ntion tasks such as abstractive summarization (Peng\net al., 2019), code generation (Hashimoto et al.,\n2018), paraphrase (Kazemnejad et al., 2020; Su\net al., 2021b), and knowledge-intensive generation'
 'tion tasks such as abstractive summarization (Peng\net al., 2019), code generation (Hashimoto et al.,\n2018), paraphrase (Kazemnejad et al., 2020; Su\net al., 2021b), and knowledge-intensive generation\n(Lewis et al., 2020b). Finally, we also point out\nsome promising directions on retrieval-augmented\ngeneration to push forward the future research.\n2\nRetrieval-Augmented Paradigm\nIn this section, we ﬁrst give a general formulation\nof retrieval-augmented text generation. Then, we\ndiscuss three major components of the retrieval-\naugmented generation paradigm, including the re-\narXiv:2202.01110v2  [cs.CL]  13 Feb 2022\nInput\nSources \n(Sec. 2.2):\nTraining \nCorpus\nExternal Data\nUnsupervised \nData\nMetrics\n(Sec. 2.3):\nSparse-vector \nRetrieval\nDense-vector \nRetrieval\nTask-specific \nRetrieval\nRetrieval Memory\nGeneration Model\nSec. 4: Machine \nTranslation\nSec. 5: Other \nTasks\nData \nAugmentation\nAttention \nMechanism\nSkeleton & \nTemplates\nInformation Retrieval\nTasks:\nSec. 3: Dialogue \nGeneration\nModels \n(Sec 2.4):\nOutput\nFigure 1: The overview of this survey.\ntrieval source, retrieval metric and integration meth-\nods.\n2.1\nFormulation\nMost text generation tasks can be formulated as a\nmapping from input sequence x to output sequence\ny : y = f(x). For instance, x and y could be the\ndialogue history and the corresponding response\nfor dialogue response generation, the text in the\nsource language and the translation in the target\nlanguage for machine translation, and so on.\nRecently, some researchers suggest to endow\nmodels the capability to access external memory\nvia some information retrieval techniques, so that\nthey can acquire more information in the generation\nprocess (Gu et al., 2018; Weston et al., 2018; Cai\net al., 2019b). The retrieval-augmented generation\ncan be further formulated as:\ny = f(x, z)\n(1)\nwhere z = {⟨xr, yr⟩} is a set of relevant instances\nretrieved from the original training set or external\ndatasets. The main idea of this paradigm is that yr']","['The three major components of the retrieval-augmented generation paradigm are the retrieval source, retrieval metric, and integration methods.']"
What is the main idea behind the cross-lingual retriever in the context of unsupervised corpus?,"The main idea behind the cross-lingual retriever in the context of an unsupervised corpus is to align source-side sentences with their corresponding target-side translations in a dense vector space. This alignment allows the retriever to directly connect the source-side input with the target-side translations, enabling the use of monolingual data in the target language as memories. In other words, the cross-lingual retriever retrieves target sentences from the unsupervised corpus by leveraging the dense vector representations of the aligned source and target sentences.","['as domain adaptation and knowledge update. For\nexample, Khandelwal et al. (2020a); Zheng et al.\n(2021a) employ the in-domain dataset as the exter-\nnal memory to achieve fast domain adaptation for\nmachine translation.\nUnsupervised Data\nOne limitation for previous\ntwo sources is that the datasets have to be super-\nvised datasets consisting of aligned input-output\npairs. For machine translation, Cai et al. (2021) pro-\npose a cross-lingual retriever to directly retrieve tar-\nget sentence from unsupervised corpus (i.e., mono-\nlingual corpus in the target language). The main\nidea is aligning source-side sentences and the corre-\nsponding target-side translations in a dense vector\nspace, i.e., aligning x and yr when xr is absent.\nAs a result, the retriever directly connects the dots\nbetween the source-side input and target-side trans-\nlations, enabling monolingual data in the target\nlanguage to be used alone as memories.\n2.3\nRetrieval Metrics\nSparse-vector Retrieval\nGiven an input se-\nquence x and a retrieval corpus, retrieval model\naims to retrieve a set of relevant examples z =\n{⟨xr, yr⟩} from the corpus. When a supervised\ncorpus is used, {⟨xr, yr⟩} is retrieved by measur-\ning the similarity between x and xr. For simi-\nlarity measurement, sparse-vector retrieval meth-\nods such as TF-IDF and BM25 (Robertson and\nZaragoza, 2009) are widely used. They match key-\nwords efﬁciently with an inverted index.\nDense-vector Retrieval\nHowever, these meth-\nods prefer examples with similar surfaces, and may\nfail to retrieve examples that are only semantically\nrelevant. To alleviate above problem, some stud-\nies (Cao and Xiong, 2018) attempt to retrieve in\ndense-vector space instead of the lexical overlap.\nRecent work (Lee et al., 2019) makes use of pre-\ntrained language models, which encodes the text to\nlow-dimensional dense vectors via BERT-based en-\ncoders. The retrieval score are computed via inner\nproducts between vectors.\nTask-speciﬁc\nRetrieval\nSimilarity-based\nre-'
 'ters and computing cost (Kaplan et al., 2020; Hoffmann\net al., 2022). Hence, scaling pretrained language mod-\nels in this way is economically unsustainable (Strubell\net al., 2020; Bender et al., 2021; Zhang et al., 2022).\nRetrieval augmented language models provide a\npromising alternative. They allow language models\nto efﬁciently access vast resources from an external cor-\npus (Guu et al., 2020; Borgeaud et al., 2022) that serves\nas a kind of “memory” they can refer to when making\npredictions, alleviating the need to memorize as much\n∗Work partly done during Suyu’s internship at Microsoft.\ninformation in their own network parameters (Roberts\net al., 2020). This open-book approach helps language\nmodels to better generalize on token prediction tasks and\nmachine translation (Khandelwal et al., 2019; Borgeaud\net al., 2022), and tasks which already involve a ﬁrst-\nstage retrieval component, e.g., OpenQA (Borgeaud\net al., 2022; Izacard et al., 2022). Existing retrieval\naugmentation methods usually stick to one single re-\ntrieval corpus throughout training and inference so that\nthe retrieval component can be indirectly guided by the\nsupervision from end tasks.\nIn this paper we improve the zero-shot generalization\nability of language models using “mixture-of-memory”\n(MoMA), a new retrieval augmentation mechanism. In-\nstead of a single corpus, MoMA retrieves documents\nfrom a “mixture” of multiple external corpora and en-\njoys the merits of a larger and more comprehensive\nsource of knowledge. This mechanism also allows re-\nmoving and/or “plugging-in” new corpora during in-\nference time, when more information from the target\ntask is revealed, or as an additional way for users to\ncontrol the model. Speciﬁcally, we apply MoMA on the\nzero-shot dense retrieval task, which is the foundation of\nmany important real-world applications (Thakur et al.,\n2021a; Kim, 2022) and also the retrieval component of\nrecent retrieval augmented language models (Guu et al.,']","['The main idea behind the cross-lingual retriever in the context of an unsupervised corpus is to align source-side sentences and their corresponding target-side translations in a dense vector space. This is done even when the source-side reference (xr) is absent. As a result, the retriever directly connects the source-side input and target-side translations, enabling monolingual data in the target language to be used alone as memories.']"
What are the two categories of dialogue systems mentioned in the context?,The two categories of dialogue systems mentioned in the context are chit-chat systems and task-oriented systems.,"['retrieved memory through attention becomes a\nvery nature and efﬁcient way.\nSkeleton Extraction\nIn the previous two meth-\nods, the downstream generation model learns how\nto ﬁlter out irrelevant or even harmful informa-\ntion from the retrieved examples implicitly. There\nalso exist some works that try to explicitly extract\nuseful information, i.e., skeleton extraction, from\nthe retrieved memory (Cai et al., 2019a; Wu et al.,\n2019; Cai et al., 2019b). For example, one skeleton\nshould be a part of a whole utterance with irrelevant\ncontent masked, and the generation model only in-\ntegrate this skeleton in the generation process.\n3\nDialogue Response Generation\nBackground\nDialogue systems can be grouped\ninto two categories: chit-chat systems and task-\noriented systems. While task-oriented dialogue\nsystems are designed to accomplish speciﬁc user\ntasks such as air tickets booking, chit-chat dialogue\nsystems aim at giving a meaningful and ﬂuent re-\nsponse for any dialogue history in the open domain.\nDialogue response generation in chit-chat dialogue\nsystem is challenging partly due to the diversity\nof possible responses to a single dialogue history\n(i.e., the one-to-many problem). The dialogue his-\ntory alone cannot decide a meaningful and speciﬁc\nresponse. Also, external knowledge that is not\npresent in the dialogue history are often necessary\nfor avoiding safe but boring responses. We focus\non recent efforts tackling the challenges to develop\nchit-chat dialogue systems.\nMost modern chit-chat dialogue systems can\nbe categorized into two classes, namely, retrieval-\nbased models and generation-based models. The\nretrieval-based models (Ji et al., 2014; Hu et al.,\n2014) directly copy an existing response from cu-\nrated dialogue corpora (i.e., the retrieval pool)\nwhen receiving a response request. The retrieved\nresponses are often informative and grammatical\nas they are collected from real-world conversa-\ntions and possibly post-edited by a human. How-']",['The two categories of dialogue systems mentioned in the context are chit-chat systems and task-oriented systems.']
What are the three major limitations in existing work for dialogue response generation?,"The three major limitations in existing work for dialogue response generation are:

1. Current methods only use one retrieved response for generation. It could be more beneficial to combine multiple retrieval responses, but this is challenging due to the one-to-many nature of dialogue response generation.

2. Current methods use a universal relevance score for retrieval. It could be more effective to use a more customized retrieval metric, especially for controlled dialogue response generation, such as considering persona or emotion.

3. The retrieval pool of existing methods is limited to dialogue corpora or documents. It could be useful to expand the retrieval pool by including more corpora from other domains or modalities.","['mentioned work demonstrates that retrieval-based\ndialogue systems can be used for building bet-\nter generation-based models. In general, this is\ndone by conditioning the generation on some re-\ntrieved responses. More traditionally, to infuse\nthe response with external knowledge, the retrieval\npool is not necessarily a dialogue corpus. In fact,\nknowledge-grounded dialogue response generation\nexploring different forms of knowledge such as\nknowledge bases and external documents (Dinan\net al., 2018; Zhou et al., 2018; Lian et al., 2019;\nLi et al., 2019; Qin et al., 2019; Wu et al., 2021;\nZhang et al., 2021; Komeili et al., 2021) has been\nactively explored.\nLimitations\nWe note that there are three major\nlimitations in existing work for dialogue response\ngeneration. First, current methods only use one\nretrieved response for generation. It can be more\nbeneﬁcial to combine multiple retrieval responses.\nHowever, this can be difﬁcult due to the one-to-\nmany nature of dialogue response generation. Sec-\nond, current methods use universal relevance score\nfor retrieval. It can be more effective if we can\nuse more customized retrieval metric especially\nfor controlled dialogue response generation (e.g.,\npersona, emotion, etc). Third, the retrieval pool\nof existing methods is limited to dialogue corpora\n(context-response pairs) or documents. It might\nbe useful to enlarge the retrieval pool by including\nmore corpora in other domains or in other modali-\nties. As discussed, there leaves plenty of possible\ndirections to explore in the future.\n4\nMachine Translation\nRetrieval augmented translation originates from hu-\nman translation scenarios (Somers, 2003). When\ntranslating ˆy from an input source sentence x, a hu-\nman translator typically involves a search engine to\nretrieve similar sentences {⟨xr, yr⟩} from a bilin-\ngual database. Such a technique called translation\nmemory is helpful to improve the translation qual-\nity and efﬁciency for human translators (Dillon'
 'retrieved memory through attention becomes a\nvery nature and efﬁcient way.\nSkeleton Extraction\nIn the previous two meth-\nods, the downstream generation model learns how\nto ﬁlter out irrelevant or even harmful informa-\ntion from the retrieved examples implicitly. There\nalso exist some works that try to explicitly extract\nuseful information, i.e., skeleton extraction, from\nthe retrieved memory (Cai et al., 2019a; Wu et al.,\n2019; Cai et al., 2019b). For example, one skeleton\nshould be a part of a whole utterance with irrelevant\ncontent masked, and the generation model only in-\ntegrate this skeleton in the generation process.\n3\nDialogue Response Generation\nBackground\nDialogue systems can be grouped\ninto two categories: chit-chat systems and task-\noriented systems. While task-oriented dialogue\nsystems are designed to accomplish speciﬁc user\ntasks such as air tickets booking, chit-chat dialogue\nsystems aim at giving a meaningful and ﬂuent re-\nsponse for any dialogue history in the open domain.\nDialogue response generation in chit-chat dialogue\nsystem is challenging partly due to the diversity\nof possible responses to a single dialogue history\n(i.e., the one-to-many problem). The dialogue his-\ntory alone cannot decide a meaningful and speciﬁc\nresponse. Also, external knowledge that is not\npresent in the dialogue history are often necessary\nfor avoiding safe but boring responses. We focus\non recent efforts tackling the challenges to develop\nchit-chat dialogue systems.\nMost modern chit-chat dialogue systems can\nbe categorized into two classes, namely, retrieval-\nbased models and generation-based models. The\nretrieval-based models (Ji et al., 2014; Hu et al.,\n2014) directly copy an existing response from cu-\nrated dialogue corpora (i.e., the retrieval pool)\nwhen receiving a response request. The retrieved\nresponses are often informative and grammatical\nas they are collected from real-world conversa-\ntions and possibly post-edited by a human. How-']","['The three major limitations in existing work for dialogue response generation are: 1) Current methods only use one retrieved response for generation, when it could be more beneficial to combine multiple retrieval responses. However, this can be difficult due to the one-to-many nature of dialogue response generation. 2) Current methods use a universal relevance score for retrieval. It could be more effective if a more customized retrieval metric was used, especially for controlled dialogue response generation (e.g., persona, emotion, etc). 3) The retrieval pool of existing methods is limited to dialogue corpora (context-response pairs) or documents. It might be useful to enlarge the retrieval pool by including more corpora in other domains or in other modalities. These limitations suggest several possible directions for future exploration.']"
What are the limitations of the three kinds of methods discussed in the context?,"The limitations of the three kinds of methods discussed in the context are as follows:

1. Fuzzy score for retrieval: These methods employ fuzzy score for retrieval, which is highly dependent on word matching. As a result, they may not be able to recall examples that are similar in word semantics but different in surface form.

2. Suboptimal integration of retrieved examples: The integration methods used in these methods, such as constrained decoding and phrase table aggregation, are heuristic and not optimized towards translation quality. Additionally, the parameter tuning method fine-tunes only a few parameters, which may not preserve sufficient knowledge from retrieved examples.

3. Intractability of joint optimization: Since these methods perform in a pipeline manner, it is difficult to jointly optimize retrieval metrics as well as SMT models. As a result, these methods often adopt an off-the-shelf metric for retrieval, leading to suboptimal performance.","['x, they ﬁrstly retrieve many similar bilingual sen-\ntences {⟨xr, yr⟩}, and then tune the parameters on\ntop of the retrieved sentences as well as a given de-\nvelopment dataset in a sentence-wise manner, i.e.,\nit performs an independent tuning for each input\nsentence. To improve the efﬁciency of each tuning\nstep, it propose a local update on top of {⟨xr, yr⟩}\nfrom a baseline model.\nDespite the successes of translation memory in\nSMT, there are still some limitations for the above\nthree kinds of methods. Firstly, all these methods\nemploy fuzzy score for retrieval which is highly de-\npendent on word matching and thus can not recall\nsuch examples which are similar in word seman-\ntics but different in surface form. Secondly, these\nmethods integrate the retrieved examples into a\nmodule of SMT in the ways which can not make\nfull use of the knowledge in retrieved examples.\nFor example, the integration ways in the ﬁrst two\nkinds (constrained decoding and phrase table ag-\ngregation) are heuristic and not optimized towards\ntranslation quality; the parameter tuning method\nﬁne-tunes few parameters for log-linear based SMT\nwhich are not enough to preserve sufﬁcient knowl-\nedge from retrieved examples. Thirdly, since SMT\nperforms in a pipeline manner, it is intractable to\njointly optimize retrieval metrics as well as SMT\nmodels. Consequently, all these methods adopt an\noff-the-shelf metric for retrieval, leading to sub-\noptimal performance.\n4.2\nTranslation Memory in NMT\nTranslation memory has been widely explored in\nNeural Machine Translation (NMT). Depending\non when retrieval is involved, we can categorize\nprevious works into two classes: 1) an NMT model\nleans how to cooperate with the retrieval model in\nthe training phase; 2) an NMT model is only aware\nof the retrieved data in the inference phase.\nInference Phase\nThe key point of literature in\nthis line is to reward some target words based on\nwords in yr in the inference process. Thus, a de-'
 'mentioned work demonstrates that retrieval-based\ndialogue systems can be used for building bet-\nter generation-based models. In general, this is\ndone by conditioning the generation on some re-\ntrieved responses. More traditionally, to infuse\nthe response with external knowledge, the retrieval\npool is not necessarily a dialogue corpus. In fact,\nknowledge-grounded dialogue response generation\nexploring different forms of knowledge such as\nknowledge bases and external documents (Dinan\net al., 2018; Zhou et al., 2018; Lian et al., 2019;\nLi et al., 2019; Qin et al., 2019; Wu et al., 2021;\nZhang et al., 2021; Komeili et al., 2021) has been\nactively explored.\nLimitations\nWe note that there are three major\nlimitations in existing work for dialogue response\ngeneration. First, current methods only use one\nretrieved response for generation. It can be more\nbeneﬁcial to combine multiple retrieval responses.\nHowever, this can be difﬁcult due to the one-to-\nmany nature of dialogue response generation. Sec-\nond, current methods use universal relevance score\nfor retrieval. It can be more effective if we can\nuse more customized retrieval metric especially\nfor controlled dialogue response generation (e.g.,\npersona, emotion, etc). Third, the retrieval pool\nof existing methods is limited to dialogue corpora\n(context-response pairs) or documents. It might\nbe useful to enlarge the retrieval pool by including\nmore corpora in other domains or in other modali-\nties. As discussed, there leaves plenty of possible\ndirections to explore in the future.\n4\nMachine Translation\nRetrieval augmented translation originates from hu-\nman translation scenarios (Somers, 2003). When\ntranslating ˆy from an input source sentence x, a hu-\nman translator typically involves a search engine to\nretrieve similar sentences {⟨xr, yr⟩} from a bilin-\ngual database. Such a technique called translation\nmemory is helpful to improve the translation qual-\nity and efﬁciency for human translators (Dillon'
 'old when he died.\nSo the ﬁnal answer is: Harry Vaughan Watkins.\nFor the test case, retrieval is triggered dynami-\ncally whenever the model generates a sub-question\n(e.g., “follow up” in self-ask).\nThe aforementioned three approaches are capa-\nble of retrieving additional information while gen-\nerating. However, they have notable drawbacks:\n(1) ﬁxed-interval approaches use previously gener-\nated tokens as queries which might not reﬂect what\nLMs intend to generate in the future. (2) Retriev-\ning information at a ﬁxed interval can be inefﬁcient\nbecause it might occur at inappropriate points. (3)\nQuestion decomposition approaches require task-\nspeciﬁc prompt engineering, which restricts their\ngeneralizability in new tasks.\n5\nExperimental Setup\nWe evaluate the effectiveness of FLARE on 4 di-\nverse knowledge-intensive tasks using few-shot in-\ncontext learning (Radford et al., 2019; Brown et al.,\n2020), as summarized in Table 6 of Appendix A.\nTo ensure fair head-to-head comparisons, we com-\npare the results of FLARE with baselines using\nthe same setting, namely, the same in-context ex-\nemplars, prompt format, retriever, and document\ncorpus. We follow previous works (Trivedi et al.,\n2022) to sub-sample at most 500 examples from\neach dataset due to the cost of running experi-\nments. The hyperparameters of FLARE are se-\nlected based on the development set and listed in\nTable 8. FLARE refers to FLAREdirect if not specif-\nically stated. For previous-window approaches,\nwe follow Ram et al. (2023) to use a window size\nl = 16 in our experiments.\n5.1\nMultihop QA\nDataset\nThe goal of multihop QA is to answer\ncomplex questions through a process of informa-\ntion retrieval and reasoning (Yang et al., 2018; Ho\net al., 2020). For instance, to answer “Why did\nthe founder of Versus die?”, we must ﬁrst identify\nwho founded Versus and subsequently determine\nthe cause of their death. Multihop QA also uniﬁes\ninto long-form generation when solved with the']","['The limitations of the three methods discussed in the context are: 1) They all employ a fuzzy score for retrieval which is highly dependent on word matching and thus cannot recall examples that are similar in word semantics but different in surface form. 2) These methods integrate the retrieved examples into a module of SMT in ways that cannot make full use of the knowledge in retrieved examples. For instance, the integration methods in the first two kinds (constrained decoding and phrase table aggregation) are heuristic and not optimized towards translation quality; the parameter tuning method fine-tunes few parameters for log-linear based SMT which are not enough to preserve sufficient knowledge from retrieved examples. 3) Since SMT performs in a pipeline manner, it is intractable to jointly optimize retrieval metrics as well as SMT models. Consequently, all these methods adopt an off-the-shelf metric for retrieval, leading to sub-optimal results.']"
What is the key point of literature in the inference phase of Translation Memory in NMT?,The key point of literature in the inference phase of Translation Memory in NMT is to reward target words based on words in the retrieved data. This is done by assigning higher rewards to target words that appear in the retrieved sentences and have aligned source words in both the input sentence and the retrieved sentence. The distribution of the generation model and the additional reward from the retrieval model are taken into account to make decisions during the inference process.,"['x, they ﬁrstly retrieve many similar bilingual sen-\ntences {⟨xr, yr⟩}, and then tune the parameters on\ntop of the retrieved sentences as well as a given de-\nvelopment dataset in a sentence-wise manner, i.e.,\nit performs an independent tuning for each input\nsentence. To improve the efﬁciency of each tuning\nstep, it propose a local update on top of {⟨xr, yr⟩}\nfrom a baseline model.\nDespite the successes of translation memory in\nSMT, there are still some limitations for the above\nthree kinds of methods. Firstly, all these methods\nemploy fuzzy score for retrieval which is highly de-\npendent on word matching and thus can not recall\nsuch examples which are similar in word seman-\ntics but different in surface form. Secondly, these\nmethods integrate the retrieved examples into a\nmodule of SMT in the ways which can not make\nfull use of the knowledge in retrieved examples.\nFor example, the integration ways in the ﬁrst two\nkinds (constrained decoding and phrase table ag-\ngregation) are heuristic and not optimized towards\ntranslation quality; the parameter tuning method\nﬁne-tunes few parameters for log-linear based SMT\nwhich are not enough to preserve sufﬁcient knowl-\nedge from retrieved examples. Thirdly, since SMT\nperforms in a pipeline manner, it is intractable to\njointly optimize retrieval metrics as well as SMT\nmodels. Consequently, all these methods adopt an\noff-the-shelf metric for retrieval, leading to sub-\noptimal performance.\n4.2\nTranslation Memory in NMT\nTranslation memory has been widely explored in\nNeural Machine Translation (NMT). Depending\non when retrieval is involved, we can categorize\nprevious works into two classes: 1) an NMT model\nleans how to cooperate with the retrieval model in\nthe training phase; 2) an NMT model is only aware\nof the retrieved data in the inference phase.\nInference Phase\nThe key point of literature in\nthis line is to reward some target words based on\nwords in yr in the inference process. Thus, a de-'
 'of the retrieved data in the inference phase.\nInference Phase\nThe key point of literature in\nthis line is to reward some target words based on\nwords in yr in the inference process. Thus, a de-\ncision can be made based on both the distribution\nof generation model and the additional reward of\nretrieval model. Some previous works propose to\nreward target words based on the sentence-level\nsimilarity between x and xr, and the word align-\nment between xr and yr. Given the input sentence\nx, Zhang et al. (2018) try to assign target words\nin ˆy with higher rewards, when they appear in yr\nand the aligned source words are in both xr and\nx. He et al. (2019) follow a similar framework\nand consider the position information of those tar-\nget words when rewarding. Those works reward\nthe target words in an explicit way, however, the\none-sentence-one-model approach (Li et al., 2016c;\nTurchi et al., 2017) propose to reward target word\nimplicitly. For each testing input x, their approach\nwill ﬁrst ﬁnetune the translation model on retrieved\nmemory {⟨xr, yr⟩} and then translate x.\nOthers try to reward target words based on token-\nlevel similarity score. Most works in this line are\nbased on the dense retriever (Khandelwal et al.,\n2020a), e.g., faiss. Khandelwal et al. (2020a) build\na key-value datastore, where key h(xr, yr\n<t) is the\nhidden state at each time step when translating yr\nfrom xr, and value is its golden-truth target word\nyr\nt. Therefore, in the inference time, they can use\nthe h(x, ˆy<t) as query and reward target words\nwith similar hidden representations in the datas-\ntore. Although this method achieves signiﬁcant\nperformance gain, one drawback of it is the high la-\ntency. To address this issue, Meng et al. (2021) use\nsome heuristics, e.g., pre-ﬁltering, to avoid search-\ning on the entire datastore. The reward score of\nprevious works is got from some non-parametric\napproaches, however, Zheng et al. (2021a) propose\na light-weight network to learn the reward score.'
 'trieved human-written reference, which potentially\nalleviates the difﬁculty of text generation.\nThis paper aims to review many representative\napproaches for retrieval-augmented text generation\ntasks including dialogue response generation (We-\nston et al., 2018), machine translation (Gu et al.,\n2018) and others (Hashimoto et al., 2018). We\n∗All authors contributed equally.\nﬁrstly present the generic paradigm of retrieval-\naugmented generation as well as three key com-\nponents under this paradigm, which are retrieval\nsources, retrieval metrics and generation models.\nThen, we introduce notable methods about\nretrieval-augmented generation, which are orga-\nnized with respect to different tasks. Speciﬁcally,\non the dialogue response generation task, exem-\nplar/template retrieval as an intermediate step has\nbeen shown beneﬁcial to informative response gen-\neration (Weston et al., 2018; Wu et al., 2019; Cai\net al., 2019a,b). In addition, there has been growing\ninterest in knowledge-grounded generation explor-\ning different forms of knowledge such as knowl-\nedge bases and external documents (Dinan et al.,\n2018; Zhou et al., 2018; Lian et al., 2019; Li et al.,\n2019; Qin et al., 2019; Wu et al., 2021; Zhang et al.,\n2021). On the machine translation task, we summa-\nrize the early work on how the retrieved sentences\n(called translation memory) are used to improve\nstatistical machine translation (SMT) (Koehn et al.,\n2003) models (Simard and Isabelle, 2009; Koehn\nand Senellart, 2010) and in particular, we inten-\nsively highlight several popular methods to inte-\ngrating translation memory to NMT models (Gu\net al., 2018; Zhang et al., 2018; Xu et al., 2020;\nHe et al., 2021). We also review the applications\nof retrieval-augmented generation in other genera-\ntion tasks such as abstractive summarization (Peng\net al., 2019), code generation (Hashimoto et al.,\n2018), paraphrase (Kazemnejad et al., 2020; Su\net al., 2021b), and knowledge-intensive generation'
 'it through attention mechanism. He et al. (2021)\npropose a light-weight method to encode the re-\ntrieved target sentences and leverage the alignment\ninformation to ﬁlter out irrelevant information. Dif-\nferent from previous works that rely on bilingual\nmemories, Cai et al. (2021) propose a framework\nthat can retrieve the most similar target sentence in\na monolingual dataset, using a source sentence as\nquery.\nLimitations\nIn the section of SMT, we have\nshowed some limitations of the retrieval augmented\napproaches. There also exist some limitations in\nthe line of NMT. First, the information used for\nderiving reward scores is limited. The similarity\nbetween an input and retrieved examples is the\nprimary feature to derive reward scores.\nHow-\never, some information, e.g., frequencies of words\nand context, may also be beneﬁcial for integrating\nthe translation memory. Second, it remains to be\nan open question that when should we use the re-\ntrieved information and when not. In the inference\nphase, approaches tend to integrate the translation\nmemory excessively, e.g., at each time step, which\nnot only reduces the translation efﬁciency but may\nalso dampen the ﬂuency of generated results.\n5\nOther Tasks\nIn addition to dialogue system and machine trans-\nlation, retrieval-augmented generation techniques\nhave shown to be beneﬁcial in many other tasks. In\nthe following, we highlight several key tasks that\napply retrieval-augmented generation approaches.1\nLanguage Modelling\nIt has been shown that\nproperly leveraging information from retrieval\nmemory could improve the performance of large\npre-trained language model. To build a more accu-\nrate language model, Khandelwal et al. (2020b) pro-\npose to incorporate a soft memory module into the\nsystem. Speciﬁcally, an index is built by caching\nthe hidden states of the training corpus. Then, the\nlanguage model accesses the index via k-NN search\nand displays a greatly improved performance. As\nanother example, Guu et al. (2020) propose a new']","['The key point of literature in the inference phase of Translation Memory in NMT is to reward some target words based on words in yr in the inference process. This allows a decision to be made based on both the distribution of the generation model and the additional reward of the retrieval model. Some previous works propose to reward target words based on the sentence-level similarity between x and xr, and the word alignment between xr and yr. Other works reward target words based on token-level similarity score. Some methods also use a light-weight network to learn the reward score.']"
What is the proposed method by Cai et al. (2021) for retrieving the most similar target sentence in a monolingual dataset?,Cai et al. (2021) propose a framework that can retrieve the most similar target sentence in a monolingual dataset using a source sentence as a query.,"['it through attention mechanism. He et al. (2021)\npropose a light-weight method to encode the re-\ntrieved target sentences and leverage the alignment\ninformation to ﬁlter out irrelevant information. Dif-\nferent from previous works that rely on bilingual\nmemories, Cai et al. (2021) propose a framework\nthat can retrieve the most similar target sentence in\na monolingual dataset, using a source sentence as\nquery.\nLimitations\nIn the section of SMT, we have\nshowed some limitations of the retrieval augmented\napproaches. There also exist some limitations in\nthe line of NMT. First, the information used for\nderiving reward scores is limited. The similarity\nbetween an input and retrieved examples is the\nprimary feature to derive reward scores.\nHow-\never, some information, e.g., frequencies of words\nand context, may also be beneﬁcial for integrating\nthe translation memory. Second, it remains to be\nan open question that when should we use the re-\ntrieved information and when not. In the inference\nphase, approaches tend to integrate the translation\nmemory excessively, e.g., at each time step, which\nnot only reduces the translation efﬁciency but may\nalso dampen the ﬂuency of generated results.\n5\nOther Tasks\nIn addition to dialogue system and machine trans-\nlation, retrieval-augmented generation techniques\nhave shown to be beneﬁcial in many other tasks. In\nthe following, we highlight several key tasks that\napply retrieval-augmented generation approaches.1\nLanguage Modelling\nIt has been shown that\nproperly leveraging information from retrieval\nmemory could improve the performance of large\npre-trained language model. To build a more accu-\nrate language model, Khandelwal et al. (2020b) pro-\npose to incorporate a soft memory module into the\nsystem. Speciﬁcally, an index is built by caching\nthe hidden states of the training corpus. Then, the\nlanguage model accesses the index via k-NN search\nand displays a greatly improved performance. As\nanother example, Guu et al. (2020) propose a new'
 'as domain adaptation and knowledge update. For\nexample, Khandelwal et al. (2020a); Zheng et al.\n(2021a) employ the in-domain dataset as the exter-\nnal memory to achieve fast domain adaptation for\nmachine translation.\nUnsupervised Data\nOne limitation for previous\ntwo sources is that the datasets have to be super-\nvised datasets consisting of aligned input-output\npairs. For machine translation, Cai et al. (2021) pro-\npose a cross-lingual retriever to directly retrieve tar-\nget sentence from unsupervised corpus (i.e., mono-\nlingual corpus in the target language). The main\nidea is aligning source-side sentences and the corre-\nsponding target-side translations in a dense vector\nspace, i.e., aligning x and yr when xr is absent.\nAs a result, the retriever directly connects the dots\nbetween the source-side input and target-side trans-\nlations, enabling monolingual data in the target\nlanguage to be used alone as memories.\n2.3\nRetrieval Metrics\nSparse-vector Retrieval\nGiven an input se-\nquence x and a retrieval corpus, retrieval model\naims to retrieve a set of relevant examples z =\n{⟨xr, yr⟩} from the corpus. When a supervised\ncorpus is used, {⟨xr, yr⟩} is retrieved by measur-\ning the similarity between x and xr. For simi-\nlarity measurement, sparse-vector retrieval meth-\nods such as TF-IDF and BM25 (Robertson and\nZaragoza, 2009) are widely used. They match key-\nwords efﬁciently with an inverted index.\nDense-vector Retrieval\nHowever, these meth-\nods prefer examples with similar surfaces, and may\nfail to retrieve examples that are only semantically\nrelevant. To alleviate above problem, some stud-\nies (Cao and Xiong, 2018) attempt to retrieve in\ndense-vector space instead of the lexical overlap.\nRecent work (Lee et al., 2019) makes use of pre-\ntrained language models, which encodes the text to\nlow-dimensional dense vectors via BERT-based en-\ncoders. The retrieval score are computed via inner\nproducts between vectors.\nTask-speciﬁc\nRetrieval\nSimilarity-based\nre-'
 'of the retrieved data in the inference phase.\nInference Phase\nThe key point of literature in\nthis line is to reward some target words based on\nwords in yr in the inference process. Thus, a de-\ncision can be made based on both the distribution\nof generation model and the additional reward of\nretrieval model. Some previous works propose to\nreward target words based on the sentence-level\nsimilarity between x and xr, and the word align-\nment between xr and yr. Given the input sentence\nx, Zhang et al. (2018) try to assign target words\nin ˆy with higher rewards, when they appear in yr\nand the aligned source words are in both xr and\nx. He et al. (2019) follow a similar framework\nand consider the position information of those tar-\nget words when rewarding. Those works reward\nthe target words in an explicit way, however, the\none-sentence-one-model approach (Li et al., 2016c;\nTurchi et al., 2017) propose to reward target word\nimplicitly. For each testing input x, their approach\nwill ﬁrst ﬁnetune the translation model on retrieved\nmemory {⟨xr, yr⟩} and then translate x.\nOthers try to reward target words based on token-\nlevel similarity score. Most works in this line are\nbased on the dense retriever (Khandelwal et al.,\n2020a), e.g., faiss. Khandelwal et al. (2020a) build\na key-value datastore, where key h(xr, yr\n<t) is the\nhidden state at each time step when translating yr\nfrom xr, and value is its golden-truth target word\nyr\nt. Therefore, in the inference time, they can use\nthe h(x, ˆy<t) as query and reward target words\nwith similar hidden representations in the datas-\ntore. Although this method achieves signiﬁcant\nperformance gain, one drawback of it is the high la-\ntency. To address this issue, Meng et al. (2021) use\nsome heuristics, e.g., pre-ﬁltering, to avoid search-\ning on the entire datastore. The reward score of\nprevious works is got from some non-parametric\napproaches, however, Zheng et al. (2021a) propose\na light-weight network to learn the reward score.']",['Cai et al. (2021) propose a framework that retrieves the most similar target sentence in a monolingual dataset using a source sentence as a query.']
What is the proposed framework for paraphrase generation by Kazemnejad et al. (2020)?,"The proposed framework for paraphrase generation by Kazemnejad et al. (2020) involves a two-step process. First, a sentence that is similar to the input sentence is retrieved. Then, a neural editor uses the retrieved sentence to produce the resulting paraphrased sentence.","['with respect to the source document. Then, the\ndocument summarization is produced by re-writing\nthe selected templates.\nParaphrase Generation\nTo address the lack of\nquality as well as diversity in the generation of para-\nphrases, Kazemnejad et al. (2020) propose a gen-\neration framework which ﬁrst retrieves a sentence\nthat is similar to input sentence. Then, based on\nthe retrieved sentence, a neural editor produces the\nresulting paraphrased sentence. Chen et al. (2019)\ninvestigate a different aspect of paraphrasing, i.e.\nhow to control the linguistic syntax displayed in\nthe generated text. To achieve this goal, Chen et al.\n(2019) propose to ﬁrst extract a sentential exem-\nplar that serves as the syntax template. A neural\nmodel then generates the paraphrase with desired\nlinguistic syntax following the retrieved exemplar.\nText Style Transfer\nTo improve the quality of\ngenerated text, Li et al. (2018) propose a retrieval-\naugmented framework which ﬁrst retrieves texts\nthat are similar to the input based on lexical-level\nsimilarity. Then, the retrieved tokens that are irrel-\nevant to the source are deleted, and the output is\nderived from the edited template. Xiao et al. (2021)\nalso adopte this framework by incorporating re-\ntrieval information from two sources (i.e. sparse\nand dense memories) and obtained an improved\nmodel performance.\nData-to-Text Generation\nRecently, retrieval-\naugmented generation has been adapted to the task\nof data-to-text generation. To bridge the gap be-\ntween the structured data and natural language\ntext, Su et al. (2021a) propose a novel retrieval-\naugmented framework.\nSpeciﬁcally, given the\nsource data, a set of candidate texts are ﬁrst re-\ntrieved from a large unlabelled corpus. Then, a\nneural selector is applied to measure the similari-\nties between the source data and candidate texts,\nand extract a set of more ﬁne-grained prototypes\nfrom the candidates. Lastly, a generation model\ntakes the prototypes as input to produce the text'
 'moyer. 2020.\nSimple and effective retrieve-edit-\nrerank text generation. In Proceedings of the 58th\nAnnual Meeting of the Association for Computa-\ntional Linguistics, pages 2532–2538.\nBaotian Hu, Zhengdong Lu, Hang Li, and Qingcai\nChen. 2014. Convolutional neural network architec-\ntures for matching natural language sentences. In\nNIPS, pages 2042–2050.\nZongcheng Ji, Zhengdong Lu, and Hang Li. 2014. An\ninformation retrieval approach to short text conver-\nsation. arXiv preprint arXiv:1408.6988.\nChao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana\nParekh, Hieu Pham, Quoc V. Le, Yun-Hsuan Sung,\nZhen Li, and Tom Duerig. 2021. Scaling up visual\nand vision-language representation learning with\nnoisy text supervision. In Proceedings of the 38th In-\nternational Conference on Machine Learning, ICML\n2021, 18-24 July 2021, Virtual Event, volume 139 of\nProceedings of Machine Learning Research, pages\n4904–4916. PMLR.\nAndrej Karpathy and Fei-Fei Li. 2015. Deep visual-\nsemantic alignments for generating image descrip-\ntions. In IEEE Conference on Computer Vision and\nPattern Recognition, CVPR 2015, Boston, MA, USA,\nJune 7-12, 2015, pages 3128–3137. IEEE Computer\nSociety.\nAmirhossein Kazemnejad, Mohammadreza Salehi, and\nMahdieh Soleymani Baghshah. 2020.\nParaphrase\ngeneration by learning how to edit from samples. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 6010–\n6021, Online. Association for Computational Lin-\nguistics.\nUrvashi Khandelwal, Angela Fan, Dan Jurafsky, Luke\nZettlemoyer,\nand Mike Lewis. 2020a.\nNear-\nest neighbor machine translation.\narXiv preprint\narXiv:2010.00710.\nUrvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke\nZettlemoyer, and Mike Lewis. 2020b. Generaliza-\ntion through memorization: Nearest neighbor lan-\nguage models. In 8th International Conference on\nLearning Representations, ICLR 2020, Addis Ababa,\nEthiopia, April 26-30, 2020. OpenReview.net.\nPhilipp Koehn, Franz J. Och, and Daniel Marcu. 2003.']","['Kazemnejad et al. (2020) proposed a paraphrase generation framework that first retrieves a sentence similar to the input sentence. Then, a neural editor produces the resulting paraphrased sentence based on the retrieved sentence.']"
What is the purpose of this paper?,The purpose of this paper is to propose a method called MoMA (Model-Agnostic Meta-Learning) for better augmenting language models. The authors aim to provide alternatives to achieve generalization ability beyond solely relying on model scale. They evaluate the efficacy of MoMA through empirical performances on BEIR (Benchmarking End-to-End Information Retrieval) tasks. The paper also discusses the limitations of their approach and highlights the need for choosing effective grounding corpora and evaluating their relative contribution in real-world scenarios.,"['better augmenting language models, to provide other al-\nternatives to achieve generalization ability beyond solely\nrelying on model scale.\nLimitations\nAlthough MoMA (T5-ANCE) and MoMA (COCO)\nachieve strong zero-shot performances, we mainly ver-\nify their efﬁcacy from the empirical performances\non BEIR tasks, where the target corpora, Wiki and\nMARCO serve as readily available retrieval sources.\nIn a real-world scenario, the grounding corpora usually\nneed to be customized according to query domains and\nuser needs. Thus, how to choose effective grounding\ncorpora and efﬁciently evaluate their relative contribu-\ntion remain an open problem. These analyses will go\nbeyond our empirical settings and reveal a wider appli-\ncation scenario of MoMA.\nEthics Statement\nAll data in this study are publicly available and used\nunder ethical considerations. Text and ﬁgures in the\npaper are used for illustration only, they do not represent\nthe ethical attitude of the authors.\nReferences\nPayal Bajaj, Daniel Campos, Nick Craswell, Li Deng,\nJianfeng Gao, Xiaodong Liu, Rangan Majumder,\nAndrew McNamara, Bhaskar Mitra, Tri Nguyen,\net al. 2016. MS MARCO: A human generated ma-\nchine reading comprehension dataset. arXiv preprint\narXiv:1611.09268.\nEmily M Bender, Timnit Gebru, Angelina McMillan-\nMajor, and Shmargaret Shmitchell. 2021. On the\ndangers of stochastic parrots: Can language models\nbe too big? In Proceedings of the 2021 ACM Confer-\nence on Fairness, Accountability, and Transparency,\npages 610–623.\nAlexander Bondarenko, Maik Fröbe, Meriem Be-\nloucif, Lukas Gienapp, Yamen Ajjour, Alexander\nPanchenko, Chris Biemann, Benno Stein, Henning\nWachsmuth, Martin Potthast, and Matthias Hagen.\n2020.\nOverview of Touché 2020: Argument Re-\ntrieval. In Working Notes Papers of the CLEF 2020\nEvaluation Labs, volume 2696 of CEUR Workshop\nProceedings.\nSebastian Borgeaud, Arthur Mensch, Jordan Hoff-\nmann, Trevor Cai, Eliza Rutherford, Katie Milli-\ncan, George Bm Van Den Driessche, Jean-Baptiste'
 'August 4, Volume 1: Long Papers, pages 1870–1879.\nAssociation for Computational Linguistics.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, Parker Schuh, Kensen Shi,\nSasha Tsvyashchenko, Joshua Maynez, Abhishek\nRao, Parker Barnes, Yi Tay, Noam Shazeer, Vin-\nodkumar Prabhakaran, Emily Reif, Nan Du, Ben\nHutchinson, Reiner Pope, James Bradbury, Jacob\nAustin, Michael Isard, Guy Gur-Ari, Pengcheng\nYin, Toju Duke, Anselm Levskaya, Sanjay Ghe-\nmawat, Sunipa Dev, Henryk Michalewski, Xavier\nGarcia, Vedant Misra, Kevin Robinson, Liam Fe-\ndus, Denny Zhou, Daphne Ippolito, David Luan,\nHyeontaek Lim, Barret Zoph, Alexander Spiridonov,\nRyan Sepassi, David Dohan, Shivani Agrawal, Mark\nOmernick, Andrew M. Dai, Thanumalayan Sankara-\nnarayana Pillai, Marie Pellat, Aitor Lewkowycz,\nErica Moreira, Rewon Child, Oleksandr Polozov,\nKatherine Lee, Zongwei Zhou, Xuezhi Wang, Bren-\nnan Saeta, Mark Diaz, Orhan Firat, Michele Catasta,\nJason Wei, Kathy Meier-Hellstern, Douglas Eck,\nJeff Dean, Slav Petrov, and Noah Fiedel. 2022.\nPalm: Scaling language modeling with pathways.\nCoRR, abs/2204.02311.\nNachshon Cohen, Oren Kalinsky, Yftah Ziser, and\nAlessandro Moschitti. 2021.\nWikisum:\nCoher-\nent summarization dataset for efﬁcient human-\nevaluation. In Proceedings of the 59th Annual Meet-\ning of the Association for Computational Linguis-\ntics and the 11th International Joint Conference on\nNatural Language Processing, ACL/IJCNLP 2021,\n(Volume 2: Short Papers), Virtual Event, August 1-\n6, 2021, pages 212–219. Association for Computa-\ntional Linguistics.\nEmily Dinan, Stephen Roller, Kurt Shuster, Angela\nFan, Michael Auli, and Jason Weston. 2019. Wizard\nof wikipedia: Knowledge-powered conversational\nagents. In 7th International Conference on Learn-\ning Representations, ICLR 2019, New Orleans, LA,\nUSA, May 6-9, 2019. OpenReview.net.\nAngela Fan, Yacine Jernite, Ethan Perez, David Grang-'
 'Roberts, Ian Soboroff, and Lucy Lu Wang. 2021.\nTREC-COVID: Constructing a pandemic informa-\ntion retrieval test collection. SIGIR Forum, 54(1).\nEllen M Voorhees et al. 2004. Overview of the trec\n2004 robust retrieval track. In Trec, pages 69–77.\nHenning Wachsmuth, Shahbaz Syed, and Benno Stein.\n2018. Retrieval of the best counterargument without\nprior topic knowledge. In Proceedings of the 56th\nAnnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 241–251,\nMelbourne, Australia. Association for Computational\nLinguistics.\nDavid Wadden, Shanchuan Lin, Kyle Lo, Lucy Lu\nWang, Madeleine van Zuylen, Arman Cohan, and\nHannaneh Hajishirzi. 2020. Fact or ﬁction: Verifying\nscientiﬁc claims. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 7534–7550, Online. As-\nsociation for Computational Linguistics.\nKexin Wang, Nandan Thakur, Nils Reimers, and Iryna\nGurevych. 2022. GPL: Generative pseudo labeling\nfor unsupervised domain adaptation of dense retrieval.\nIn Proceedings of the 2022 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nSeattle, United States. Association for Computational\nLinguistics.\nYining Wang, Liwei Wang, Yuanzhi Li, Di He, Wei\nChen, and Tie-Yan Liu. 2013. A theoretical analysis\nof ndcg ranking measures. In Proceedings of the 26th\nannual conference on learning theory (COLT 2013),\nvolume 8, page 6. Citeseer.\nGuillaume Wenzek, Marie-Anne Lachaux, Alexis Con-\nneau, Vishrav Chaudhary, Francisco Guzmán, Ar-\nmand Joulin, and Edouard Grave. 2020.\nCCNet:\nExtracting high quality monolingual datasets from\nweb crawl data. In Proceedings of the 12th Lan-\nguage Resources and Evaluation Conference, pages\n4003–4012, Marseille, France. European Language\nResources Association.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-']","['The purpose of this paper is to conduct a survey about retrieval-augmented text generation. It highlights the generic paradigm of retrieval-augmented generation and reviews notable approaches according to different tasks including dialogue response generation, machine translation, and other generation tasks. The paper also points out some important directions on top of recent methods to facilitate future research.']"

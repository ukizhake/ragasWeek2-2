question,answer,contexts,ground_truths
What are the advantages of retrieval-augmented text generation compared to generation-based models?,"Retrieval-augmented text generation has several advantages compared to generation-based models:

1. Scalability: Retrieval-augmented text generation does not rely solely on implicitly stored model parameters. Instead, it acquires information explicitly in a plug-and-play manner, which allows for greater scalability.

2. Improved Performance: Retrieval-augmented text generation has achieved state-of-the-art performance in many natural language processing tasks. It has shown remarkable advantages over conventional generation models.

3. Easier Text Generation: Instead of generating text from scratch, retrieval-augmented text generation generates text from retrieved human-written references. This approach potentially alleviates the difficulty of text generation.

These advantages make retrieval-augmented text generation an attractive approach in the computational linguistics community.","['attracted increasing attention of the compu-\ntational linguistics community.\nCompared\nwith conventional generation models, retrieval-\naugmented text generation has remarkable ad-\nvantages and particularly has achieved state-of-\nthe-art performance in many NLP tasks. This\npaper aims to conduct a survey about retrieval-\naugmented text generation. It ﬁrstly highlights\nthe generic paradigm of retrieval-augmented\ngeneration, and then it reviews notable ap-'
 'the generic paradigm of retrieval-augmented\ngeneration, and then it reviews notable ap-\nproaches according to different tasks including\ndialogue response generation, machine trans-\nlation, and other generation tasks. Finally, it\npoints out some promising directions on top of\nrecent methods to facilitate future research.\n1\nIntroduction\nRetrieval-augmented text generation, as a new\ntext generation paradigm that fuses emerging deep\nlearning technology and traditional retrieval tech-'
 'A Survey on Retrieval-Augmented Text Generation\nHuayang Li♥,∗\nYixuan Su♠,∗\nDeng Cai♦,∗\nYan Wang♣,∗\nLemao Liu♣,∗\n♥Nara Institute of Science and Technology\n♠University of Cambridge\n♦The Chinese University of Hong Kong\n♣Tencent AI Lab\nli.huayang.lh6@is.naist.jp, ys484@cam.ac.uk\nthisisjcykcd@gmail.com, brandenwang@tencent.com\nlemaoliu@gmail.com\nAbstract\nRecently, retrieval-augmented text generation\nattracted increasing attention of the compu-\ntational linguistics community.\nCompared'
 'implicitly stored in model parameters, but is explic-\nitly acquired in a plug-and-play manner, leading\nto great scalibility; 2) Instead of generating from\nscratch, the paradigm generating text from some re-\ntrieved human-written reference, which potentially\nalleviates the difﬁculty of text generation.\nThis paper aims to review many representative\napproaches for retrieval-augmented text generation\ntasks including dialogue response generation (We-']","['The advantages of retrieval-augmented text generation compared to generation-based models include: 1) The knowledge is not necessary to be implicitly stored in model parameters, but is explicitly acquired in a plug-and-play manner, leading to great scalability; 2) Instead of generating from scratch, the paradigm generating text from some retrieved human-written reference, which potentially alleviates the difficulty of text generation. It has achieved state-of-the-art performance in many NLP tasks.']"
What is the proposed method for active retrieval augmented generation?,The proposed method for active retrieval augmented generation is called Forward-Looking Active Retrieval augmented generation (FLARE). It is a generic retrieval-augmented generation method that uses a prediction of the upcoming sentence to anticipate future content. This prediction is then used as a query to retrieve relevant documents to regenerate the sentence if it contains low-confidence tokens.,"['active retrieval augmented generation, meth-\nods that actively decide when and what to re-\ntrieve across the course of the generation. We\npropose Forward-Looking Active REtrieval\naugmented generation (FLARE), a generic\nretrieval-augmented generation method which\niteratively uses a prediction of the upcoming\nsentence to anticipate future content, which is\nthen utilized as a query to retrieve relevant doc-\numents to regenerate the sentence if it contains\nlow-conﬁdence tokens. We test FLARE along'
 'Active Retrieval Augmented Generation\nZhengbao Jiang1∗\nFrank F. Xu1∗\nLuyu Gao1∗\nZhiqing Sun1∗\nQian Liu2\nJane Dwivedi-Yu3\nYiming Yang1\nJamie Callan1\nGraham Neubig1\n1Language Technologies Institute, Carnegie Mellon University\n2Sea AI Lab\n3Meta AI Research\n{zhengbaj,fangzhex,luyug,zhiqings,gneubig}@cs.cmu.edu\nAbstract\nDespite the remarkable ability of large lan-\nguage models (LMs) to comprehend and gen-\nerate language, they have a tendency to hal-\nlucinate and create factually inaccurate out-\nput.'
 'the generic paradigm of retrieval-augmented\ngeneration, and then it reviews notable ap-\nproaches according to different tasks including\ndialogue response generation, machine trans-\nlation, and other generation tasks. Finally, it\npoints out some promising directions on top of\nrecent methods to facilitate future research.\n1\nIntroduction\nRetrieval-augmented text generation, as a new\ntext generation paradigm that fuses emerging deep\nlearning technology and traditional retrieval tech-'
 'such as a company’s centralized document stor-\nage, and the retrieval step can introduce additional\nlatency overhead.\nTo address the challenges previously outlined,\nwe propose the Hybrid Retrieval-Augmented Gen-\neration (HybridRAG) framework. This framework\nleverages cloud-generated memory augmentation\nto boost the performance of small language mod-\nels on edge devices, while operating in an asyn-\nchronous manner. The HybridRAG framework\nconsists of a retriever model and memory generator']","['The proposed method for active retrieval augmented generation is called Forward-Looking Active REtrieval augmented generation (FLARE). This method iteratively uses a prediction of the upcoming sentence to anticipate future content, which is then utilized as a query to retrieve relevant documents to regenerate the sentence if it contains low-confidence tokens.']"
What is the proposed framework for enhancing composition assistance in real-time?,The proposed framework for enhancing composition assistance in real-time is called the Hybrid Retrieval-Augmented Generation (HybridRAG) framework. It leverages a hybrid setting that combines both client and cloud models. HybridRAG incorporates retrieval-augmented memory generated asynchronously to overcome the processing time limitations of retrieval augmented large language models. This framework allows for prompt suggestions of next words or sentences within tight latency budgets.,"['Hybrid Retrieval-Augmented Generation for\nReal-time Composition Assistance\nXuchao Zhang∗\nMenglin Xia∗\nCamille Couturier\nGuoqing Zheng\nSaravan Rajmohan\nVictor Rühle\nMicrosoft\n{xuchaozhang, mollyxia, cacoutur, zheng, saravar, viruh}@microsoft.com\nAbstract\nRetrieval augmented models show promise in\nenhancing traditional language models by im-\nproving their contextual understanding, inte-\ngrating private data, and reducing hallucination.\nHowever, the processing time required for re-'
 'grating private data, and reducing hallucination.\nHowever, the processing time required for re-\ntrieval augmented large language models poses\na challenge when applying them to tasks that\nrequire real-time responses, such as composi-\ntion assistance. To overcome this limitation,\nwe propose the Hybrid Retrieval-Augmented\nGeneration (HybridRAG) framework that lever-\nages a hybrid setting that combines both client\nand cloud models. HybridRAG incorporates\nretrieval-augmented memory generated asyn-'
 'promptly suggest next words or sentences, and\ntherefore operate within tight latency budgets (typ-\nically in the order of 100ms or less). To avoid\nlatency overheads for sending inference requests\nto the cloud, these models are usually deployed\non users’ edge devices. This imposes strict con-\nstraints on the model’s size and capabilities, lim-\niting the effectiveness of composition assistance.\nWhile recent advancements have enabled LLMs\nlike LLAMA (Touvron et al., 2023) to generate 5'
 'currence of hallucinations. However, the retrieval-\naugmented large language models can be slow to\n*These authors contributed equally to this work.\nrun due to the size of the model and the extra re-\ntrieval step they require, which can cause latency\nand limit its application in tasks requiring real-time\nresponses, such as composition assistance.\nReal-time composition tools are designed to\npromptly suggest next words or sentences, and\ntherefore operate within tight latency budgets (typ-']","[""The proposed framework for enhancing composition assistance in real-time is the Hybrid Retrieval-Augmented Generation (HybridRAG) framework. This framework leverages a hybrid setting that combines both client and cloud models. HybridRAG incorporates retrieval-augmented memory generated asynchronously by a Large Language Model (LLM) in the cloud. By integrating this retrieval augmented memory, the client model acquires the capability to generate highly effective responses, benefiting from the LLM's capabilities. Furthermore, through asynchronous memory integration, the client model is capable of delivering real-time responses to user requests without the need to wait for memory synchronization from the cloud.""]"
What is the purpose of retrieval augmentation in language models?,"The purpose of retrieval augmentation in language models is to provide external information from a vast corpus, acting as a ""memory"" for the language model. This allows the language model to access and refer to this external information when making predictions, reducing the need for the model to memorize large amounts of information in its own network parameters. Retrieval augmentation helps language models better generalize on token prediction tasks, machine translation, and tasks that involve a retrieval component.","['Retrieval augmented language models provide a\npromising alternative. They allow language models\nto efﬁciently access vast resources from an external cor-\npus (Guu et al., 2020; Borgeaud et al., 2022) that serves\nas a kind of “memory” they can refer to when making\npredictions, alleviating the need to memorize as much\n∗Work partly done during Suyu’s internship at Microsoft.\ninformation in their own network parameters (Roberts\net al., 2020). This open-book approach helps language'
 'liuzy@tsinghua.edu.cn\nAbstract\nRetrieval augmentation can aid language\nmodels (LMs) in knowledge-intensive tasks\nby supplying them with external information.\nPrior works on retrieval augmentation usually\njointly fine-tune the retriever and the LM,\nmaking them closely coupled. In this paper, we\nexplore the scheme of generic retrieval plug-in:\nthe retriever is to assist target LMs that may\nnot be known beforehand or are unable to\nbe fine-tuned together.\nTo retrieve useful'
 'recent retrieval augmented language models (Guu et al.,\n2020; Izacard et al., 2022). However, it is not trivial\nto guide a retrieval model to leverage multiple corpora.\nWe need to jointly train the augmentation component\nand dense retriever using supervised relevance signals\nand self-mined hard nega'
 'et al., 2020). This open-book approach helps language\nmodels to better generalize on token prediction tasks and\nmachine translation (Khandelwal et al., 2019; Borgeaud\net al., 2022), and tasks which already involve a ﬁrst-\nstage retrieval component, e.g., OpenQA (Borgeaud\net al., 2022; Izacard et al., 2022). Existing retrieval\naugmentation methods usually stick to one single re-\ntrieval corpus throughout training and inference so that\nthe retrieval component can be indirectly guided by the']",['The purpose of retrieval augmentation in language models is to aid these models in knowledge-intensive tasks by supplying them with external information. This is achieved by enhancing the language model with a retriever that can retrieve knowledge from an external corpus. The retriever assists target language models that may not be known beforehand or are unable to be fine-tuned together. This process can significantly improve the zero-shot generalization of larger target language models.']
What is the mechanism used to retrieve augmentation documents from multiple information corpora?,"The mechanism used to retrieve augmentation documents from multiple information corpora is called ""mixture-of-memory"" (MoMA). It retrieves documents from a ""mixture"" of multiple external corpora, allowing for a larger and more comprehensive source of knowledge.","['recent retrieval augmented language models (Guu et al.,\n2020; Izacard et al., 2022). However, it is not trivial\nto guide a retrieval model to leverage multiple corpora.\nWe need to jointly train the augmentation component\nand dense retriever using supervised relevance signals\nand self-mined hard nega'
 'formation from external knowledge resources such\nas document corpora (Lewis et al., 2020; Izacard\nand Grave, 2021; Khandelwal et al., 2020; Izacard\net al., 2022; Jiang et al., 2022; Shi et al., 2023).\nRetrieval-augmented LMs commonly use a\nretrieve-and-generate setup where they retrieve doc-\numents based on the user’s input (e.g. questions\nin question answering), and then generate a com-\nplete answer conditioning on the retrieved docu-\nments (Lewis et al., 2020; Izacard and Grave, 2021;'
 'knowledge from an external corpus. On the other\nhand, prior retrieval augmentation methods (Izac-\nard and Grave, 2021a; Izacard et al., 2022) necessi-\ntate fine-tuning the backbone LM to adjust to the\nretriever and tackle specific downstream tasks. This\nkind of fine-tuning can be expensive when more\nand more unique demands emerge (Maronikolakis\nand Schütze, 2021). More importantly, many top-\ntier LMs can only be accessed through black-box\nAPIs (Ouyang et al., 2022; OpenAI, 2023). These'
 'the retrieval component can be indirectly guided by the\nsupervision from end tasks.\nIn this paper we improve the zero-shot generalization\nability of language models using “mixture-of-memory”\n(MoMA), a new retrieval augmentation mechanism. In-\nstead of a single corpus, MoMA retrieves documents\nfrom a “mixture” of multiple external corpora and en-\njoys the merits of a larger and more comprehensive\nsource of knowledge. This mechanism also allows re-']","[""The mechanism used to retrieve augmentation documents from multiple information corpora is called Mixture-Of-Memory Augmentation (MoMA). This mechanism allows for the retrieval of documents from a 'mixture' of multiple external corpora, also referred to as 'external memories'. It also provides the option to 'plug in' new memory at inference time.""]"
